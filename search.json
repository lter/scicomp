[
  {
    "objectID": "wg_tools.html",
    "href": "wg_tools.html",
    "title": "Suggested Tools",
    "section": "",
    "text": "We primarily work on collaborative projects where we synthesize existing data to draw larger inferences than any single data set would allow. Because of this, we strongly recommend that each tool used by a team accomplish as many purposes as possible to avoid a project accruing endless “one off” tools that fit a specific purpose but do not accomplish any other tasks. Streamlining your workflow to just a few broadly useful programs also helps reduce the barrier to entry to training new team members and ensures that within team protocols are clear and concise to follow.\nThe analytical software options available at NCEAS follow directly from this ethos. Although occasionally providing specialty programs (upon request), we have otherwise carefully assembled a powerful lineup of scripted, cross-platform, scalable applications that are well-supported, generate robust results, and permit batch processing. Although these packages require an initial time investment to learn, and may seem intimidating to scientists familiar with only “point-and-click” software, we strongly argue that the long-term payoff is well worth the time investment at the start.\nWe recommend the following programs, websites, and platforms:"
  },
  {
    "objectID": "wg_tools.html#collaborative-tools",
    "href": "wg_tools.html#collaborative-tools",
    "title": "Suggested Tools",
    "section": "Collaborative Tools",
    "text": "Collaborative Tools\n\nVideo Conferencing\n\nBetween your in-person meetings at NCEAS (and during them if you have remote participants!) we recommend that you use Zoom. During the COVID-19 pandemic, Zoom became–arguably–the video chatting program and the value of your group’s familiarity with this program should not be underestimated.\n\n\nMessaging\n\nYour group will be doing a lot of written communication so we recommend creating a Slack community. The advantage of Slack over email is that you can have faster real-time conversations among multiple people without crowding your inbox. Slack also supports attaching files and “pinning” posts to make it easier to find them later. You can also create “channels” for sub-topics within your group (e.g., #paper 1, #analysis, etc.) that will allow your group members to track only the subtopics that are relevant to them. You can download the Slack desktop app here. Another team at NCEAS has created this guide for getting set up on Slack.\n\n\nDocument Sharing\n\nWe recommend that you share documents among your group using Google Drive to help you centralize your information. Our team will set up a Shared Google Drive for every working group. This drive can also be used as a staging area for your data during the data collection and exploration phases of your project. This is in part because many people are already at least somewhat familiar with Google Drive. In addition, there is an R package called googledrive that will allow you to directly connect any R scripts to Google Drive files and folders. This can be great for ensuring that all scripts use the same raw data and can be useful for exporting synthesized data products to a given Drive folder.\nFor the more analytical intensive phase of your project, NCEAS also has several analytical servers that can be used to store large data that need to be processed frequently. We’ll discuss NCEAS’ servers in greater detail further on in the onboarding process."
  },
  {
    "objectID": "wg_tools.html#computing-tools",
    "href": "wg_tools.html#computing-tools",
    "title": "Suggested Tools",
    "section": "Computing Tools",
    "text": "Computing Tools\n\nCode Storage & Versioning\n\nGit is a great program to use for tracking changes to your code as your project grows and evolves. Git is one type of “version control” software which is specifically built to track changes to code in an informative and useful way. It is analogous to using “track changes” in Microsoft Word but is built in a more seamless way and is purpose-built for working with code. You can install Git following the instructions here.\n\nSimilarly to R versus RStudio, we recommend that you create an account with GitHub so that the changes you track with Git can be viewed and interacted with in a straightforward way. GitHub is the industry standard for tracking changes to code and is a great way of making code for a specific project publicly-accessible once you are at a stage where that feels appropriate. We offer an introductory workshop on Git and GitHub that we are happy to offer to your group if that is of interest!\n\n\nAnalysis\n\nOur team is most well-versed in R and many data synthesis ends can be accomplished in this language. R’s primary advantage is its amazing user community! R users have developed all kinds of custom functions and packages so even when what you want to do is not supported by an R package on CRAN, chances are that you can find the tools you need online!\n\nWe strongly recommend using R through RStudio as the RStudio interface allows several ‘quality of life’ improvements that you will grow to greatly appreciate if you’re not already an RStudio user. RStudio enables an easier connection to Git (see below), facilitates use of the command line, and allows you to generate PDF or HTML reports with embedded code chunks for easy sharing in and outside of your group.\n\nIf your group is working with genomes or other large data files, you may find R’s memory limit to be a serious hindrance. If this is the case (or if you prefer to not use R!) we suggest Python as an alternative. Python works better with larger files and is also a well-respected programming language."
  },
  {
    "objectID": "wg_services.html",
    "href": "wg_services.html",
    "title": "Working Together",
    "section": "",
    "text": "We are excited to work with your team to help you develop reproducible workflows to process, harmonize, and analyze the large amount of (heterogeneous) data necessary to conduct synthesis science. Those reproducible workflows will help you to integrate new information more easily, iterate more quickly to test your various hypotheses, and enable you to better collaborate. We currently offer a range of options for what our collaboration might look like.\nThe categories below are not exhaustive so if you think your needs will fall between or outside of these tasks, please don’t hesitate to contact our team to start that conversation!"
  },
  {
    "objectID": "wg_services.html#tasks",
    "href": "wg_services.html#tasks",
    "title": "Working Together",
    "section": "Tasks",
    "text": "Tasks\nThis level of collaboration is the core of our value to working groups! When your group identifies a data-related need (e.g., designing an analytical workflow, creating a website, writing an R Shiny app, etc.), you reach out to our team and get the conversation started. During that time we will work closely with you to define the scope of the work and get a clear picture of what “success” looks like in this context.\nOnce the task is appropriately defined, the conversation moves on to how independently you’d like us to work. This varies dramatically between tasks even within a single working group and there is no single right answer! For some tasks, we are capable of working completely independently and returning to your team with a finished product in hand for review but we are equally comfortable working closely with you throughout the life-cycle of a task."
  },
  {
    "objectID": "wg_services.html#analytical-sprints",
    "href": "wg_services.html#analytical-sprints",
    "title": "Working Together",
    "section": "Analytical Sprints",
    "text": "Analytical Sprints\nFor more intense support, we offer “analytical sprints.” If your group requests an analytical sprint, you will get one of our team members working full time only for your group’s tasks for 3-4 weeks! This can be a great option if your group has many smaller tasks or fewer larger projects that are particularly time-sensitive.\nWe are excited that these sprints are a part of our menu of offerings to you all but please reach out to us to start the conversation before requesting a sprint so that we can make sure we are all on the same page."
  },
  {
    "objectID": "wg_services.html#weekly-office-hours",
    "href": "wg_services.html#weekly-office-hours",
    "title": "Working Together",
    "section": "Weekly Office Hours",
    "text": "Weekly Office Hours\nEach of our staff members offers a one-hour block weekly as a standing office hour each week. This is a great time to join us with small hurdles or obstacles you’re experiencing in a given week. For example, previous office hours have dealt with topics like refreshing on Git/GitHub vocabulary, authenticating the googledrive R package, or solving a specific error in a new R script."
  },
  {
    "objectID": "wg_services.html#trainings",
    "href": "wg_services.html#trainings",
    "title": "Working Together",
    "section": "Trainings",
    "text": "Trainings\nWe have three primary methods for helping your group further develop skills in reproducible data science to enable your team to better collaborate and efficiently tackle data and analytical challenges. To access a category of our skill development content, click the corresponding dropdown menu in the navbar at the top of this website.\nWe are also happy to design new workshops, tutorials, or tips if your group wants training on something within our knowledge base for which we haven’t yet built something.\n\nWorkshops\nWorkshops are typically done remotely and last 2-3 hours but we can be flexible with that timing depending on your group’s needs. We can also accommodate time during one of your meetings if desired.\n\n\nTutorials\nThe tutorials tend to be smaller in scope than the full workshops and more ‘go at your own pace’-style. Despite that, they are still built to maximize value to your group either as review or first contact with a given subject.\n\n\nTips\nFinally, we’ve also curated a set of ‘code tips’ that are even smaller in scale than the tutorials. These are often just a short summary of our team’s opinion on a given subject."
  },
  {
    "objectID": "wg_services.html#selected-portfolio-items",
    "href": "wg_services.html#selected-portfolio-items",
    "title": "Working Together",
    "section": "Selected Portfolio Items",
    "text": "Selected Portfolio Items\nWe thought it might be nice to highlight a few representative products that have come out of our support in order to make our services a little more tangible.\n\nData harmonization & wrangling –  | \nSpatial data processing –  | \nCreating a website for your working group to showcase products – \nAdvice on project structure and file naming conventions – \nDesigning standalone GitHub repositories for particular papers –  | \nCreating an R package for frequently used tools –  | \nWriting & deploying interactive Shiny apps – \n\nIf desired, we are also happy to write parts of the Methods and/or Results sections of manuscripts being prepared for publication. We do not require you to add members of our team as co-authors on any product. That said, many groups offer authorship if we meet the criteria they set out for their other collaborators."
  },
  {
    "objectID": "tutorial_user-info.html",
    "href": "tutorial_user-info.html",
    "title": "Storing User-Specific Information Simply",
    "section": "",
    "text": "Working groups sometimes need to handle user-specific information in their code. For example, if your group stores your data in the cloud (e.g., in Box, in Dropbox, etc.) each user will have a different “absolute file path” to the synced version of the data folder on their personal computer. Similarly, groups may find it valuable to use their email address in the code. While you could simply have each group member add their information (file path, email, etc.) and comment out all but one of them when you work in that script, there is a better option: creating a small file to store that information!\nThe main advantage of this method is that you and your group members will not have to manually change any user-specific information in scripts just because a different person runs them!\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\n\nFeel free to skip any steps that you have already completed!\n\n\n\nAs in so many facets of collaborative work, the first step is to discuss with your group. While creating this small file is useful, you and your group still need to agree on two pieces of information:\n\nWhat is the name of the small file that each person will have?\n\nA consistent file name lets all scripts expect the same file even though the contents of that file differ for each user\n\nWhat are the column names that contain the information your group wants to store?\n\nConsistent column names allow scripts to find the user-specific content they need under a predictable subheading\n\n\nWe recommend keeping it simple so consider naming the file “user.csv”. Similarly, for the column names consider short, all-lowercase names that are succinct and clear (e.g., dropbox_path for each user’s path to a local Dropbox sync, email for each user’s email address, etc.).\n\n\n\nCreating the file does unfortunately need to be done manually but it can be done either in script form or via Microsoft Excel/etc. depending on each group member’s preference. For the purposes of this tutorial, we’ll demonstrate how to do this with code.\n\n# Create the data frame\n1my_info &lt;- data.frame(\"dropbox_path\" = \"~/Users/lyon/Dropbox/LTER Data/\",\n                      \"email\" = \"lyon@nceas.ucsb.edu\")\n\n# Look at it\nmy_info\n\n\n1\n\nNote that the column names should be standardized across all group members but the values in the rows should be unique to each user\n\n\n\n\n                     dropbox_path               email\n1 ~/Users/lyon/Dropbox/LTER Data/ lyon@nceas.ucsb.edu\n\n\n\n\n\n\n\n\nWarningGitHub Note\n\n\n\nIf your group is using GitHub, we strongly recommend that you add this file name to the .gitignore file. That file tells Git what not to track changes to which is ideal because each member of your group will have a different version of user.csv (but they’ll all share the same name).\nThis is especially critical if you want to store sensitive information in the user-specific file (e.g., passwords, tokens, etc.).\nYou can edit the .gitignore by clicking it in RStudio’s “Files” pane and editing it as you would any other file. For a deeper dive on this topic, see our GitHub workshop here.\n\n\nOnce you’re happy with your data frame, save a local copy as a CSV with the file name that your group agreed on.\n\n# Export CSV to your computer\nwrite.csv(x = my_info, file = \"user.csv\", na = '', row.names = F)\n\n\n\n\nNow that all group members have their own version of user.csv, your project scripts can read it in and use it to handle any actions that require differences among users! See an example below:\n\n# Read in user file\nuser_info &lt;- read.csv(file = \"user.csv\")\n\n# Use its contents!\ntidy_v1 &lt;- read.csv(file = file.path(user_info$dropbox_path, \"harmonized-data.csv\"))\n\n# Syntax is the same as accessing a column in any other dataframe\ngoogledrive::drive_auth(email = user_info$email)\n\nNow everyone in your group can use the same script because their personal file paths are readily accessible without needing to be hard-coded! The same theory applies to any other piece of information your group finds it valuable to store.\n\n\n\nIdentifying and manually writing out the file path you want to preserve in this kind of file can be cumbersome so we’ve found a nice work-around (at least for Mac users) that you may find useful.\n\nOpen Finder and navigate to the last folder in the file path (i.e., the most nested one)\nIn the row of folder names in the bottom of the Finder window, right-click the folder name and select “Copy ‘&lt;folder name&gt;’ as Pathname”\nPaste this into your code where you prepare to make your user-specific CSV"
  },
  {
    "objectID": "tutorial_user-info.html#overview",
    "href": "tutorial_user-info.html#overview",
    "title": "Storing User-Specific Information Simply",
    "section": "",
    "text": "Working groups sometimes need to handle user-specific information in their code. For example, if your group stores your data in the cloud (e.g., in Box, in Dropbox, etc.) each user will have a different “absolute file path” to the synced version of the data folder on their personal computer. Similarly, groups may find it valuable to use their email address in the code. While you could simply have each group member add their information (file path, email, etc.) and comment out all but one of them when you work in that script, there is a better option: creating a small file to store that information!\nThe main advantage of this method is that you and your group members will not have to manually change any user-specific information in scripts just because a different person runs them!\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\n\nFeel free to skip any steps that you have already completed!\n\n\n\nAs in so many facets of collaborative work, the first step is to discuss with your group. While creating this small file is useful, you and your group still need to agree on two pieces of information:\n\nWhat is the name of the small file that each person will have?\n\nA consistent file name lets all scripts expect the same file even though the contents of that file differ for each user\n\nWhat are the column names that contain the information your group wants to store?\n\nConsistent column names allow scripts to find the user-specific content they need under a predictable subheading\n\n\nWe recommend keeping it simple so consider naming the file “user.csv”. Similarly, for the column names consider short, all-lowercase names that are succinct and clear (e.g., dropbox_path for each user’s path to a local Dropbox sync, email for each user’s email address, etc.).\n\n\n\nCreating the file does unfortunately need to be done manually but it can be done either in script form or via Microsoft Excel/etc. depending on each group member’s preference. For the purposes of this tutorial, we’ll demonstrate how to do this with code.\n\n# Create the data frame\n1my_info &lt;- data.frame(\"dropbox_path\" = \"~/Users/lyon/Dropbox/LTER Data/\",\n                      \"email\" = \"lyon@nceas.ucsb.edu\")\n\n# Look at it\nmy_info\n\n\n1\n\nNote that the column names should be standardized across all group members but the values in the rows should be unique to each user\n\n\n\n\n                     dropbox_path               email\n1 ~/Users/lyon/Dropbox/LTER Data/ lyon@nceas.ucsb.edu\n\n\n\n\n\n\n\n\nWarningGitHub Note\n\n\n\nIf your group is using GitHub, we strongly recommend that you add this file name to the .gitignore file. That file tells Git what not to track changes to which is ideal because each member of your group will have a different version of user.csv (but they’ll all share the same name).\nThis is especially critical if you want to store sensitive information in the user-specific file (e.g., passwords, tokens, etc.).\nYou can edit the .gitignore by clicking it in RStudio’s “Files” pane and editing it as you would any other file. For a deeper dive on this topic, see our GitHub workshop here.\n\n\nOnce you’re happy with your data frame, save a local copy as a CSV with the file name that your group agreed on.\n\n# Export CSV to your computer\nwrite.csv(x = my_info, file = \"user.csv\", na = '', row.names = F)\n\n\n\n\nNow that all group members have their own version of user.csv, your project scripts can read it in and use it to handle any actions that require differences among users! See an example below:\n\n# Read in user file\nuser_info &lt;- read.csv(file = \"user.csv\")\n\n# Use its contents!\ntidy_v1 &lt;- read.csv(file = file.path(user_info$dropbox_path, \"harmonized-data.csv\"))\n\n# Syntax is the same as accessing a column in any other dataframe\ngoogledrive::drive_auth(email = user_info$email)\n\nNow everyone in your group can use the same script because their personal file paths are readily accessible without needing to be hard-coded! The same theory applies to any other piece of information your group finds it valuable to store.\n\n\n\nIdentifying and manually writing out the file path you want to preserve in this kind of file can be cumbersome so we’ve found a nice work-around (at least for Mac users) that you may find useful.\n\nOpen Finder and navigate to the last folder in the file path (i.e., the most nested one)\nIn the row of folder names in the bottom of the Finder window, right-click the folder name and select “Copy ‘&lt;folder name&gt;’ as Pathname”\nPaste this into your code where you prepare to make your user-specific CSV"
  },
  {
    "objectID": "tip_paths.html",
    "href": "tip_paths.html",
    "title": "Reproducible File Paths",
    "section": "",
    "text": "This section contains our recommendations for handling file paths. When you code collaboratively (e.g., with GitHub), accounting for the difference between your folder structure and those of your colleagues becomes critical. Ideally your code should be completely agnostic about (1) the operating system of the computer it is running on (i.e., Windows vs. Mac) and (2) the folder structure of the computer. We can–fortunately–handle these two considerations relatively simply. This may seem somewhat dry but it is worth mentioning that failing to use relative file paths is a significant hindrance to reproducibility (see Trisovic et al. 2022).\nYou may also find our tutorial on storing user-specific information valuable in this context."
  },
  {
    "objectID": "tip_paths.html#overview",
    "href": "tip_paths.html#overview",
    "title": "Reproducible File Paths",
    "section": "",
    "text": "This section contains our recommendations for handling file paths. When you code collaboratively (e.g., with GitHub), accounting for the difference between your folder structure and those of your colleagues becomes critical. Ideally your code should be completely agnostic about (1) the operating system of the computer it is running on (i.e., Windows vs. Mac) and (2) the folder structure of the computer. We can–fortunately–handle these two considerations relatively simply. This may seem somewhat dry but it is worth mentioning that failing to use relative file paths is a significant hindrance to reproducibility (see Trisovic et al. 2022).\nYou may also find our tutorial on storing user-specific information valuable in this context."
  },
  {
    "objectID": "tip_paths.html#preserve-file-paths-as-objects",
    "href": "tip_paths.html#preserve-file-paths-as-objects",
    "title": "Reproducible File Paths",
    "section": "1. Preserve File Paths as Objects",
    "text": "1. Preserve File Paths as Objects\nDepending on the operating system of the computer, the slashes between folder names are different (\\ versus /). The file.path function automatically detects the computer operating system and inserts the correct slash. We recommend using this function and assigning your file path to an object.\n\nmy_path &lt;- file.path(\"path\", \"to\", \"my\", \"file\")\nmy_path\n\n[1] \"path/to/my/file\"\n\n\nOnce you have that path object, you can use it everywhere you import or export information to/from the code (with another use of file.path to get the right type of slash!).\n\n# Import\nmy_raw_data &lt;- read.csv(file = file.path(my_path, \"raw_data.csv\"))\n\n# Export\nwrite.csv(x = data_object, file = file.path(my_path, \"tidy_data.csv\"))"
  },
  {
    "objectID": "tip_paths.html#create-necessary-sub-folders-in-the-code",
    "href": "tip_paths.html#create-necessary-sub-folders-in-the-code",
    "title": "Reproducible File Paths",
    "section": "2. Create Necessary Sub-Folders in the Code",
    "text": "2. Create Necessary Sub-Folders in the Code\nUsing file.path guarantees that your code will work regardless of the upstream folder structure but what about the folders that you need to export or import things to/from? For example, say your graphs.R script saves a couple of useful exploratory graphs to the “Plots” folder, how would you guarantee that everyone running graphs.R has a “Plots folder”? You can use the dir.create function to create the folder in the code (and include your path object from step 1!).\n\n# Create needed folder\ndir.create(path = file.path(my_path, \"Plots\"), showWarnings = FALSE)\n\n# Then export to that folder\nggplot2::ggsave(filename = file.path(my_path, \"Plots\", \"my_plot.png\"))\n\nThe showWarnings argument of dir.create simply warns you if the folder you’re creating already exists or not. There is no negative to “creating” a folder that already exists (nothing is overwritten!!) but the warning can be confusing so we can silence it ahead of time."
  },
  {
    "objectID": "tip_paths.html#file-paths-summary",
    "href": "tip_paths.html#file-paths-summary",
    "title": "Reproducible File Paths",
    "section": "File Paths Summary",
    "text": "File Paths Summary\nWe strongly recommend following these guidelines so that your scripts work regardless of (1) the operating system, (2) folders “upstream” of the working directory, and (3) folders within the project. This will help your code by flexible and reproducible when others are attempting to re-run your scripts!"
  },
  {
    "objectID": "tip_organize.html",
    "href": "tip_organize.html",
    "title": "Project Organization",
    "section": "",
    "text": "Project organization is one of those topics that is–maybe–not exciting but is absolutely critical to the success of synthesis working groups. In individual projects conducted over short timescales, an absence of an overarching strategy for organization can be survivable but in a project that will last for several years and will have many actively contributing colleagues this lack quickly becomes an insurmountable hurdle."
  },
  {
    "objectID": "tip_organize.html#overview",
    "href": "tip_organize.html#overview",
    "title": "Project Organization",
    "section": "",
    "text": "Project organization is one of those topics that is–maybe–not exciting but is absolutely critical to the success of synthesis working groups. In individual projects conducted over short timescales, an absence of an overarching strategy for organization can be survivable but in a project that will last for several years and will have many actively contributing colleagues this lack quickly becomes an insurmountable hurdle."
  },
  {
    "objectID": "tip_organize.html#when-should-we-plan",
    "href": "tip_organize.html#when-should-we-plan",
    "title": "Project Organization",
    "section": "When Should We Plan?",
    "text": "When Should We Plan?\nIt is never too early to draft your organization plan! The longer you wait to discuss an organization strategy and decide on a system the worse it will be to go back and retroactively sort the content your group has produced into the structure you agree upon. If you make a plan early on it should be clear where a given file should ‘live’ as it is created which completely removes the labor-intensive reorganization that is inherent to organizing plans made later in a project’s lifecycle."
  },
  {
    "objectID": "tip_organize.html#broad-considerations",
    "href": "tip_organize.html#broad-considerations",
    "title": "Project Organization",
    "section": "Broad Considerations",
    "text": "Broad Considerations\nThere is no single “best” mode of organizing a project. However, there are some decision points that most groups consider even if not all groups reach the same conclusions at those junctures. Here are some guiding questions that might prove helpful as your group discusses your organizing plan:\n\nWhat “modules” or “silos” are likely to be necessary for your project?\n\nSemi-discrete subcomponents of the project are likely to exist and should (probably) be separated into different folders\nFor example, groups almost always have at least the following four major folders: “data”, “notes”, “publications”, and “presentations”\n\nWhat level of organization can your group easily maintain for 2-4 years?\n\nIdeally your chosen structure will require little to no maintenance after it is initially set-up\n\nHow hard will it be to onboard new members to navigate your chosen system?\n\nYour group will likely need to onboard new members and if they don’t know where they should add their contributions they may add files in incorrect places or refrain from contributing at all–either outcome would be a sad loss for your group\nNote this question also applies to ‘future you’ if you focus on other work for a time and then need to remind yourself how to navigate this project"
  },
  {
    "objectID": "tip_organize.html#scicomp-recommendation-example",
    "href": "tip_organize.html#scicomp-recommendation-example",
    "title": "Project Organization",
    "section": "SciComp Recommendation Example",
    "text": "SciComp Recommendation Example\nWhile there is no “one size fits all” solution, our team has identified a structure that has worked quite well for past groups because it is relatively simple to maintain and easily extensible as project questions evolve. In addition, it avoids an overly nested folder structure which makes it easier for new members (or ‘future you’) to become familiar with the overall schema.\n\n\n\nGoogle Drive Structure\n Shared Drive\n |–  data\n |    |–   data-log.csv\n |    |–   raw\n |    |–   tidy\n |    |     |–  01_data-harmonized.csv\n |    |     |–  02_data-wrangled.csv\n |    |      L   03_data-filtered.csv\n |    |–   climate\n |     L   land-cover\n |–  notes\n |–  presentations\n  L   publications\n       |–  community-composition\n       |    L  graphs\n        L   synchrony\n\n\n\nGitHub Structure\n GitHub Repository\n |–  README.md\n |–  .gitignore\n |–  scripts\n |    |–   00_download-data.R\n |    |–   01_harmonize.R\n |    |–   02_quality-control.R\n |    |–   03_filter.R\n |    |–   04_stats.R\n |     L   05_graph.R\n |–  tools\n |    |–   README.md\n |    |–   fxn_calc-beta-diversity.R\n |     L   fxn_bookmark-graph.R\n  L  explore\n      |–   README.md\n      |–   downs-stats.py\n       L   lyon-graphs.R\n\n\n\nCheck out the tabs below for highlights of these structures!\n\nBoth! Google Drive-Specific GitHub-Specific\n\n\n\nLimited use of sub-folders\nConsistent folder/file naming conventions\n\nGood names should be be both human and machine-readable\nAvoid spaces and special characters\nConsistent use of delimeters (e.g., “-”, “_“, etc.)\n\nShared file prefix (a.k.a. “slug”) connecting code files with files they create\n\nAllows for easy tracing of errors because the file with issues has an explicit tie to the script that likely introduced that error\n\n\n\n\n\nContains inputs to code and outputs from code but not code itself\n\nTrust that GitHub does a better job of tracking code files than Drive can\nAlso remember that duplicating code files and storing them in multiple places is a recipe for heartache as there is no “single source of truth” upon which to depend\n\nDedicated place for notes / presentations\nShared data/ folder agnostic to specific product\n\nWill let your group start with the same data product for each paper (just filtered/analyzed differently for each product)\n\n\n\n\n\nContains code but not inputs/outputs\n\nUse the .gitignore file to regulate what Git will/won’t track (for more info, see here)\n\nDedicated “README” files containing high-level information about each folder\nNumbered script names making workflow order explicit\nUse of custom functions for repeated operations\n\nThis is a great way of ensuring reproducibility–and if you create enough functions, a software package might be a nice ‘bonus’ product for your group!\n\nexplore/ folder for ‘rough draft’ scripts developed by particular members\n\nIncluding this can be a great way of making everyone feel more comfortable contributing–even if they are not completely confident in their coding skills\nIncluding surnames in file names here can be a nice way of avoiding merge conflicts\nYou can always rename these files and move them to the scripts/ folder if they seem valuable for the core workflow!"
  },
  {
    "objectID": "tip_organize.html#additional-resources",
    "href": "tip_organize.html#additional-resources",
    "title": "Project Organization",
    "section": "Additional Resources",
    "text": "Additional Resources\nIf you’d like more resources on project organization and reproducibility in general, check out the following:\n\nLTER’s Synthesis Skills for Early Career Researchers (SSECR) Reproducibility module\nLTER Scientific Computing Team’s Tips for File Naming"
  },
  {
    "objectID": "tip_names.html",
    "href": "tip_names.html",
    "title": "Good Naming Conventions",
    "section": "",
    "text": "When you first start working on a project with your group members, figuring out what to name your folders/files may not be at the top of your priority list. However, following a good naming convention will allow team members to quickly locate files and figure out what they contain. The organized naming structure will also allow new members of the group to be onboarded more easily!"
  },
  {
    "objectID": "tip_names.html#overview",
    "href": "tip_names.html#overview",
    "title": "Good Naming Conventions",
    "section": "",
    "text": "When you first start working on a project with your group members, figuring out what to name your folders/files may not be at the top of your priority list. However, following a good naming convention will allow team members to quickly locate files and figure out what they contain. The organized naming structure will also allow new members of the group to be onboarded more easily!"
  },
  {
    "objectID": "tip_names.html#naming-tips",
    "href": "tip_names.html#naming-tips",
    "title": "Good Naming Conventions",
    "section": "Naming Tips",
    "text": "Naming Tips\nHere is a summary of some naming tips that we recommend. These were taken from the Reproducibility Best Practices module in the LTER’s SSECR course. Please feel free to refer to the aforementioned link for more information.\n\nNames should be informative\n\nAn ideal file name should give some information about the file’s contents, purpose, and relation to other project files.\nFor example, if you have a bunch of scripts that need to be run in order, consider adding step numbers to the start of each file name (e.g., “01_harmonize_data.R” or “step01_harmonize_data.R”).\n\nNames should avoid spaces and special characters\n\nSpaces and special characters (e.g., é, ü, etc.) in folder/file names may cause errors when someone with a Windows computer tries to read those file paths. You can replace spaces with delimiters like underscores or hyphens to increase machine readability.\n\nFollow a consistent naming convention throughout!\n\nIf you and your group members find a naming convention that works, stick with it! Having a consistent naming convention is key to getting new collaborators to follow it."
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Our Team",
    "section": "",
    "text": "Because we live in an era where we may only meet in person sporadically, we felt it would be nice to introduce ourselves here to help you put a face to the emails / Slack messages / GitHub issues we exchange going forward! If you would like to email the whole team as one send your questions to scicomp [at] nceas.ucsb.edu"
  },
  {
    "objectID": "staff.html#past-team-members",
    "href": "staff.html#past-team-members",
    "title": "Our Team",
    "section": "Past Team Members",
    "text": "Past Team Members\nWe have been privileged to work with the following teammates who have since gone on to other positions.\n\nJulien Brun (he/him)\n brunj7.github.io –  brunj7 –  @brunj7 – jb160 [at] ucsb.edu\n\nJulien was responsible for mentoring the other members of the scientific computing support team as well as advising researchers on how best to address their data challenges with scripted, reproducible solutions. Julien now works for the UCSB Library team as a research facilitator in the earth and environmental sciences. Julien is also a Lecturer in the Master in Environmental Data Science program at Bren School of Environmental Science and Management at UC Santa Barbara, where he teaches “good enough” practices in reproducible and collaborative data science.\n\n\nAngel Chen (she/her)\n angelchen7.github.io –  angelchen7 – angel.chen.ac3656 [at] yale.edu\n\nComing from a background in statistics and data science, Angel supported working groups by developing data pipelines and reproducible analytical workflows to integrate various sources of data. She now works for the Malone Disturbance Ecology Lab at Yale University as a data scientist."
  },
  {
    "objectID": "modules_tutorials/googledrive-auth.html",
    "href": "modules_tutorials/googledrive-auth.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "In order to connect R with a GoogleDrive, we’ll need to authorize googledrive to act on our behalf. This only needs to be done once (per computer) so follow along and you’ll be building GoogleDrive into your workflows in no time!\nFirst, install the googledrive and httpuv R packages. The googledrive package’s need is self-evident while the httpuv package makes the following steps a little easier than googledrive makes it alone. Be sure to load the googledrive package after you install it!\n\n# Install packages\ninstall.packages(c(\"googledrive\", \"httpuv\"))\n\n# Load them\nlibrary(googledrive)\n\nOnce you’ve installed the packages we can begin the authentication in R using the drive_auth function in the googledrive package.\n\ngoogledrive::drive_auth(email = \"enter your gmail here!\")\n\nIf this is your first time using googledrive, drive_auth will kick you to a new tab of your browser (see below for a screen grab of that screen) where you can pick which Gmail you’d like to connect to R.\n\n\n\nClick the Gmail you want to use and you will get a second screen where Google tells you that “Tidyverse API” wants access to your Google Account. This message is followed by three checkboxes, the first two are grayed out but the third is unchecked.\n\n\n\n\n\n\n\n\n\nImportantNOTE\n\n\n\nThis next bit is vitally important so carefully read and follow the next instruction!\n\n\nIn this screen, you must check the unchecked box to be able to use the googledrive R package. If you do not check this box all attempts to use googledrive functions will get an error that says “insufficient permissions”.\n\n\n\nWhile granting access to “see, edit, create, and”delete” all of your Google Drive files” sounds like a significant security risk, those powers are actually why you’re using the googledrive package in the first place! You want to be able to download existing Drive files, change them in R on your computer, and then put them back in Google Drive which is exactly what is meant by “see, edit, create, and delete”.\nAlso, this power only applies to the computer you’re currently working on! Granting access on your work computer allows only that computer to access your Drive files. So don’t worry about giving access to your Drive to the whole world, that is protected by the same failsafes that you use when you let your computer remember a password to a website you frequent.\nAfter you’ve checked the authorization box, scroll down and click the “Continue” button.\n\n\n\nThis should result in a plain text page that tells you to close this window and return to R. If you see this message you are ready to use the googledrive package!"
  },
  {
    "objectID": "modules_tutorials/github-create.html",
    "href": "modules_tutorials/github-create.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "From your GitHub “Repositories” tab, click the “New” button. It may be either blue or green depending on your GitHub settings.\n\nAdd a title to your repository and add a description.\n\n\n\nOnce that is done, scroll down and decide whether to add (1) a README, (2) a .gitignore, and (3) a license. For Quarto websites, you will want a README and a .gitignore but a license is not required. If you are an RStudio user, you should pick the “R” template for your .gitignore. For more information on READMEs see here; for more information on the .gitignore see here.\n\n\n\nOnce you’ve added these two things, scroll down and click the “Create repository” button. Again, this may be either blue or green.\nAfter a few seconds you should be placed on your new repository’s landing page."
  },
  {
    "objectID": "internal/team-onboard.html",
    "href": "internal/team-onboard.html",
    "title": "SciComp Team Onboarding",
    "section": "",
    "text": "This page contains a quick, cookbook-style set of instructions for new staff on the Scientific Computing Support Team. We are so excited to have you join us and we hope that these instructions are clear and easy-to-follow. Please don’t hesitate to reach out if you run into any issues (see the “Our Team” tab of this site) or post a GitHub issue yourself on this website’s repository!\nListed below are a mix of references and tutorials on concepts we aim to promote to researchers who we support, as well as tools and workflows we use in our team. The goal is to give context about open and reproducible science principles NCEAS is promoting to the scientific community. You will also find information on getting access to the tools that you will need and some general information about working at NCEAS."
  },
  {
    "objectID": "internal/team-onboard.html#background-reading-material",
    "href": "internal/team-onboard.html#background-reading-material",
    "title": "SciComp Team Onboarding",
    "section": "Background Reading Material",
    "text": "Background Reading Material\n\nHampton et al 2015. “The Tao of open science for ecology”\nBorer et al 2009. “Effective Data Management”\nHeidborn 2008. “Shedding Light on the Dark Data in the Long Tail of Science”\nFegraus et al 2005. “Maximizing the Value of Ecological Data with Structured Metadata:”"
  },
  {
    "objectID": "internal/team-onboard.html#necessary-software-platforms",
    "href": "internal/team-onboard.html#necessary-software-platforms",
    "title": "SciComp Team Onboarding",
    "section": "Necessary Software / Platforms",
    "text": "Necessary Software / Platforms\n\n1. Get on GitHub\nWe rely heavily on GitHub for collaboration, and there are a few things you must do to get set up. See below for details:\n\nCreate a personal GitHub account\nSend your GitHub user name to Marty Downs to be added to the LTER Network Office GitHub organization (github.com/lter)\nAsk Marty to add you the “scientific-computing” GitHub Team\n\nThis is a sub-team of the “network-office” team\n\nComplete the LTER GitHub workshop\n\n\n\n2. Familiarize Self with GitHub Issues\nWe use GitHub issues extensively to track progress on tasks and make it easier for SciComp team members to (A) collaborate with one another and (B) share progress with working groups in a completely transparent manner. Incidentally this also ensures we ‘walk the walk’ with regard to our guidance to working groups.\nWe store tasks as issues on this website’s GitHub repository and in a GitHub Project in the LTER GitHub organization. We’ve created a number of issue templates so that when you make a new issue, you start with a scaffold that–when filled out–ensures you have most of the vital context information documented explicitly. Use those templates and check out past/closed issues if you need an example of implementation.\n\n\n3. Join Slack\nWe use Slack to communicate with one another throughout the day. To be added to the NCEAS Slack group, register here. If you already have a Slack account, be sure to use the email address you used previously to register for this organization.\nFind and join our #lter-network-office channel by clicking the plus sign next to the Channels section. Feel free to join any other channels that you might find interesting! Popular channels include #diversity, #nceas-residents, and #social.\n\n\n4. Get an Account on NCEAS Server: Aurora\nWe use the Aurora server (located at aurora.nceas.ucsb.edu) when working with particularly data-intensive projects in RStudio or JupyterHub. Send an email to help [at] nceas.ucsb.edu requesting an account mentioning you are working with the LTER Scientific Computing team. Thomas Hetmank or Nick Outin (NCEAS’ IT personnel) will contact you with directions for setting up an account."
  },
  {
    "objectID": "internal/team-onboard.html#recurring-nceas-meetings",
    "href": "internal/team-onboard.html#recurring-nceas-meetings",
    "title": "SciComp Team Onboarding",
    "section": "Recurring NCEAS Meetings",
    "text": "Recurring NCEAS Meetings\nThere are a number of recurring meetings that you are encouraged to attend. At your hiring, you should receive an invitation to the NCEAS Google Calendar which has links to all the meeting information. The most common of these is Coffee Klatch–a coffee social like a departmental coffee hour–where announcements are shared and NCEAS staff have a chance to do minor chit-chat before returning to work. At time of writing, Coffee Klatch is 10:30-11:00 AM (Pacific Time) on Tuesdays."
  },
  {
    "objectID": "internal/team-onboard.html#data-science-background-and-tutorials",
    "href": "internal/team-onboard.html#data-science-background-and-tutorials",
    "title": "SciComp Team Onboarding",
    "section": "Data Science Background and Tutorials",
    "text": "Data Science Background and Tutorials\nFor good background information on many of the tools that we use and practices that we encourage, read through the LTER Synthesis Skills for Early Career Researchers (SSECR) course materials. SciComp team members had a strong presence in the design of these materials and the shape of the course closely mirrors patterns that have worked for past working groups.\n\nR Self-Assessment\nFinally, please complete this R Training Assessment to self-assess your skills in R.\nSchedule a session with the rest of the team to debrief on your experience."
  },
  {
    "objectID": "internal/team-onboard.html#hr-related-resources",
    "href": "internal/team-onboard.html#hr-related-resources",
    "title": "SciComp Team Onboarding",
    "section": "HR-related Resources",
    "text": "HR-related Resources\n\nReporting Hours & Kronos\nWe submit vacation and sick day hours online through Kronos. You should receive an email at the start of your employment adding you to an email listserv that will send reminders on when to approve each month’s timesheet.\n\n\nUC Path\nUC Path is the online HR portal and can be accessed with your UCSB Net ID. It is where you can access things like your paycheck, next date of paycheck, and benefits (if applicable)."
  },
  {
    "objectID": "internal/get-data.html",
    "href": "internal/get-data.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Since working groups may ask us to get and wrangle data from some popular databases, this page serves as a guide on acquiring data from said databases! Below are some common places where groups have asked us to request from. Please feel free to add more to the growing list as we collaborate with more and more groups."
  },
  {
    "objectID": "internal/get-data.html#guide",
    "href": "internal/get-data.html#guide",
    "title": "Data Acquisition",
    "section": "Guide",
    "text": "Guide\nThe TRY database offers a wide variety of plant trait data for many species. As of 2023, the database contains over 15 million trait records for over 300 thousand plant taxa.\n\n1. Account Registration\nTo get started, first create an account on TRY. After filling out the required fields, TRY will send an email to you with your password. Log in with your email and password.\n\n\n\n2. Terms and Conditions\nAfter logging in for the first time, TRY will direct you to its Intellectual Property Guidelines. Scroll down and click ‘I accept’ to proceed.\n\n\n\n\n\n\n3. Start a Data Request and Select Traits\nTRY will then redirect you to the Request Data page to begin a new request. This page can also be accessed by clicking “Data Portal” &gt; “TRY Database”. The recommendation is to request by traits/species.\nThen enter the numeric IDs of the traits you want data for. For example, plant height has the IDs 3106 (Plant height vegetative) and 3107 (Plant height generative).\nThe list of all trait IDs is here. There is also an option on the page to download a .txt file of all the traits.\n\n\n\n\n4. Select Species\nNext, you will be prompted to select the species you want data for. Each species is associated with a numeric ID, so enter the desired IDs in the field below. For example, 29 represents Abies alba and 56 represents Abies lasiocarpa.\nThe list of all plant IDs is here. Again, you also have the option to download a .txt file of all the plant species in case you wish to get the IDs programmatically.\n\n\n\n\n5. Choose Data Type: Public or Private\nThen, you must choose whether you want to request public data or public+private data. The former option will get you the data faster, while the latter option may take up to 14 days because the dataset custodians must respond to your request.\nIn our previous experience, requesting public data usually gets you the data within 24 hours.\n\n\n\n6. Describe Data Request\nOnce you have chosen which type of data you want, you will be prompted to enter the title and description of your project. These fields will be necessary if you requested for public+private data since the dataset custodians will need a reason to make their data available for your request.\n\n\n\n7. Add Coauthors\nAdd any relevant coauthors to your request before proceeding. It’s a good idea to add your fellow data analysts and/or working group PIs.\n\n\n\n\n8. Finish Request and Wait\nYour request is now complete! You will receive an email from TRY notifying you that they have received your request. Wait for a subsequent email from them to get the actual download link to the data."
  },
  {
    "objectID": "internal/get-data.html#relevant-example",
    "href": "internal/get-data.html#relevant-example",
    "title": "Data Acquisition",
    "section": "Relevant Example",
    "text": "Relevant Example\nSee our GitHub issue #122 and the scripts in this folder for an example on how we pulled and integrated data from the TRY database."
  },
  {
    "objectID": "internal/get-data.html#guide-1",
    "href": "internal/get-data.html#guide-1",
    "title": "Data Acquisition",
    "section": "Guide",
    "text": "Guide\nNASA’s AppEEARS (Application for Extracting and Exploring Analysis Ready Samples) platform is a useful way of extracting spatial data within a user-defined bounding box or specific points. A ton of spatial data (including MODIS datasets) are available here so it makes this portal a useful ‘one-stop shop’ for groups that want more than one spatial variable.\n\n1. Account Registration / Sign In\nTo begin, visit the AppEEARS website and register for an account. After filling out the necessary personal information and confirming your email address you should be able to sign into the portal using your username and password.\n\n\n\n2. Begin an Extraction Request\nIn order to begin an extraction request, navigate to the “Extract” button in the navbar at the top of the site and click it. Select “Area” in the resulting dropdown menu of options.\n\n\n\n3. Pick Request Type\nYou can now decide whether to start your data request from scratch (“Start a new request” on the left) or if you’d like to use a previous request as a starting point (“Copy a previous request”). Note that uploading a request as a JSON is also available but I found the user interface intuitive enough that I wasn’t tempted to use this option.\nThe primary advantage–as I see it–of duplicating a request is that it allows you to re-use your manually-drawn bounding box. This makes multiple requests for different datasets exactly share the same area which makes the eventual harmonization of those data that much simpler.\nStarting a new request is the option I take only when I haven’t previously drawn a bounding box for the area of interest.\n\n\n\n4. Fill out Request\nRegardless of whether you’re starting from a blank slate or from a previous request, the next step involves customizing this request. You’ll need to perform the following steps to complete the request:\n\nEnter a name for this request\n\nThis name is only for your internal use so it can use your idiosyncrasies but should imply something about the data layer(s) and bounding box location to make it decipherable by others.\n\nDraw a polygon over your area of interest\n\nNote that the multi-point polygon tends to result in larger data requests than a square because it overlaps more separate tiles of the source data.\n\nDecide on starting/ending dates\n\nRegardless of the temporal granularity of the source data the time range requires you to specify specific days, months, and years.\n\nPick data layers for that area and time range\n\nThe search field for the data layers is pretty robust and allows you to either search MODIS codes or names of the data (e.g., “snow” or “MODIS10A2”)\n\nChoose output file format\n\nYour options are GeoTiff (i.e., raster) or netCDF\n\nChoose projection system\n\nThis option is so helpful! Specifying your projection system lets you put all of the computational labor for re-projecting into a different coordinate reference system (CRS) onto AppEEARS rather than leaving it for you to handle after the fact\n \n\n\n5. Submit Request\nWhen you are ready, click “Submit” at the bottom of the request screen \nIf your request is at or below the data limit per request, you will receive a narrow green banner at the top of the request page notifying you of the success.\n\nIf you have requested too much data you will receive a red banner notifying you of this fact and quantifying how far over the limit your request is.\n\nThe data limit is affected by (1) the spatial extent of the request, (2) the temporal extent of the request, and (3) the number of included data layers. If you are over the limit you will need to reduce one of these parameters.\nIf possible, I recommend first reducing the number of data layers as duplicating a request with a shared bounding box and time range with a different data layer is really straightforward. If you need to re-draw the bounding box you’ll need to open another data request and manually draw additional bounding boxes to eventually fill out the total area of interest which can be somewhat cumbersome.\n\n\n6. Download Completed Request\nMoments after submitting the request successfully you will receive a “Request Received” email from AppEEARS inviting you to ‘explore’ your request. This can be safely ignored as it contains only the information that you just entered.\nSome time later (usually hours if not a day or two for larger requests) you will receive a second email notifying you that the request is complete! That email has both the “Explore” link from the preceding email and a “Download” link.\nClick the provided link and on the resulting AppEEARS page you can download the data your request yields onto your computer to do with what you will.\nNote that requests do expire after a few weeks/months so you’ll want to download the data as soon as possible. If a request is expired and you’d like to re-request, there is a button to do exactly that but you’ll need to wait for the request to process again before being able to re-download the data."
  },
  {
    "objectID": "internal/get-data.html#relevant-example-1",
    "href": "internal/get-data.html#relevant-example-1",
    "title": "Data Acquisition",
    "section": "Relevant Example",
    "text": "Relevant Example\nConsult the LTER Organization-owned Silica Export GitHub repository (see the README here) for an example of how we wrangled and extracted data retrieved from AppEEARS.\nOf particular relevance is likely the “crop-drivers.R” script where we read in the data downloaded from AppEEARS and crop each data request to avoid the small amount of spatial overlap among requests. These requests ran into the data limit and had to be split among several requests that (slightly) overlap one another. To avoid re-sampling the same pixels, the rasters retrieved from AppEEARS each needed to be cropped slightly."
  },
  {
    "objectID": "internal/get-data.html#guide-2",
    "href": "internal/get-data.html#guide-2",
    "title": "Data Acquisition",
    "section": "Guide",
    "text": "Guide\nThe National Ecological Observatory Network (NEON) is a facility that collects long-term ecological data from aquatic and terrestrial ecosystems in the United States. Their data usually falls into one of three categories: data collected by an airborne observation platform like LIDAR, data collected by a person in the field, or data collected by an automated sensor.\nNEON has a handy API that will allow you to pull their data using R. Additionally, they have created a whole suite of tutorials for various needs, including a tutorial on how to download and explore NEON data using their helper R packages."
  },
  {
    "objectID": "internal/contributing.html",
    "href": "internal/contributing.html",
    "title": "SciComp Team Contributing Guidelines",
    "section": "",
    "text": "While our team is composed of members from different backgrounds, we strive to adhere to a shared set of contributing and style guidelines. This ensures a consistent “feel” of our team’s products, regardless of which team member took the lead on a particular task.\nStandard convention in GitHub is to house such information in a “CONTRIBUTING.md” file in the top-level of the relevant repository. We have done that in the repository underpinning this website but you can also quickly jump to that file by clicking here.\nNote that these are only required for Scientific Computing team members but you may find some of this guidance helpful even if you’re not a part of our team."
  },
  {
    "objectID": "index.html#what-we-do",
    "href": "index.html#what-we-do",
    "title": "LTER Scientific Computing Team",
    "section": "What We Do",
    "text": "What We Do\nThe Scientific Computing team is a small (but mighty!) team of data scientists supporting synthesis working groups funded by the Long Term Ecological Research (LTER) Network Office. Participation in these groups is open to scientists from within and outside of the LTER Network. We provide modern technological infrastructure to support analytical, computing, or network-based needs for these synthesis working groups.\nWe are housed at the National Center for Ecological Analysis and Synthesis (NCEAS). Our goal is to support and promote an open and reproducible approach to synthesis science. We do so by providing on-demand training, advising, and direct code/data support. In addition to the support during in-person meetings at NCEAS, our team is available in-between visits to discuss and advise on data science and scientific programming tasks, such as:\n\nStructuring and integrating heterogeneous datasets\nWriting code to wrangle, analyze, model, or visualize the data your group has already collected\nDesigning workflows, scripting best practices for reproducible science, and reviewing code\nHelping you get set up on NCEAS’ server and scale your analysis\nPreserving and promoting your products on the internet\n\nIncluding everything from derived datasets and terminological glossaries/vocabularies to scripts, model codes, and interactive “web applications”\n\nOffering workshops on new skills or programs (such as Git/GitHub)\n\nDepending on your team’s preferences, we can operate on a spectrum of independence ranging from complete self-sufficiency after initial definition of task scope to coding together with your team.\nContact our team with your requests at scicomp [at] nceas.ucsb.edu"
  },
  {
    "objectID": "index.html#navigating-this-website",
    "href": "index.html#navigating-this-website",
    "title": "LTER Scientific Computing Team",
    "section": "Navigating this Website",
    "text": "Navigating this Website\n\nWorking Group MembersOther Visitors\n\n\nFor members of our working groups, this website is a centralized hub of resources designed to help you get familiar with the way our team works and what we offer. The pages under the Working Group Onboarding dropdown menu are particularly targeted at facilitating collaboration between your group and our team!\n\n\nWelcome! Even if you’re not a part of a working group, we hope that you find the various resources on this website helpful! If you’re interested in learning more about open, reproducible science, please feel free to take a look at the items under our Workshops and Tips dropdown menus.\n\n\n\n\n\n\nLTER All Scientists’ Meeting 2022, Pacific Grove CA"
  },
  {
    "objectID": "internal/deploy-shiny.html",
    "href": "internal/deploy-shiny.html",
    "title": "Deploy Shiny Apps",
    "section": "",
    "text": "Note\n\n\n\nPlease note that the following instructions for deploying Shiny apps are meant to serve as internal documentation and will only work if you have sudo power on the SciComp team’s Shiny server.\nFeel free to contact us if you have a LTER-related Shiny app that you would like to deploy on our server!\nSciComp team members can deploy LTER-related Shiny apps on our server at: https://shiny.lternet.edu/\nTo deploy your working app, first make sure that all the files live at a GitHub repository. Once our sysadmin Nick Outin has made you an account on the server, you can log in via SSH."
  },
  {
    "objectID": "internal/deploy-shiny.html#log-in",
    "href": "internal/deploy-shiny.html#log-in",
    "title": "Deploy Shiny Apps",
    "section": "Log In",
    "text": "Log In\nFor Mac users, open the Terminal app and type the following command, replacing &lt;YOUR-USERNAME&gt; with your own username.\n\nssh &lt;YOUR-USERNAME&gt;@shiny.lternet.edu\n\nWindows users can log in with PuTTY, using shiny.lternet.edu as the host name.\nCheck out NCEAS’ guide to using SSH for additional help."
  },
  {
    "objectID": "internal/deploy-shiny.html#set-up-proper-permissions",
    "href": "internal/deploy-shiny.html#set-up-proper-permissions",
    "title": "Deploy Shiny Apps",
    "section": "Set Up Proper Permissions",
    "text": "Set Up Proper Permissions\n\nJoin shiny-apps\nA shiny-apps user group has been created in this server. Set up the proper permissions for your account by adding yourself to this group with this command.\n\nsudo usermod -aG shiny-apps &lt;YOUR-USERNAME&gt;\n\nTo decipher this command a bit: sudo will enable you to run commands as a user with full control and privileges. The usermod command is used to modify user account details. The basic syntax is:\n\nusermod [OPTIONS] &lt;YOUR-USERNAME&gt;\n\nThe -G option will add the user to a supplementary group. The -aG combined options will add the user to the new supplementary group while also leaving them in the other supplementary group(s) they were already a part of.\n\n\n\n\n\n\nNote\n\n\n\nAs an FYI, the apps on the server will run as the user shiny, so shiny is also in the shiny-apps user group.\n\n\n\n\nAllow Access to Your Home Directory\nAdditionally, you need to allow shiny access to your user home directory by running:\n\nsudo chmod a+x /home/&lt;YOUR-USERNAME&gt;\n\nThis chmod (change mode) command is used to control file permissions. Basic syntax:\n\nchmod [OPTIONS] MODE FILE/DIRECTORY\n\nThere are two “modes” that you can use to set permissions: Symbolic and Octal mode. The Symbolic mode uses operators and letters. For example, the a option denotes all the owner/groups the file/directory belongs to. The + operator grants the permission, and the letter x stands for the execute permission."
  },
  {
    "objectID": "internal/deploy-shiny.html#tell-git-who-you-are",
    "href": "internal/deploy-shiny.html#tell-git-who-you-are",
    "title": "Deploy Shiny Apps",
    "section": "Tell git Who You Are",
    "text": "Tell git Who You Are\nNow that your account has the proper permissions, use the git config commands to tell git who you are.\n\ngit config --global user.name \"&lt;YOUR-GITHUB-USERNAME&gt;\"\ngit config --global user.email \"&lt;YOUR-GITHUB-EMAIL&gt;\"\n\nReplace &lt;YOUR-GITHUB-USERNAME&gt; and &lt;YOUR-GITHUB-EMAIL&gt; with your own credentials. You can check to see if you entered your details correctly by entering git config --list."
  },
  {
    "objectID": "internal/deploy-shiny.html#copy-your-app-to-the-server",
    "href": "internal/deploy-shiny.html#copy-your-app-to-the-server",
    "title": "Deploy Shiny Apps",
    "section": "Copy Your App to the Server",
    "text": "Copy Your App to the Server\nThe easiest way to get all the files for your app to the server is by git clone.\nIf you like, you can create a folder named “github” to store all your future apps. For example, in the screenshot below, I created a “github” folder using the mkdir command in my home directory.\nTo check that the folder was created, I can list all the files/folders in my current directory with ls. Since I want my app to be inside the “github” folder, I used the cd command to change into that directory.\nFinally, git clone the GitHub repo that has all the files for your app. Here, I am cloning the lterpalettefinder-shiny repo to my local directory on the virtual machine as lterpalettefinder.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can go one level above the current directory by typing cd .."
  },
  {
    "objectID": "internal/deploy-shiny.html#install-necessary-r-packages",
    "href": "internal/deploy-shiny.html#install-necessary-r-packages",
    "title": "Deploy Shiny Apps",
    "section": "Install Necessary R Packages",
    "text": "Install Necessary R Packages\nAfter you cloned the repository for your app, you can start installing the necessary R packages! To make these packages available for all users, you will want to execute commands with the root user’s privileges by typing the sudo -i command. Then open R by simply typing “R” and install packages with the usual install.packages() function.\n\n\nMissing Dependencies?\nIf you run into an error installing a R package, it’s likely because the server does not have the required dependencies installed yet.\nFor example, I wanted to install the lterpalettefinder R package, but I got lots of errors on missing dependencies instead.\n\nI saw that the curl dependency was missing, so I attempted to install that. However, R gives me another error.\n\nLooking closer, it asks me to install libcurl4-openssl-dev first. To install this Ubuntu package, I exited R with q() and logged off root with exit. Once I’m back in my own user profile, I can use the sudo apt install command to install libcurl4-openssl-dev.\n\nIf you get these prompts to restart services, you can tap Return/Enter to continue the installation.\n\n\nAfterwards, libcurl4-openssl-dev installed successfully, and then I can finally install the missing curl dependency in R!\n\nYou can repeat a similar process to find and install the rest of the required dependencies before you are able to install certain R packages."
  },
  {
    "objectID": "internal/deploy-shiny.html#symlink-to-deployed-folder",
    "href": "internal/deploy-shiny.html#symlink-to-deployed-folder",
    "title": "Deploy Shiny Apps",
    "section": "Symlink to Deployed Folder",
    "text": "Symlink to Deployed Folder\nSince all the Shiny apps are located under /srv/shiny-server/, how can we deploy the app we have in our local directory? We can create a symlink (symbolic link) between the local directory and /srv/shiny-server/. A symlink is essentially a pointer to other folders.\nCreate the link by running this command:\n\nsudo ln -s &lt;LOCAL-DIRECTORY-OF-APP&gt; &lt;SHINY-SERVER-DIRECTORY-OF-APP&gt;\n\nReplace &lt;LOCAL-DIRECTORY-OF-APP&gt; with your own directory and replace &lt;SHINY-SERVER-DIRECTORY-OF-APP&gt; with the file path of where you would like your app to deploy.\nFor example, the actual lterpalettefinder app lives under my home directory, but it needs to be deployed under /srv/shiny-server/, so I ran this command to link the two:\n\nNow /srv/shiny-server/lterpalettefinder points to /home/anchen/github/lterpalettefinder!\nYou can check the current symlinks by navigating to /srv/shiny-server/ and typing ls -l.\n\n\n\n\n\n\nNote\n\n\n\nThe name of the deployed folder corresponds to the URL: https://shiny.lternet.edu/&lt;YOUR-APP&gt;/"
  },
  {
    "objectID": "internal/deploy-shiny.html#debug-app-and-check-its-live-link",
    "href": "internal/deploy-shiny.html#debug-app-and-check-its-live-link",
    "title": "Deploy Shiny Apps",
    "section": "Debug App and Check its Live Link",
    "text": "Debug App and Check its Live Link\nIf everything goes right, your app will be live at https://shiny.lternet.edu/&lt;YOUR-APP&gt;/! If not, don’t worry and try troubleshooting what went wrong. Remember to check file paths and required R packages."
  },
  {
    "objectID": "internal/new-wg-setup.html",
    "href": "internal/new-wg-setup.html",
    "title": "Working Group Setup Checklist",
    "section": "",
    "text": "Note\n\n\n\nPlease note that the following checklist of working group setup steps is meant to serve as internal documentation only! Our team will be taking these steps on your behalf when you are funded so none of these steps are things you (a working member/PI) need to handle.\nFeel free to contact us if you find these instructions useful and want to apply them to non-working group contexts!\nWhen new working groups are funded, our team takes a number of setup steps to create some of the infrastructure that past groups have requested/found useful. This is mainly an attempt to help the group avoid spending their precious in-person meeting time doing relatively dry technical steps that we can easily accomplish early-on. Some of these steps also set a useful ‘tone’ in terms of facilitating groups’ adherence to reproducibility best practices."
  },
  {
    "objectID": "internal/new-wg-setup.html#attend-lter-network-office-new-group-onboarding-meeting",
    "href": "internal/new-wg-setup.html#attend-lter-network-office-new-group-onboarding-meeting",
    "title": "Working Group Setup Checklist",
    "section": "1. Attend LTER Network Office ‘New Group Onboarding’ Meeting",
    "text": "1. Attend LTER Network Office ‘New Group Onboarding’ Meeting\nOnce the decision is made for which group(s) to fund, Marty Downs will schedule an onboarding meeting for group PIs to talk about NCEAS / LTER Network Office resources. A SciComp team member needs to be there to give a brief introduction and pitch for the kinds of support that our team can offer."
  },
  {
    "objectID": "internal/new-wg-setup.html#create-the-infrastructure-for-the-group",
    "href": "internal/new-wg-setup.html#create-the-infrastructure-for-the-group",
    "title": "Working Group Setup Checklist",
    "section": "2. Create the Infrastructure for the Group",
    "text": "2. Create the Infrastructure for the Group\n\nGoogle GroupShared DriveGitHub RepositoryNCEAS Server Account\n\n\nRationale: Google Groups centralize all group members’ Google identities. This makes sharing access to a piece of the Google ecosystem with an entire team really simple.\nNaming Convention:\n\n“LTER-WG_&lt;Abbreviated-Group-Name&gt;”\n“LTER-SPARC_&lt;Abbreviated-Group-Name&gt;”\n\nNote that the group abbreviation should be title case (e.g., “Ecological-Synthesis”)\n\n\nRationale: A Shared Google Drive is a great place to store raw data as well as preserve documentation and script outputs. A true Shared Drive also distributes ownership in a way that makes it safe even when individual Google accounts get deactivated (as is the case when someone changes institutions).\nNaming Convention: Same as the Google Group!\nOther Notes: Add the Google Group as ‘maintainers’ and add Marty Downs and Thomas Hetmank as ‘administrators’. Also, move copies of the critical LTER template Google files into the top-level of the Shared Drive. Groups are not required to use these but they generally have been useful to groups in the past.\n\n\nRationale: GitHub is the way we recommend collaborating on code. This emphasis is made more particularly clear elsewhere so we’ll leave it at that here.\nNaming Convention:\n\n“lter / lterwg-&lt;abbreviated-group-name&gt;\"\n“lter / lter-sparc-&lt;abbreviated-group-name&gt;\"\n\nNote that the group abbreviation should be entirely lowercase.\nOther Notes: Create the repository in the LTER GitHub organization and use the working group template repository as the starting point. Add any usernames that you have from the group to this repository.\n\n\nDon’t do this unless it is needed! The server is another account/login/workflow group members have to keep in mind. If you make this part of the group’s intitial infrastructure, you risk overwhelming them for the really vital stuff when the server is not always necessary for all projects.\nIf it is needed, contact Thomas Hetmank or Nick Outin to get a ‘team’ set up on Aurora for the working group."
  },
  {
    "objectID": "internal/new-wg-setup.html#offer-scicomp-specific-onboarding",
    "href": "internal/new-wg-setup.html#offer-scicomp-specific-onboarding",
    "title": "Working Group Setup Checklist",
    "section": "3. Offer SciComp-Specific Onboarding",
    "text": "3. Offer SciComp-Specific Onboarding\nAfter the group has done the LNO onboarding, schedule a time to do SciComp specific onboarding. You’ll discuss the types of support the SciComp team can offer as well as the workshops we’re currently capable of offering. You’ll share the infrastructure you’ve made with the group.\nCritically, ask the group if they have any tasks on their collective mind already!\nFinally, if this onboarding meeting is for a SPARC group, reiterate that they get one (1) year of SciComp support but they choose when to start that clock. Some groups use the entire year before their in-person meeting while others don’t ask for support until after that meeting."
  },
  {
    "objectID": "modules_tutorials/github-connect.html",
    "href": "modules_tutorials/github-connect.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "The following steps include a sequence of command line operations that will be relayed in code chunks below. Unless otherwise stated, all of the following code should be run in “Terminal”.\nIf you didn’t check the “Create a git repository” button while creating the R project, you’ll need to do that via the command line now. If you did check that box, you should skip this step!\n\n# Start a git repository on the \"main\" branch\ngit init -b main\n\nStage all of the files in your project to the git repository. This includes the .yml file, all .qmd files and all of their rendered versions created when you ran quarto render earlier. This code is equivalent to checking the box for the files in the “Git” pane of RStudio.\n\n# Stage all files\ngit add .\n\nOnce everything has been staged, you now must commit those staged files with a message.\n\n# Commit all files with the message in quotes\ngit commit -m \"Initial commit\"\n\nNow that your project files have been committed, you need to tell your computer where you will be pushing to and pulling from. Paste the link you copied at the end of the “Make a New GitHub Repository” into the code shown in the chunk below (instead of GITHUB_URL) and run it.\n\n# Tell your computer which GitHub repository to connect to\ngit remote add origin GITHUB_URL\n\nVerify that URL before continuing.\n\n# Confirm that URL worked\ngit remote -v\n\nFinally, push your committed changes to the repository that you set as the remote in the preceding two steps.\n\n# Push all of the content to the main branch\ngit push -u origin main\n\nNow, go back to GitHub and refresh the page to see your project content safe and sound in your new GitHub repository!"
  },
  {
    "objectID": "modules_tutorials/github-website-deploy.html",
    "href": "modules_tutorials/github-website-deploy.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "In order to get your new website actually on the web, we’ll need to tell GitHub that we want our website to be accessible at a .github.io URL.\nTo do this, go to the “Settings” tab with a gear icon and click it. You may be prompted to re-enter your GitHub password, do so and you can proceed.\n\nIn the resulting page, look towards the bottom of the left sidebar of settings categories and click the “Pages” option. This is at the very bottom of the sidebar in the screen capture below but is towards the middle of all of the settings categories Github offers you.\n\nScroll down to the middle of this page and where it says “Branch” click the dropdown menu that says “None” by default.\n\nSelect “main” from the dropdown.\n\n\n\nThis opens up a new dropdown menu where you can select which folder in your repository contains your website’s content (it defaults to “/ (root)”). Because we specified output-dir: docs in the .yml file earlier we can select “/docs” from the dropdown menu.\n\n\n\nOnce you’ve told GitHub that you want a website generated from the “docs” folder on the main branch, click the “Save” button.\n\n\n\nFrom this moment your website has begun being deployed by GitHub! You can check the status of the building process by navigating to the “Actions” tab of your repository.\nSelect the “pages build and deployment workflow” in the list of workflows on the bottom righthand side of the page.\n\nThis shows you GitHub’s building and deployment process as a flowchart. While it is working on each step there will be an amber circle next to the name of that sub-task. When a sub-task is completed, the amber circle becomes a green circle with a check mark.\n\nWhen the three steps are complete the amber clock symbol next to the “pages build and deployment” action will turn into a larger green circle with a check mark. This is GitHub’s way of telling you that your website is live and accessible to anyone on the internet.\n\nYou can now visit your website by visiting its dedicated URL. This URL can be found by returning to the “Settings” tab and then scrolling through the sidebar to the “Pages” section.\nAlternately, the website for your repository always uses the following composition: https://repository owner.github.io/repository name/\n\nIf we visit that link, we can see that our website is live!\n\n\nGitHub Housekeeping\nWe recommend a quick housekeeping step now to make it easier to find this URL in the future. Copy the URL from the Pages setting area and return to the “Code” tab of the repository.\nOnce there, click the small gear icon to the right of the “About” header.\n\nIn the resulting window, paste the copied URL into the “Website” field. Once you’ve pasted it in, click the green “Save changes” button.\n\n\n\nThis places the link to your deployed website in an intuitive, easy-to-find location both for interested third parties and yourself in the future."
  },
  {
    "objectID": "modules_tutorials/googledrive-fxns.html",
    "href": "modules_tutorials/googledrive-fxns.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "Now that you’ve authorized the googledrive package, you can start downloading the Google Drive files you need through R! Let’s say that you want to download a csv file from a folder or shared drive. You can save the URL of that folder/shared drive to a variable.\nThe googledrive package makes it straightforward to access Drive folders and files with the as_id function. This function allows the full link to a file or folder to serve as a direct connection to that file/folder. Most of the other googledrive functions will require a URL that is wrapped with as_id in this way. You would replace “your url here” with the actual link but make sure it is in quotation marks.\n\ndrive_url &lt;- googledrive::as_id(\"your url here\")\n\nTo list all the contents of this folder, we can use the drive_ls function. You will get a dataframe-like object of the files back as the output. An example is shown below in the screenshot. Here, this Google Drive folder contains 4 csv files: ingredients.csv, favorite_soups.csv, favorite_fruits.csv and favorite_desserts.csv\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url)\ndrive_folder\n\n\n\n\nIf it has been a while since you’ve used googledrive, it will prompt you to refresh your token. Simply enter the number that corresponds to the correct Google Drive account.\n\n\n\nIf you only want to list files of a certain type, you can specify this in the type argument. And let’s say that my folder contains a bunch of csv files, but I only want to download the one named “favorite_desserts.csv”. In that case, I can also put a matching string in the pattern argument in order to filter down to 1 file.\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url,\n                                      type = \"csv\", \n                                      pattern = \"favorite_desserts\")\ndrive_folder\n\n\n\n\nOnce we’ve narrowed down to the file we want, we can download it using drive_download. This function takes the file identifier as an argument so we can access it using drive_folder$id.\n\ngoogledrive::drive_download(file = drive_folder$id)\n\nThis will automatically download the file to our working directory. If you want, you can specify a different path to download to. Just put the new file path into the path argument, replacing the “your path here”, but keep the quotation marks.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\")\n\nIf you’ve downloaded the file before, and you want to overwrite it, there’s a handy overwrite argument that you can set to TRUE. Note that the default is FALSE.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\",\n                            overwrite = T)\n\nIf there are multiple files in the Drive folder and you want to download them all, you can use a loop like so:\n\n# For each file:\nfor(focal_file in drive_folder$name){\n  \n  # Find the file identifier for that file\n  file_id &lt;- subset(drive_folder, name == focal_file)\n\n  # Download that file\n  drive_download(file = file_id$id, \n                 path = \"your path here\",\n                 overwrite = T)\n}"
  },
  {
    "objectID": "tip_github-multi-prods.html",
    "href": "tip_github-multi-prods.html",
    "title": "Using GitHub for Multiple Products",
    "section": "",
    "text": "Working groups typically begin by harmonizing and wrangling a sizable synthesis database that they can then use to answer their scientific questions. While that effort often takes the entire group, once it is done, smaller groups are then capable of pursuing research questions separately. For groups that store their work in GitHub, this raises the question: how do we keep working together in GitHub as we pursue multiple semi-independent products?\nContinue reading this page for some methods that the Scientific Computing team has seen work for past working groups."
  },
  {
    "objectID": "tip_github-multi-prods.html#overview",
    "href": "tip_github-multi-prods.html#overview",
    "title": "Using GitHub for Multiple Products",
    "section": "",
    "text": "Working groups typically begin by harmonizing and wrangling a sizable synthesis database that they can then use to answer their scientific questions. While that effort often takes the entire group, once it is done, smaller groups are then capable of pursuing research questions separately. For groups that store their work in GitHub, this raises the question: how do we keep working together in GitHub as we pursue multiple semi-independent products?\nContinue reading this page for some methods that the Scientific Computing team has seen work for past working groups."
  },
  {
    "objectID": "tip_github-multi-prods.html#option-1-multiple-github-repositories",
    "href": "tip_github-multi-prods.html#option-1-multiple-github-repositories",
    "title": "Using GitHub for Multiple Products",
    "section": "Option 1: Multiple GitHub Repositories",
    "text": "Option 1: Multiple GitHub Repositories\nIf you know you’re pursuing multiple separate products, it can sometimes be simplest to use multiple GitHub repositories! There’s nothing inherently wrong with this approach despite the fact that it may feel like an overly simple solution.\n\nStrengthsPotential WeaknessesExamples\n\n\nMore clarity about where specific files should be stored\nAny given file should be stored in the respective GitHub repository for that product. If a file touches multiple products (e.g., extracting spatial covariates for all sites in the synthesis database), a new repository for support scripts can be created\nPairs well with the GitHub-Zenodo connection\nGitHub and Zenodo have a really nice integration that allows particular repositories to be cited (with a DOI!) on Zenodo. GitHub has nice documentation on this but essentially this means that when a given product is nearly complete, you could get a formal citation for the corresponding GitHub repository and cite / publicize only the code that supports that specific product.\nDon’t need to reorganize first repository\nIf subsequent products get their own repositories you won’t need to mess with the structure or content of your first repository. If you needed to be clear about which product a given file supports but all files lived in the same repository you’d likely have to change some facets of your approach to make sure it was clear to ‘future you’ and people outside of your team.\nChance to let particular team members shine\nAnyone in your team could be the owner of the repository for a particular product so if you use separate repos for separate products you could use the fact of that ownership to maximize the professional benefits of each product for each team member. For instance, if a member of your group is an early career scientist, it might be a nice accolade to be the owner of a very active, collaborative, and reproducible GitHub repository.\n\n\nReduced findability\nUnless you take steps to prevent it, creating multiple repositories can make it difficult to find a particular repository (especially if different users/organizations own each).\nManaging access becomes more difficult\nGenerally, users must be invited separately to each repository. This invite expires after 48 hours if not accepted and then must be re-sent. Also, typically only the repository owner (or a sufficiently empowered collaborator) can send that invite in the first place. There is a notable exception when all repositories are owned by the same organization (see GitHub’s documentation on “teams”) but that requires an organization and some amount of preparation with an organization administrator.\n\n\nThe Silica Synthesis working group (2020) adopted this ‘multiple repositories’ approach to great success. They began with a single giant repository before deciding that was sufficiently unwieldy that the benefits of reorganization outweighed the time cost of that pivot.\nThey then created a standalone ‘data’ repository for all of their data harmonization and wrangling (see lter / lterwg-silica-data) and then created a satellite of repositories owned by various users / organizations where each was dedicated to a particular product (typically peer-reviewed publications).\nTo make sure that these would remain findable, they added a small section to the end of each GitHub README that linked to all of the other repositories. See one such README here and scroll to the bottom."
  },
  {
    "objectID": "tip_github-multi-prods.html#option-2-one-repository-to-rule-them-all",
    "href": "tip_github-multi-prods.html#option-2-one-repository-to-rule-them-all",
    "title": "Using GitHub for Multiple Products",
    "section": "Option 2: One Repository to Rule Them All",
    "text": "Option 2: One Repository to Rule Them All\nIf you’re worried about decentralization, you can also live in a single GitHub repository and make extensive use of sub-folders to silo separate products.\n\nStrengthsPotential WeaknessesExamples\n\n\nAll work is centralized and findable\nIf you bookmark (or “star”) the GitHub repository for your group, you know that link will direct you to where all of your group’s content lives.\nManaging access is straightforward\nAgain, if all of your group’s content lives in one place, it is straightforward to modify access to that repository. Note however that granting or removing access then becomes ‘all or nothing’ which may be a drawback in some contexts.\n\n\nDifficult to manage content\nPlacing all content in the same repository means that you need to be very careful about separating files that support a particular product–or at least indicating which product(s) they support. GitHub will not require any architecture on this front so it will be up to your team to create and maintain sufficient documentation to silo content.\nMust publicize all project files to share any\nIf you want to cite or otherwise publicize code for one product, you’ll need to share the code for everything. At its coarsest level, this means your entire repository must be public (which is not necessarily a bad thing!) for you to cite anything. Additionally, if you cite code in a paper you’d need to direct interested readers to the appropriate part of your repository to find just the files that support that product.\nLikely need to reorganize the first repository\nThis may not necessarily be the case but if you develop the first repository as if it is a standalone project (e.g., limited use of sub-folders, generic naming convention, etc.) you will likely need to reorganize all of those files–and any related documentation–so that you can create the structure you’ll need to support multiple products.\n\n\nThe Marine Consumer Nutrient Dynamics working group (2023) started with this approach for their data harmonization scripts. In their first repository (see lter / lterwg-marine-cnd), they created a folder titled “scripts-harmonization” and placed all scripts related to the creation of their synthesis dataset there. This was in preparation for additional sub-folders dedicated to other, specific products.\nHowever, after using the GitHub-Zenodo integration to cite their code, they realized that (1) citing their code for specific products would be difficult and (2) using separate repositories would enable them to give more credit to early career members of their group (by letting those individuals own subsequent repositories).\nSubsequently they have used separate repositories for separate products but the structure of their first repo gives insight into what a centralized repo might look like at its simplest."
  },
  {
    "objectID": "tip_notebook-vs-script.html",
    "href": "tip_notebook-vs-script.html",
    "title": "Notebooks versus Scripts",
    "section": "",
    "text": "When writing code, either scripts (e.g., .R, .py, .sh, etc.) or notebook files (e.g., .Rmd, .ipynb, .qmd, etc.) can be viable options but they have different advantages and disadvantages that we will cover below."
  },
  {
    "objectID": "tip_notebook-vs-script.html#scripts",
    "href": "tip_notebook-vs-script.html#scripts",
    "title": "Notebooks versus Scripts",
    "section": "Scripts",
    "text": "Scripts\n\nStrengthsPotential Weaknesses\n\n\nThe greatest strength of scripts is their flexibility. They allow you to format a file in whatever way is most intuitive to you. Additionally, scripts can be cleaner for iterative operations (i.e., for loops) insofar as they need not be concerned with staying within a given code chunk (as would be the case for a notebook file). Developing a new workflow can be swiftly accomplished in a script as some or all of the code in a script can be run by simply selecting the desired lines rather than manually running the desired chunks in a notebook file. Finally, scripts can also be a better home for custom functions that can be sourced by another file (even by a notebook!) for making repeated operations simpler to read.\n\n\nThe benefit of extreme flexibility in scripts can sometimes be a disadvantage however. We’ve all seen (and written) scripts that have few or no comments or where lines of code are densely packed without spacing or blank lines to help someone new to the code understand what is being done. Scripts can certainly be written in a way that is accessible to those without prior knowledge of what the script accomplishes but they do not enforce such structure. This can make it easy–especially when we’re feeling pressed for time–to exclude structure that helps our code remain reproducible and understandable."
  },
  {
    "objectID": "tip_notebook-vs-script.html#notebooks",
    "href": "tip_notebook-vs-script.html#notebooks",
    "title": "Notebooks versus Scripts",
    "section": "Notebooks",
    "text": "Notebooks\n\nStrengthsPotential Weaknesses\n\n\nNotebook files’ ability to “knit” or “rendered” as HTML or PDF documents makes them extremely useful in creating outward-facing reports. This is particularly the case when the specific code is less important to communicate than visualizations and/or analyses of the data but notebook files do facilitate echoing the code so that report readers can see how background operations were accomplished. The code chunk structure of these files can also nudge users towards including valuable comments (both between chunks and within them) though of course notebook files do not enforce such non-code content.\n\n\nNotebook files can fail to knit/render due to issues even when the code within the chunks works as desired. Duplicate code chunk names or a failure to install LaTeX (or related support software) can be a frustrating hurdle to overcome between functioning code and a rendered output file. When code must be re-run repeatedly (as is often the case when developing a new workflow) the stop-and-start nature of running each code chunk separately can also be a small irritation."
  },
  {
    "objectID": "tip_notebook-vs-script.html#script-vs.-notebook-summary",
    "href": "tip_notebook-vs-script.html#script-vs.-notebook-summary",
    "title": "Notebooks versus Scripts",
    "section": "Script vs. Notebook Summary",
    "text": "Script vs. Notebook Summary\nTaken together, both scripts and notebooks can empower users to write reproducible, transparent code. However, both file types have some key limitations that should be taken into consideration when choosing which to use as you set out to create a new code product."
  },
  {
    "objectID": "tip_packages.html",
    "href": "tip_packages.html",
    "title": "Streamlined R Package Loading",
    "section": "",
    "text": "Loading packages / libraries in R can be cumbersome when working collaboratively because there is no guarantee that you all have the same packages installed. While you could comment-out an install.packages() line for every package you need for a given script, we recommend using the R package librarian to greatly simplify this process!\nlibrarian::shelf() accepts the names of all of the packages–either CRAN or GitHub–installs those that are missing in that particular R session and then attaches all of them."
  },
  {
    "objectID": "tip_packages.html#overview",
    "href": "tip_packages.html#overview",
    "title": "Streamlined R Package Loading",
    "section": "",
    "text": "Loading packages / libraries in R can be cumbersome when working collaboratively because there is no guarantee that you all have the same packages installed. While you could comment-out an install.packages() line for every package you need for a given script, we recommend using the R package librarian to greatly simplify this process!\nlibrarian::shelf() accepts the names of all of the packages–either CRAN or GitHub–installs those that are missing in that particular R session and then attaches all of them."
  },
  {
    "objectID": "tip_packages.html#traditional-package-loading",
    "href": "tip_packages.html#traditional-package-loading",
    "title": "Streamlined R Package Loading",
    "section": "Traditional Package Loading",
    "text": "Traditional Package Loading\nTo load packages typically you’d have something like the following in your script:\n\n# Install packages (if needed)\ninstall.packages(\"tidyverse\")\ninstall.packages(\"devtools\")\ndevtools::install_github(\"NCEAS/scicomptools\")\n\n# Load libraries\nlibrary(tidyverse); library(scicomptools)"
  },
  {
    "objectID": "tip_packages.html#package-loading-with-librarian",
    "href": "tip_packages.html#package-loading-with-librarian",
    "title": "Streamlined R Package Loading",
    "section": "Package Loading with librarian",
    "text": "Package Loading with librarian\nWith librarian::shelf() however this becomes much cleaner! In addition to being fewer lines, using librarian also removes the possibility that someone running your code misses one of the packages that your script depends on and then the script breaks for them later on. librarian::shelf() automatically detects whether a package is installed, installs it if necessary, and then attaches the package.\nIn essence, librarian::shelf() wraps install.packages(), devtools::install_github(), and library() into a single, human-readable function.\n\n# Install and load packages!\nlibrarian::shelf(tidyverse, NCEAS/scicomptools)\n\nWhen using librarian::shelf(), package names do not need to be quoted and GitHub packages can be installed without the additional steps of installing the devtools package and using devtools::install_github() instead of install.packages()."
  },
  {
    "objectID": "tutorial_googledrive-pkg.html",
    "href": "tutorial_googledrive-pkg.html",
    "title": "Connecting R & Google Drive",
    "section": "",
    "text": "The googledrive R package is a package that lets R users directly interact with files on GoogleDrive. This can be extremely useful because it lets all members of a team share the same source data file(s) and guarantees that updates to “living” documents are received by all group members the next time they run their R script. This package is technically part of the Tidyverse but is not loaded by running library(tidyverse).\nBecause this package requires access to an R user’s GoogleDrive, you must “authenticate” the googledrive package. This essentially tells Google that it is okay if an R package uses your credentials to access and (potentially) modify your Drive content. There are only a few steps to this process but follow along with the below tutorial and we’ll get you ready to integrate the Google Drive into your code workflows using the googledrive package in no time!\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nCreate a Gmail account\n\nFeel free to skip any steps that you have already completed!\n\n\n\nIn order to connect R with a GoogleDrive, we’ll need to authorize googledrive to act on our behalf. This only needs to be done once (per computer) so follow along and you’ll be building GoogleDrive into your workflows in no time!\nFirst, install the googledrive and httpuv R packages. The googledrive package’s need is self-evident while the httpuv package makes the following steps a little easier than googledrive makes it alone. Be sure to load the googledrive package after you install it!\n\n# Install packages\ninstall.packages(c(\"googledrive\", \"httpuv\"))\n\n# Load them\nlibrary(googledrive)\n\nOnce you’ve installed the packages we can begin the authentication in R using the drive_auth function in the googledrive package.\n\ngoogledrive::drive_auth(email = \"enter your gmail here!\")\n\nIf this is your first time using googledrive, drive_auth will kick you to a new tab of your browser (see below for a screen grab of that screen) where you can pick which Gmail you’d like to connect to R.\n\n\n\nClick the Gmail you want to use and you will get a second screen where Google tells you that “Tidyverse API” wants access to your Google Account. This message is followed by three checkboxes, the first two are grayed out but the third is unchecked.\n\n\n\n\n\n\n\n\n\nImportantNOTE\n\n\n\nThis next bit is vitally important so carefully read and follow the next instruction!\n\n\nIn this screen, you must check the unchecked box to be able to use the googledrive R package. If you do not check this box all attempts to use googledrive functions will get an error that says “insufficient permissions”.\n\n\n\nWhile granting access to “see, edit, create, and”delete” all of your Google Drive files” sounds like a significant security risk, those powers are actually why you’re using the googledrive package in the first place! You want to be able to download existing Drive files, change them in R on your computer, and then put them back in Google Drive which is exactly what is meant by “see, edit, create, and delete”.\nAlso, this power only applies to the computer you’re currently working on! Granting access on your work computer allows only that computer to access your Drive files. So don’t worry about giving access to your Drive to the whole world, that is protected by the same failsafes that you use when you let your computer remember a password to a website you frequent.\nAfter you’ve checked the authorization box, scroll down and click the “Continue” button.\n\n\n\nThis should result in a plain text page that tells you to close this window and return to R. If you see this message you are ready to use the googledrive package!\n\n\n\n\n\n\nIf you have tried to use drive_auth and did not check the box indicated above, you need to make the googledrive package ask you again. Using drive_auth will not (annoyingly) return you to the place it sent you the first time. However, if you run the following code chunk it should give you another chance to check the needed box.\nThe gargle R package referenced below is required for interacting with Google Application Program Interfaces (APIs). This package does the heavy lifting of secure password and token management and is necessary for the googledrive authentication chunk below.\n\ngoogledrive::drive_auth(\n  email = gargle::gargle_oauth_email(),\n  path = NULL,\n  scopes = \"https://www.googleapis.com/auth/drive\",\n  cache = gargle::gargle_oauth_cache(),\n  use_oob = gargle::gargle_oob_default(),\n  token = NULL)\n\nUnfortunately, to use the googledrive package you must check the box that empowers the package to function as designed. If you’re uncomfortable giving the googledrive that much power you will need to pivot your workflow away from using GoogleDrive directly. However, NCEAS does offer access to an internal server called “Aurora” where data can be securely saved and shared among group members without special authentication like what googledrive requires. Reach out to our team if this seems like a more attractive option for your working group and we can offer training on how to use this powerful tool!\n\n\n\nNow that you’ve authorized the googledrive package, you can start downloading the Google Drive files you need through R! Let’s say that you want to download a csv file from a folder or shared drive. You can save the URL of that folder/shared drive to a variable.\nThe googledrive package makes it straightforward to access Drive folders and files with the as_id function. This function allows the full link to a file or folder to serve as a direct connection to that file/folder. Most of the other googledrive functions will require a URL that is wrapped with as_id in this way. You would replace “your url here” with the actual link but make sure it is in quotation marks.\n\ndrive_url &lt;- googledrive::as_id(\"your url here\")\n\nTo list all the contents of this folder, we can use the drive_ls function. You will get a dataframe-like object of the files back as the output. An example is shown below in the screenshot. Here, this Google Drive folder contains 4 csv files: ingredients.csv, favorite_soups.csv, favorite_fruits.csv and favorite_desserts.csv\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url)\ndrive_folder\n\n\n\n\nIf it has been a while since you’ve used googledrive, it will prompt you to refresh your token. Simply enter the number that corresponds to the correct Google Drive account.\n\n\n\nIf you only want to list files of a certain type, you can specify this in the type argument. And let’s say that my folder contains a bunch of csv files, but I only want to download the one named “favorite_desserts.csv”. In that case, I can also put a matching string in the pattern argument in order to filter down to 1 file.\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url,\n                                      type = \"csv\", \n                                      pattern = \"favorite_desserts\")\ndrive_folder\n\n\n\n\nOnce we’ve narrowed down to the file we want, we can download it using drive_download. This function takes the file identifier as an argument so we can access it using drive_folder$id.\n\ngoogledrive::drive_download(file = drive_folder$id)\n\nThis will automatically download the file to our working directory. If you want, you can specify a different path to download to. Just put the new file path into the path argument, replacing the “your path here”, but keep the quotation marks.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\")\n\nIf you’ve downloaded the file before, and you want to overwrite it, there’s a handy overwrite argument that you can set to TRUE. Note that the default is FALSE.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\",\n                            overwrite = T)\n\nIf there are multiple files in the Drive folder and you want to download them all, you can use a loop like so:\n\n# For each file:\nfor(focal_file in drive_folder$name){\n  \n  # Find the file identifier for that file\n  file_id &lt;- subset(drive_folder, name == focal_file)\n\n  # Download that file\n  drive_download(file = file_id$id, \n                 path = \"your path here\",\n                 overwrite = T)\n}"
  },
  {
    "objectID": "tutorial_googledrive-pkg.html#overview",
    "href": "tutorial_googledrive-pkg.html#overview",
    "title": "Connecting R & Google Drive",
    "section": "",
    "text": "The googledrive R package is a package that lets R users directly interact with files on GoogleDrive. This can be extremely useful because it lets all members of a team share the same source data file(s) and guarantees that updates to “living” documents are received by all group members the next time they run their R script. This package is technically part of the Tidyverse but is not loaded by running library(tidyverse).\nBecause this package requires access to an R user’s GoogleDrive, you must “authenticate” the googledrive package. This essentially tells Google that it is okay if an R package uses your credentials to access and (potentially) modify your Drive content. There are only a few steps to this process but follow along with the below tutorial and we’ll get you ready to integrate the Google Drive into your code workflows using the googledrive package in no time!\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nCreate a Gmail account\n\nFeel free to skip any steps that you have already completed!\n\n\n\nIn order to connect R with a GoogleDrive, we’ll need to authorize googledrive to act on our behalf. This only needs to be done once (per computer) so follow along and you’ll be building GoogleDrive into your workflows in no time!\nFirst, install the googledrive and httpuv R packages. The googledrive package’s need is self-evident while the httpuv package makes the following steps a little easier than googledrive makes it alone. Be sure to load the googledrive package after you install it!\n\n# Install packages\ninstall.packages(c(\"googledrive\", \"httpuv\"))\n\n# Load them\nlibrary(googledrive)\n\nOnce you’ve installed the packages we can begin the authentication in R using the drive_auth function in the googledrive package.\n\ngoogledrive::drive_auth(email = \"enter your gmail here!\")\n\nIf this is your first time using googledrive, drive_auth will kick you to a new tab of your browser (see below for a screen grab of that screen) where you can pick which Gmail you’d like to connect to R.\n\n\n\nClick the Gmail you want to use and you will get a second screen where Google tells you that “Tidyverse API” wants access to your Google Account. This message is followed by three checkboxes, the first two are grayed out but the third is unchecked.\n\n\n\n\n\n\n\n\n\nImportantNOTE\n\n\n\nThis next bit is vitally important so carefully read and follow the next instruction!\n\n\nIn this screen, you must check the unchecked box to be able to use the googledrive R package. If you do not check this box all attempts to use googledrive functions will get an error that says “insufficient permissions”.\n\n\n\nWhile granting access to “see, edit, create, and”delete” all of your Google Drive files” sounds like a significant security risk, those powers are actually why you’re using the googledrive package in the first place! You want to be able to download existing Drive files, change them in R on your computer, and then put them back in Google Drive which is exactly what is meant by “see, edit, create, and delete”.\nAlso, this power only applies to the computer you’re currently working on! Granting access on your work computer allows only that computer to access your Drive files. So don’t worry about giving access to your Drive to the whole world, that is protected by the same failsafes that you use when you let your computer remember a password to a website you frequent.\nAfter you’ve checked the authorization box, scroll down and click the “Continue” button.\n\n\n\nThis should result in a plain text page that tells you to close this window and return to R. If you see this message you are ready to use the googledrive package!\n\n\n\n\n\n\nIf you have tried to use drive_auth and did not check the box indicated above, you need to make the googledrive package ask you again. Using drive_auth will not (annoyingly) return you to the place it sent you the first time. However, if you run the following code chunk it should give you another chance to check the needed box.\nThe gargle R package referenced below is required for interacting with Google Application Program Interfaces (APIs). This package does the heavy lifting of secure password and token management and is necessary for the googledrive authentication chunk below.\n\ngoogledrive::drive_auth(\n  email = gargle::gargle_oauth_email(),\n  path = NULL,\n  scopes = \"https://www.googleapis.com/auth/drive\",\n  cache = gargle::gargle_oauth_cache(),\n  use_oob = gargle::gargle_oob_default(),\n  token = NULL)\n\nUnfortunately, to use the googledrive package you must check the box that empowers the package to function as designed. If you’re uncomfortable giving the googledrive that much power you will need to pivot your workflow away from using GoogleDrive directly. However, NCEAS does offer access to an internal server called “Aurora” where data can be securely saved and shared among group members without special authentication like what googledrive requires. Reach out to our team if this seems like a more attractive option for your working group and we can offer training on how to use this powerful tool!\n\n\n\nNow that you’ve authorized the googledrive package, you can start downloading the Google Drive files you need through R! Let’s say that you want to download a csv file from a folder or shared drive. You can save the URL of that folder/shared drive to a variable.\nThe googledrive package makes it straightforward to access Drive folders and files with the as_id function. This function allows the full link to a file or folder to serve as a direct connection to that file/folder. Most of the other googledrive functions will require a URL that is wrapped with as_id in this way. You would replace “your url here” with the actual link but make sure it is in quotation marks.\n\ndrive_url &lt;- googledrive::as_id(\"your url here\")\n\nTo list all the contents of this folder, we can use the drive_ls function. You will get a dataframe-like object of the files back as the output. An example is shown below in the screenshot. Here, this Google Drive folder contains 4 csv files: ingredients.csv, favorite_soups.csv, favorite_fruits.csv and favorite_desserts.csv\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url)\ndrive_folder\n\n\n\n\nIf it has been a while since you’ve used googledrive, it will prompt you to refresh your token. Simply enter the number that corresponds to the correct Google Drive account.\n\n\n\nIf you only want to list files of a certain type, you can specify this in the type argument. And let’s say that my folder contains a bunch of csv files, but I only want to download the one named “favorite_desserts.csv”. In that case, I can also put a matching string in the pattern argument in order to filter down to 1 file.\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url,\n                                      type = \"csv\", \n                                      pattern = \"favorite_desserts\")\ndrive_folder\n\n\n\n\nOnce we’ve narrowed down to the file we want, we can download it using drive_download. This function takes the file identifier as an argument so we can access it using drive_folder$id.\n\ngoogledrive::drive_download(file = drive_folder$id)\n\nThis will automatically download the file to our working directory. If you want, you can specify a different path to download to. Just put the new file path into the path argument, replacing the “your path here”, but keep the quotation marks.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\")\n\nIf you’ve downloaded the file before, and you want to overwrite it, there’s a handy overwrite argument that you can set to TRUE. Note that the default is FALSE.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\",\n                            overwrite = T)\n\nIf there are multiple files in the Drive folder and you want to download them all, you can use a loop like so:\n\n# For each file:\nfor(focal_file in drive_folder$name){\n  \n  # Find the file identifier for that file\n  file_id &lt;- subset(drive_folder, name == focal_file)\n\n  # Download that file\n  drive_download(file = file_id$id, \n                 path = \"your path here\",\n                 overwrite = T)\n}"
  },
  {
    "objectID": "wg_facilitation.html",
    "href": "wg_facilitation.html",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "",
    "text": "Facilitating a working group meeting is both hugely rewarding and a significant effort. NCEAS and the LTER Network have gathered some guiding resources that may prove to be helpful to your meeting. See the sub-sections below for more details."
  },
  {
    "objectID": "wg_facilitation.html#lter-network-office-guidance",
    "href": "wg_facilitation.html#lter-network-office-guidance",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "LTER Network Office Guidance",
    "text": "LTER Network Office Guidance\nThe LTER Network Office (LNO) has curated some useful resources on their website (lternet.edu) that we strongly recommend exploring! In particular, the LNO has centralized resources for working group primary investigators (PIs) (see here). Additionally, the LNO has created some broader resources for all members of working groups (see here).\nThe LTER Code of Conduct is also publicly available if you’d like to explore it."
  },
  {
    "objectID": "wg_facilitation.html#nceas-resources-for-working-groups",
    "href": "wg_facilitation.html#nceas-resources-for-working-groups",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "NCEAS’ Resources for Working Groups",
    "text": "NCEAS’ Resources for Working Groups\nNCEAS hosts many working groups and so has drawn from its wealth of experience to assemble a set of useful information on its Resources for Working Groups page.\nIn particular, we recommend checking out the documents nested in the “How to Run a Working Group” dropdown menu. Your group may also want to check out the “Reimbursement Forms” dropdown after your meetings in Santa Barbara to ensure that your eligible expenses get reimbursed!\nAs of fall 2024, NCEAS has also begun developing a working group training series which contains a ton of valuable information both on facilitation best practices and on data management and reproducibility."
  },
  {
    "objectID": "wg_facilitation.html#diversity-equity-and-inclusion",
    "href": "wg_facilitation.html#diversity-equity-and-inclusion",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "Diversity, Equity, and Inclusion",
    "text": "Diversity, Equity, and Inclusion\nThe LTER Network and NCEAS are committed to improving diversity and inclusion in science. For the LTER Network Office’s diversity, equity and inclusion resources see here. For more information on NCEAS’ commitment to this mission–and related links–please visit NCEAS’ DEIJ page."
  },
  {
    "objectID": "wg_facilitation.html#visiting-santa-barbara",
    "href": "wg_facilitation.html#visiting-santa-barbara",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "Visiting Santa Barbara",
    "text": "Visiting Santa Barbara\nWhen your group is visiting at NCEAS’ office in beautiful Santa Barbara there are additional resources available to your group and opportunities for recreation and bonding as a team! Check out NCEAS’ general computing resources page for a quick rundown of some resources available only in Santa Barbara (including how to get on the WiFi!).\nWe very much recommend taking an afternoon or morning on one of the days of your visit to get out and explore Santa Barbara! NCEAS’ page for things to do in SB for new employees might prove to be a helpful resource for you and your group as you’re deciding where to eat and what to do!"
  },
  {
    "objectID": "wg_team-coding.html",
    "href": "wg_team-coding.html",
    "title": "Team Coding: 5 Essentials",
    "section": "",
    "text": "Have you ever had trouble running someone else’s code or re-running your own old code? Working in a synthesis group can bring up challenges like these as members try to run workflows written by others or written by themselves at the last in-person meeting months ago. To make the process easier and more reproducible, here is a list of our top 5 best practices to follow as you collaborate on scripts that address your group’s scientific questions. These are merely suggestions but we hope that they help facilitate seamless synthesis science!"
  },
  {
    "objectID": "wg_team-coding.html#prioritize-future-you",
    "href": "wg_team-coding.html#prioritize-future-you",
    "title": "Team Coding: 5 Essentials",
    "section": "1. Prioritize ‘Future You’",
    "text": "1. Prioritize ‘Future You’\nIf something takes more time now but will work better in the long run, invest the time now to save yourself heartache in the future. This can apply to taking the time to make a document listing the scripts in a given workflow, adding descriptive comments in an existing script, and many other contexts. By investing this time now, you will save ‘future you’ from unnecessary labor."
  },
  {
    "objectID": "wg_team-coding.html#always-leave-comments",
    "href": "wg_team-coding.html#always-leave-comments",
    "title": "Team Coding: 5 Essentials",
    "section": "2. Always Leave Comments",
    "text": "2. Always Leave Comments\nLeave enough comments in your code so that other members of your team (and ‘future you’!) can understand what your script does. This is a crucial habit that will benefit you immensely. By making your code more human-readable, you open the door for improved communication among team members. This makes it easier for people who weren’t involved in your workflow to jump in and give feedback if necessary or to onboard new team members who join later in the project. Plus, it is less of a hassle to edit and maintain well-commented code in the future; you can make changes without spending too much time deciphering each line of code."
  },
  {
    "objectID": "wg_team-coding.html#use-relative-file-paths",
    "href": "wg_team-coding.html#use-relative-file-paths",
    "title": "Team Coding: 5 Essentials",
    "section": "3. Use Relative File Paths",
    "text": "3. Use Relative File Paths\nWhen coding collaboratively, accounting for the difference between your folder structure and those of your colleagues becomes critical. For example, if you read in a data file using its absolute file path (e.g. “/users/my_name/documents/project/data/raw_data/example.csv”), only you will be able to successfully run that line of code and–by extension–your entire script! Also, the slashes between folder names are different depending on each person’s computer operating system, which means even a relative file path will only work for your teammates that share your computer brand.\nIf you’re an R user, there are two quick things you can do in your code to avoid these problems:\n\nRelative Paths – Use the dir.create function in your script to create any necessary folders.\nNeed a data folder? Use dir.create(\"data\") and you’ll create an empty data folder. Anyone else running your code will create the same folder and you can safely assume that part of the file path going forward.\n\n\nOperating System Differences – Use the file.path function with folder names without slashes.\nReading in data? Use file.path(\"data\", \"raw_data\", \"site_a.csv\") and file.path will automatically sense the computer’s operating system and insert the correct slashes for each user.\nFor example, if you are already working in the directory called “project”, then you can access example.csv using this relative file path: data/raw_data/example.csv. You can improve beyond even that by using the file.path function to automatically detect the computer operating system and insert the correct slash for you and anyone else running the code. We recommend using this function and assigning your file path to an object so you can use it anytime.\n\nmy_path &lt;- file.path(\"data\", \"raw_data\")\nmy_raw_data &lt;- read.csv(file = file.path(my_path, \"example.csv\"))"
  },
  {
    "objectID": "wg_team-coding.html#store-raw-data-in-the-cloud",
    "href": "wg_team-coding.html#store-raw-data-in-the-cloud",
    "title": "Team Coding: 5 Essentials",
    "section": "4. Store Raw Data in the Cloud",
    "text": "4. Store Raw Data in the Cloud\nIf you’re a GitHub user, you may be tempted to store your data files there, but GitHub limits the size of files allowed in repositories. Adding files larger than 50MB will receive a warning, and files larger than 100MB will be blocked. If you’re working with big datasets or spatial data, you can exceed this limit pretty fast.\nTo avoid this, we recommend instead that you store your raw data files in the cloud and make them available to everyone in your group. For example, you can create a folder for raw data in a Shared Google Drive (which we can create for you!). Then, you can download the data using the googledrive R package or with any other Google Drive API in your preferred language."
  },
  {
    "objectID": "wg_team-coding.html#meta-document",
    "href": "wg_team-coding.html#meta-document",
    "title": "Team Coding: 5 Essentials",
    "section": "5. Meta-Document",
    "text": "5. Meta-Document\nDocumenting every individual script is important, but it’s also well worth the time and effort to document the big picture of your workflow. As you continue to build on your workflow, it can be hard to keep track of each script’s role and how they relate to each other. You might need to update a script upstream and then try to figure out what other scripts downstream need to be updated next in order to account for the new edits. If you’re not using a workflow management software, then it’s best to thoroughly document how each script fits into the larger workflow. The README is a great place to document each step along the way.\n\n\n\nRegal Fritillary on Milkweed, Konza Prairie Biological Station - Photographer: Jill Haukos"
  }
]