[
  {
    "objectID": "internal_new-wg-setup.html",
    "href": "internal_new-wg-setup.html",
    "title": "Working Group Setup Checklist",
    "section": "",
    "text": "Note\n\n\n\nPlease note that the following checklist of working group setup steps is meant to serve as internal documentation only! Our team will be taking these steps on your behalf when you are funded so none of these steps are things you (a working member/PI) need to handle.\nFeel free to contact us if you find these instructions useful and want to apply them to non-working group contexts!\nWhen new working groups are funded, our team takes a number of setup steps to create some of the infrastructure that past groups have requested/found useful. This is mainly an attempt to help the group avoid spending their precious in-person meeting time doing relatively dry technical steps that we can easily accomplish early-on. Some of these steps also set a useful ‘tone’ in terms of facilitating groups’ adherence to reproducibility best practices."
  },
  {
    "objectID": "internal_new-wg-setup.html#shared-google-drive",
    "href": "internal_new-wg-setup.html#shared-google-drive",
    "title": "Working Group Setup Checklist",
    "section": "Shared Google Drive",
    "text": "Shared Google Drive\nMany groups gravitate towards using Google Drive for storing data, relevant scientific literature, and (eventually) manuscript drafts. One advantage of a true Shared Drive over simply creating a folder and sharing that is that the distributed ownership of the Shared Drive makes it very difficult to accidentally delete/lose important files.\nSome groups have experience serious heartbreak when one member’s Google identity gets closed by their institution and all files/folders created by that member vanish. A Shared Drive makes this horror story an impossibility.\n\nCreating the Shared Drive\nOur @nceas.ucsb.edu email addresses are empowered to create Shared Drives. Navigate to your Google Drive, then in the left sidebar click “Shared drives”. Once there you can click the “+ New” button to create a brand new Shared Drive.\nThe naming convention you should use is LTER-WG_&lt;WG-Abbreviation&gt; or LTER-SPARC_WG-Abbreviation for full synthesis working groups and SPARC groups respectively.\nNote that for groups with longer names you will want to abbreviate so that the Shared Drive name doesn’t get ambiguously cropped in a default browser window.\n\n\nAdding Users\nOnce the Shared Drive exists, add the following people as “Content Managers”:\n\nAll members of the Scientific Computing team\nMarty Downs (LTER Network Office Director)\nThomas Hetmank (NCEAS Programmer/Analyst)\n\nWhen you reach out to the working groups you can also make a note of their emails and add them as well though you may want to first tell them about the Shared Drive before sending them a semi-random Shared Drive invite.\n\n\nContent Creation\nWe want to use as light a touch as possible here to make sure that groups feel empowered to make their Shared Drive whatever they need it to be but there are a few things we can do.\nCopy the template Google Sheets we’ve created (found in this Drive folder) and move the copies into their Shared Drive."
  },
  {
    "objectID": "internal_new-wg-setup.html#github-repository",
    "href": "internal_new-wg-setup.html#github-repository",
    "title": "Working Group Setup Checklist",
    "section": "GitHub Repository",
    "text": "GitHub Repository\nWe encourage all groups to engage with GitHub for–at minimum–storing their final code products. We have found that creating a GitHub repository at this stage tends to increase adoption of GitHub and is therefore very much worthwhile even if no group members use it at the time that their group gets funded.\n\nInitialize Repository\nMake a repository in the LTER GitHub Organization with the following information:\n\nBegin with the working group template repository\n\nWhen you make a new repo it’ll provides an option for whether you want to use a template\n\nCreate a name that fits one of the following naming conventions\n\nFor full working groups (3-4 meetings): “lterwg-abbreviated-group-name”\nFor SPARC groups (1 meeting): “lter-sparc-abbreviated-group-name”\n\nSet the “Description” to the title of the working group\n\nAs indicated on the LTER Network Office website\n\nAdd a README\nCreate a .gitignore using the R template\n\nR is the most common working group language and the .gitignore is easily changed in the event the group is primarily using a different language"
  },
  {
    "objectID": "index.html#what-we-do",
    "href": "index.html#what-we-do",
    "title": "LTER Scientific Computing Team",
    "section": "What We Do",
    "text": "What We Do\nThe Scientific Computing team is a small (but mighty!) team of data scientists supporting synthesis working groups funded by the Long Term Ecological Research (LTER) Network Office. Participation in these groups is open to scientists from within and outside of the LTER Network. We provide modern technological infrastructure to support analytical, computing, or network-based needs for these synthesis working groups.\nWe are housed at the National Center for Ecological Analysis and Synthesis (NCEAS). Our goal is to support and promote an open and reproducible approach to synthesis science. We do so by providing on-demand training, advising, and direct code/data support. In addition to the support during in-person meetings at NCEAS, our team is available in-between visits to discuss and advise on data science and scientific programming tasks, such as:\n\nStructuring and integrating heterogeneous datasets\nWriting code to wrangle, analyze, model, or visualize the data your group has already collected\nDesigning workflows, scripting best practices for reproducible science, and reviewing code\nHelping you get set up on NCEAS’ server and scale your analysis\nPreserving and promoting your products on the internet\n\nIncluding everything from derived datasets and terminological glossaries/vocabularies to scripts, model codes, and interactive “web applications”\n\nOffering workshops on new skills or programs (such as Git/GitHub)\n\nDepending on your team’s preferences, we can operate on a spectrum of independence ranging from complete self-sufficiency after initial definition of task scope to coding together with your team.\nContact our team with your requests at scicomp [at] nceas.ucsb.edu"
  },
  {
    "objectID": "index.html#navigating-this-website",
    "href": "index.html#navigating-this-website",
    "title": "LTER Scientific Computing Team",
    "section": "Navigating this Website",
    "text": "Navigating this Website\n\nWorking Group MembersOther Visitors\n\n\nFor members of our working groups, this website is a centralized hub of resources designed to help you get familiar with the way our team works and what we offer. The pages under the Working Group Onboarding dropdown menu are particularly targeted at facilitating collaboration between your group and our team!\n\n\nWelcome! Even if you’re not a part of a working group, we hope that you find the various resources on this website helpful! If you’re interested in learning more about open, reproducible science, please feel free to take a look at the items under our Workshops and Tips dropdown menus.\n\n\n\n\n\n\nLTER All Scientists’ Meeting 2022, Pacific Grove CA"
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Our Team",
    "section": "",
    "text": "Because we live in an era where we may only meet in person sporadically, we felt it would be nice to introduce ourselves here to help you put a face to the emails / Slack messages / GitHub issues we exchange going forward! If you would like to email the whole team as one send your questions to scicomp [at] nceas.ucsb.edu"
  },
  {
    "objectID": "staff.html#past-team-members",
    "href": "staff.html#past-team-members",
    "title": "Our Team",
    "section": "Past Team Members",
    "text": "Past Team Members\nWe have been privileged to work with the following teammates who have since gone on to other positions.\n\nJulien Brun (he/him)\n brunj7.github.io –  brunj7 –  @brunj7\n\nJulien was responsible for mentoring the other members of the scientific computing support team as well as advising researchers on how best to address their data challenges with scripted, reproducible solutions. Julien now works for the UCSB Library team as a research facilitator in the earth and environmental sciences. Julien is also a Lecturer in the Master in Environmental Data Science program at Bren School of Environmental Science and Management at UC Santa Barbara, where he teaches “good enough” practices in reproducible and collaborative data science."
  },
  {
    "objectID": "internal_get-data.html",
    "href": "internal_get-data.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Since working groups may ask us to get and wrangle data from some popular databases, this page serves as a guide on acquiring data from said databases! Below are some common places where groups have asked us to request from. Please feel free to add more to the growing list as we collaborate with more and more groups."
  },
  {
    "objectID": "internal_get-data.html#guide",
    "href": "internal_get-data.html#guide",
    "title": "Data Acquisition",
    "section": "Guide",
    "text": "Guide\nThe TRY database offers a wide variety of plant trait data for many species. As of 2023, the database contains over 15 million trait records for over 300 thousand plant taxa.\n\n1. Account Registration\nTo get started, first create an account on TRY. After filling out the required fields, TRY will send an email to you with your password. Log in with your email and password.\n\n\n\n2. Terms and Conditions\nAfter logging in for the first time, TRY will direct you to its Intellectual Property Guidelines. Scroll down and click ‘I accept’ to proceed.\n\n\n\n\n\n\n3. Start a Data Request and Select Traits\nTRY will then redirect you to the Request Data page to begin a new request. This page can also be accessed by clicking “Data Portal” &gt; “TRY Database”. The recommendation is to request by traits/species.\nThen enter the numeric IDs of the traits you want data for. For example, plant height has the IDs 3106 (Plant height vegetative) and 3107 (Plant height generative).\nThe list of all trait IDs is here. There is also an option on the page to download a .txt file of all the traits.\n\n\n\n\n4. Select Species\nNext, you will be prompted to select the species you want data for. Each species is associated with a numeric ID, so enter the desired IDs in the field below. For example, 29 represents Abies alba and 56 represents Abies lasiocarpa.\nThe list of all plant IDs is here. Again, you also have the option to download a .txt file of all the plant species in case you wish to get the IDs programmatically.\n\n\n\n\n5. Choose Data Type: Public or Private\nThen, you must choose whether you want to request public data or public+private data. The former option will get you the data faster, while the latter option may take up to 14 days because the dataset custodians must respond to your request.\nIn our previous experience, requesting public data usually gets you the data within 24 hours.\n\n\n\n6. Describe Data Request\nOnce you have chosen which type of data you want, you will be prompted to enter the title and description of your project. These fields will be necessary if you requested for public+private data since the dataset custodians will need a reason to make their data available for your request.\n\n\n\n7. Add Coauthors\nAdd any relevant coauthors to your request before proceeding. It’s a good idea to add your fellow data analysts and/or working group PIs.\n\n\n\n\n8. Finish Request and Wait\nYour request is now complete! You will receive an email from TRY notifiying you that they have received your request. Wait for a subsequent email from them to get the actual download link to the data."
  },
  {
    "objectID": "internal_get-data.html#relevant-example",
    "href": "internal_get-data.html#relevant-example",
    "title": "Data Acquisition",
    "section": "Relevant Example",
    "text": "Relevant Example\nSee our GitHub issue #122 and the scripts in this folder for an example on how we pulled and integrated data from the TRY database."
  },
  {
    "objectID": "internal_get-data.html#guide-1",
    "href": "internal_get-data.html#guide-1",
    "title": "Data Acquisition",
    "section": "Guide",
    "text": "Guide\nNASA’s AppEEARS (Application for Extracting and Exploring Analysis Ready Samples) platform is a useful way of extracting spatial data within a user-defined bounding box or specific points. A ton of spatial data (including MODIS datasets) are available here so it makes this portal a useful ‘one-stop shop’ for groups that want more than one spatial variable.\n\n1. Account Registration / Sign In\nTo begin, visit the AppEEARS website and register for an account. After filling out the necessary personal information and confirming your email address you should be able to sign into the portal using your username and password.\n\n\n\n2. Begin an Extraction Request\nIn order to begin an extraction request, navigate to the “Extract” button in the navbar at the top of the site and click it. Select “Area” in the resulting dropdown menu of options.\n\n\n\n3. Pick Request Type\nYou can now decide whether to start your data request from scratch (“Start a new request” on the left) or if you’d like to use a previous request as a starting point (“Copy a previous request”). Note that uploading a request as a JSON is also available but I found the user interface intuitive enough that I wasn’t tempted to use this option.\nThe primary advantage–as I see it–of duplicating a request is that it allows you to re-use your manually-drawn bounding box. This makes multiple requests for different datasets exactly share the same area which makes the eventual harmonization of those data that much simpler.\nStarting a new request is the option I take only when I haven’t previously drawn a bounding box for the area of interest.\n\n\n\n4. Fill out Request\nRegardless of whether you’re starting from a blank slate or from a previous request, the next step involves customizing this request. You’ll need to perform the following steps to complete the request:\n\nEnter a name for this request\n\nThis name is only for your internal use so it can use your idiosyncrasies but should imply something about the data layer(s) and bounding box location to make it decipherable by others.\n\nDraw a polygon over your area of interest\n\nNote that the multi-point polygon tends to result in larger data requests than a square because it overlaps more separate tiles of the source data.\n\nDecide on starting/ending dates\n\nRegardless of the temporal granularity of the source data the time range requires you to specify specific days, months, and years.\n\nPick data layers for that area and time range\n\nThe search field for the data layers is pretty robust and allows you to either search MODIS codes or names of the data (e.g., “snow” or “MODIS10A2”)\n\nChoose output file format\n\nYour options are GeoTiff (i.e., raster) or netCDF\n\nChoose projection system\n\nThis option is so helpful! Specifying your projection system lets you put all of the computational labor for re-projecting into a different coordinate reference system (CRS) onto AppEEARS rather than leaving it for you to handle after the fact\n \n\n\n5. Submit Request\nWhen you are ready, click “Submit” at the bottom of the request screen \nIf your request is at or below the data limit per request, you will receive a narrow green banner at the top of the request page notifying you of the success.\n\nIf you have requested too much data you will receive a red banner notifying you of this fact and quantifying how far over the limit your request is.\n\nThe data limit is affected by (1) the spatial extent of the request, (2) the temporal extent of the request, and (3) the number of included data layers. If you are over the limit you will need to reduce one of these parameters.\nIf possible, I recommend first reducing the number of data layers as duplicating a request with a shared bounding box and time range with a different data layer is really straightforward. If you need to re-draw the bounding box you’ll need to open another data request and manually draw additional bounding boxes to eventually fill out the total area of interest which can be somewhat cumbersome.\n\n\n6. Download Completed Request\nMoments after submitting the request successfully you will receive a “Request Received” email from AppEEARS inviting you to ‘explore’ your request. This can be safely ignored as it contains only the information that you just entered.\nSome time later (usually hours if not a day or two for larger requests) you will receive a second email notifying you that the request is complete! That email has both the “Explore” link from the preceding email and a “Download” link.\nClick the provided link and on the resulting AppEEARS page you can download the data your request yields onto your computer to do with what you will.\nNote that requests do expire after a few weeks/months so you’ll want to download the data as soon as possible. If a request is expired and you’d like to re-request, there is a button to do exactly that but you’ll need to wait for the request to process again before being able to re-download the data."
  },
  {
    "objectID": "internal_get-data.html#relevant-example-1",
    "href": "internal_get-data.html#relevant-example-1",
    "title": "Data Acquisition",
    "section": "Relevant Example",
    "text": "Relevant Example\nConsult the LTER Organization-owned Silica Export GitHub repository (see the README here) for an example of how we wrangled and extracted data retrieved from AppEEARS.\nOf particular relevance is likely the “crop-drivers.R” script where we read in the data downloaded from AppEEARS and crop each data request to avoid the small amount of spatial overlap among requests. These requests ran into the data limit and had to be split among several requests that (slightly) overlap one another. To avoid re-sampling the same pixels, the rasters retrieved from AppEEARS each needed to be cropped slightly."
  },
  {
    "objectID": "internal_get-data.html#guide-2",
    "href": "internal_get-data.html#guide-2",
    "title": "Data Acquisition",
    "section": "Guide",
    "text": "Guide\nThe National Ecological Observatory Network (NEON) is a facility that collects long-term ecological data from aquatic and terrestrial ecosystems in the United States. Their data usually falls into one of three categories: data collected by an airborne observation platform like LIDAR, data collected by a person in the field, or data collected by an automated sensor.\nNEON has a handy API that will allow you to pull their data using R. Additionally, they have created a whole suite of tutorials for various needs, including a tutorial on how to download and explore NEON data using their helper R packages."
  },
  {
    "objectID": "modules_tutorials/quarto-website_deploy-prep.html",
    "href": "modules_tutorials/quarto-website_deploy-prep.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "To prepare your project for web deployment via GitHub Pages, we have three quick steps that we must first complete.\nFirst, in the “_quarto.yml” file, add output-dir: docs as a subheading beneath the project: heading. Make sure that the indentation is the same as the type: website but the new line can be either above or below that line.\n\n\n\nSecond, in the “Terminal” pane run touch .nojekyll. This creates a file called “.nojekyll” that is necessary for hosting your website via GitHub Pages.\nThird, in the “Terminal” pane run quarto render. This processes the template .qmd files you currently have in the repository and prepares them to become actual web pages.\nOnce you’ve done these three things you can move on to creating a GitHub repository so that we can take the necessary steps to having GitHub host your website!"
  },
  {
    "objectID": "modules_tutorials/github-connect.html",
    "href": "modules_tutorials/github-connect.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "The following steps include a sequence of command line operations that will be relayed in code chunks below. Unless otherwise stated, all of the following code should be run in “Terminal”.\nIf you didn’t check the “Create a git repository” button while creating the R project, you’ll need to do that via the command line now. If you did check that box, you should skip this step!\n\n# Start a git repository on the \"main\" branch\ngit init -b main\n\nStage all of the files in your project to the git repository. This includes the .yml file, all .qmd files and all of their rendered versions created when you ran quarto render earlier. This code is equivalent to checking the box for the files in the “Git” pane of RStudio.\n\n# Stage all files\ngit add .\n\nOnce everything has been staged, you now must commit those staged files with a message.\n\n# Commit all files with the message in quotes\ngit commit -m \"Initial commit\"\n\nNow that your project files have been committed, you need to tell your computer where you will be pushing to and pulling from. Paste the link you copied at the end of the “Make a New GitHub Repository” into the code shown in the chunk below (instead of GITHUB_URL) and run it.\n\n# Tell your computer which GitHub repository to connect to\ngit remote add origin GITHUB_URL\n\nVerify that URL before continuing.\n\n# Confirm that URL worked\ngit remote -v\n\nFinally, push your commited changes to the repostory that you set as the remote in the preceding two steps.\n\n# Push all of the content to the main branch\ngit push -u origin main\n\nNow, go back to GitHub and refresh the page to see your project content safe and sound in your new GitHub repository!"
  },
  {
    "objectID": "modules_tutorials/quarto-website_new-proj.html",
    "href": "modules_tutorials/quarto-website_new-proj.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "To begin, click the “Project” button in the top right of your RStudio session.\n\nIn the resulting dialogue, click the “New Directory” option.\n\n\n\nFrom the list of options for project templates, select “Quarto Website”.\n\n\n\nPick a title and check the “Create a git repository” checkbox. For your title, short but descriptive titles are most effective. Once that is done, click “Create Project” in the bottom right of the window.\n\n\n\nAfter a few seconds, RStudio should refresh with a Quarto document (such documents have the file extension “.qmd”) and a “_quarto.yml” file open.\n\n\n\nPart of Quarto’s central philosophy is that all of the formatting of individual .qmd files in a project is governed by the settings created by a singular .yml file. In an R markdown project some of the global settings are set in .yml but other settings are handled within each .Rmd file. This centralization is a key innovation in streamlining projects and is one reason for Quarto’s quick popularity."
  },
  {
    "objectID": "modules_tutorials/googledrive-fxns.html",
    "href": "modules_tutorials/googledrive-fxns.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "Now that you’ve authorized the googledrive package, you can start downloading the Google Drive files you need through R! Let’s say that you want to download a csv file from a folder or shared drive. You can save the URL of that folder/shared drive to a variable.\nThe googledrive package makes it straightforward to access Drive folders and files with the as_id function. This function allows the full link to a file or folder to serve as a direct connection to that file/folder. Most of the other googledrive functions will require a URL that is wrapped with as_id in this way. You would replace “your url here” with the actual link but make sure it is in quotation marks.\n\ndrive_url &lt;- googledrive::as_id(\"your url here\")\n\nTo list all the contents of this folder, we can use the drive_ls function. You will get a dataframe-like object of the files back as the output. An example is shown below in the screenshot. Here, this Google Drive folder contains 4 csv files: ingredients.csv, favorite_soups.csv, favorite_fruits.csv and favorite_desserts.csv\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url)\ndrive_folder\n\n\n\n\nIf it has been a while since you’ve used googledrive, it will prompt you to refresh your token. Simply enter the number that corresponds to the correct Google Drive account.\n\n\n\nIf you only want to list files of a certain type, you can specify this in the type argument. And let’s say that my folder contains a bunch of csv files, but I only want to download the one named “favorite_desserts.csv”. In that case, I can also put a matching string in the pattern argument in order to filter down to 1 file.\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url,\n                                      type = \"csv\", \n                                      pattern = \"favorite_desserts\")\ndrive_folder\n\n\n\n\nOnce we’ve narrowed down to the file we want, we can download it using drive_download. This function takes the file identifier as an argument so we can access it using drive_folder$id.\n\ngoogledrive::drive_download(file = drive_folder$id)\n\nThis will automatically download the file to our working directory. If you want, you can specify a different path to download to. Just put the new file path into the path argument, replacing the “your path here”, but keep the quotation marks.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\")\n\nIf you’ve downloaded the file before, and you want to overwrite it, there’s a handy overwrite argument that you can set to TRUE. Note that the default is FALSE.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\",\n                            overwrite = T)\n\nIf there are multiple files in the Drive folder and you want to download them all, you can use a loop like so:\n\n# For each file:\nfor(focal_file in drive_folder$name){\n  \n  # Find the file identifier for that file\n  file_id &lt;- subset(drive_folder, name == focal_file)\n\n  # Download that file\n  drive_download(file = file_id$id, \n                 path = \"your path here\",\n                 overwrite = T)\n}"
  },
  {
    "objectID": "tutorial_googledrive-pkg.html",
    "href": "tutorial_googledrive-pkg.html",
    "title": "Conecting R & Google Drive",
    "section": "",
    "text": "The googledrive R package is a package that lets R users directly interact with files on GoogleDrive. This can be extremely useful because it lets all members of a team share the same source data file(s) and guarantees that updates to “living” documents are received by all group members the next time they run their R script. This package is technically part of the Tidyverse but is not loaded by running library(tidyverse).\nBecause this package requires access to an R user’s GoogleDrive, you must “authenticate” the googledrive package. This essentially tells Google that it is okay if an R package uses your credentials to access and (potentially) modify your Drive content. There are only a few steps to this process but follow along with the below tutorial and we’ll get you ready to integrate the Google Drive into your code workflows using the googledrive package in no time!\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nCreate a Gmail account\n\nFeel free to skip any steps that you have already completed!\n\n\n\nIn order to connect R with a GoogleDrive, we’ll need to authorize googledrive to act on our behalf. This only needs to be done once (per computer) so follow along and you’ll be building GoogleDrive into your workflows in no time!\nFirst, install the googledrive and httpuv R packages. The googledrive package’s need is self-evident while the httpuv package makes the following steps a little easier than googledrive makes it alone. Be sure to load the googledrive package after you install it!\n\n# Install packages\ninstall.packages(c(\"googledrive\", \"httpuv\"))\n\n# Load them\nlibrary(googledrive)\n\nOnce you’ve installed the packages we can begin the authentication in R using the drive_auth function in the googledrive package.\n\ngoogledrive::drive_auth(email = \"enter your gmail here!\")\n\nIf this is your first time using googledrive, drive_auth will kick you to a new tab of your browser (see below for a screen grab of that screen) where you can pick which Gmail you’d like to connect to R.\n\n\n\nClick the Gmail you want to use and you will get a second screen where Google tells you that “Tidyverse API” wants access to your Google Account. This message is followed by three checkboxes, the first two are grayed out but the third is unchecked.\n\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nThis next bit is vitally important so carefully read and follow the next instruction!\n\n\nIn this screen, you must check the unchecked box to be able to use the googledrive R package. If you do not check this box all attempts to use googledrive functions will get an error that says “insufficient permissions”.\n\n\n\nWhile granting access to “see, edit, create, and”delete” all of your Google Drive files” sounds like a significant security risk, those powers are actually why you’re using the googledrive package in the first place! You want to be able to download existing Drive files, change them in R on your computer, and then put them back in Google Drive which is exactly what is meant by “see, edit, create, and delete”.\nAlso, this power only applies to the computer you’re currently working on! Granting access on your work computer allows only that computer to access your Drive files. So don’t worry about giving access to your Drive to the whole world, that is protected by the same failsafes that you use when you let your computer remember a password to a website you frequent.\nAfter you’ve checked the authorization box, scroll down and click the “Continue” button.\n\n\n\nThis should result in a plain text page that tells you to close this window and return to R. If you see this message you are ready to use the googledrive package!\n\n\n\n\n\n\nIf you have tried to use drive_auth and did not check the box indicated above, you need to make the googledrive package ask you again. Using drive_auth will not (annoyingly) return you to the place it sent you the first time. However, if you run the following code chunk it should give you another chance to check the needed box.\nThe gargle R package referenced below is required for interacting with Google Application Program Interfaces (APIs). This package does the heavy lifting of secure password and token management and is necessary for the googledrive authentication chunk below.\n\ngoogledrive::drive_auth(\n  email = gargle::gargle_oauth_email(),\n  path = NULL,\n  scopes = \"https://www.googleapis.com/auth/drive\",\n  cache = gargle::gargle_oauth_cache(),\n  use_oob = gargle::gargle_oob_default(),\n  token = NULL)\n\nUnfortunately, to use the googledrive package you must check the box that empowers the package to function as designed. If you’re uncomfortable giving the googledrive that much power you will need to pivot your workflow away from using GoogleDrive directly. However, NCEAS does offer access to an internal server called “Aurora” where data can be securely saved and shared among group members without special authentication like what googledrive requires. Reach out to our team if this seems like a more attractive option for your working group and we can offer training on how to use this powerful tool!\n\n\n\nNow that you’ve authorized the googledrive package, you can start downloading the Google Drive files you need through R! Let’s say that you want to download a csv file from a folder or shared drive. You can save the URL of that folder/shared drive to a variable.\nThe googledrive package makes it straightforward to access Drive folders and files with the as_id function. This function allows the full link to a file or folder to serve as a direct connection to that file/folder. Most of the other googledrive functions will require a URL that is wrapped with as_id in this way. You would replace “your url here” with the actual link but make sure it is in quotation marks.\n\ndrive_url &lt;- googledrive::as_id(\"your url here\")\n\nTo list all the contents of this folder, we can use the drive_ls function. You will get a dataframe-like object of the files back as the output. An example is shown below in the screenshot. Here, this Google Drive folder contains 4 csv files: ingredients.csv, favorite_soups.csv, favorite_fruits.csv and favorite_desserts.csv\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url)\ndrive_folder\n\n\n\n\nIf it has been a while since you’ve used googledrive, it will prompt you to refresh your token. Simply enter the number that corresponds to the correct Google Drive account.\n\n\n\nIf you only want to list files of a certain type, you can specify this in the type argument. And let’s say that my folder contains a bunch of csv files, but I only want to download the one named “favorite_desserts.csv”. In that case, I can also put a matching string in the pattern argument in order to filter down to 1 file.\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url,\n                                      type = \"csv\", \n                                      pattern = \"favorite_desserts\")\ndrive_folder\n\n\n\n\nOnce we’ve narrowed down to the file we want, we can download it using drive_download. This function takes the file identifier as an argument so we can access it using drive_folder$id.\n\ngoogledrive::drive_download(file = drive_folder$id)\n\nThis will automatically download the file to our working directory. If you want, you can specify a different path to download to. Just put the new file path into the path argument, replacing the “your path here”, but keep the quotation marks.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\")\n\nIf you’ve downloaded the file before, and you want to overwrite it, there’s a handy overwrite argument that you can set to TRUE. Note that the default is FALSE.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\",\n                            overwrite = T)\n\nIf there are multiple files in the Drive folder and you want to download them all, you can use a loop like so:\n\n# For each file:\nfor(focal_file in drive_folder$name){\n  \n  # Find the file identifier for that file\n  file_id &lt;- subset(drive_folder, name == focal_file)\n\n  # Download that file\n  drive_download(file = file_id$id, \n                 path = \"your path here\",\n                 overwrite = T)\n}"
  },
  {
    "objectID": "tutorial_googledrive-pkg.html#overview",
    "href": "tutorial_googledrive-pkg.html#overview",
    "title": "Conecting R & Google Drive",
    "section": "",
    "text": "The googledrive R package is a package that lets R users directly interact with files on GoogleDrive. This can be extremely useful because it lets all members of a team share the same source data file(s) and guarantees that updates to “living” documents are received by all group members the next time they run their R script. This package is technically part of the Tidyverse but is not loaded by running library(tidyverse).\nBecause this package requires access to an R user’s GoogleDrive, you must “authenticate” the googledrive package. This essentially tells Google that it is okay if an R package uses your credentials to access and (potentially) modify your Drive content. There are only a few steps to this process but follow along with the below tutorial and we’ll get you ready to integrate the Google Drive into your code workflows using the googledrive package in no time!\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nCreate a Gmail account\n\nFeel free to skip any steps that you have already completed!\n\n\n\nIn order to connect R with a GoogleDrive, we’ll need to authorize googledrive to act on our behalf. This only needs to be done once (per computer) so follow along and you’ll be building GoogleDrive into your workflows in no time!\nFirst, install the googledrive and httpuv R packages. The googledrive package’s need is self-evident while the httpuv package makes the following steps a little easier than googledrive makes it alone. Be sure to load the googledrive package after you install it!\n\n# Install packages\ninstall.packages(c(\"googledrive\", \"httpuv\"))\n\n# Load them\nlibrary(googledrive)\n\nOnce you’ve installed the packages we can begin the authentication in R using the drive_auth function in the googledrive package.\n\ngoogledrive::drive_auth(email = \"enter your gmail here!\")\n\nIf this is your first time using googledrive, drive_auth will kick you to a new tab of your browser (see below for a screen grab of that screen) where you can pick which Gmail you’d like to connect to R.\n\n\n\nClick the Gmail you want to use and you will get a second screen where Google tells you that “Tidyverse API” wants access to your Google Account. This message is followed by three checkboxes, the first two are grayed out but the third is unchecked.\n\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nThis next bit is vitally important so carefully read and follow the next instruction!\n\n\nIn this screen, you must check the unchecked box to be able to use the googledrive R package. If you do not check this box all attempts to use googledrive functions will get an error that says “insufficient permissions”.\n\n\n\nWhile granting access to “see, edit, create, and”delete” all of your Google Drive files” sounds like a significant security risk, those powers are actually why you’re using the googledrive package in the first place! You want to be able to download existing Drive files, change them in R on your computer, and then put them back in Google Drive which is exactly what is meant by “see, edit, create, and delete”.\nAlso, this power only applies to the computer you’re currently working on! Granting access on your work computer allows only that computer to access your Drive files. So don’t worry about giving access to your Drive to the whole world, that is protected by the same failsafes that you use when you let your computer remember a password to a website you frequent.\nAfter you’ve checked the authorization box, scroll down and click the “Continue” button.\n\n\n\nThis should result in a plain text page that tells you to close this window and return to R. If you see this message you are ready to use the googledrive package!\n\n\n\n\n\n\nIf you have tried to use drive_auth and did not check the box indicated above, you need to make the googledrive package ask you again. Using drive_auth will not (annoyingly) return you to the place it sent you the first time. However, if you run the following code chunk it should give you another chance to check the needed box.\nThe gargle R package referenced below is required for interacting with Google Application Program Interfaces (APIs). This package does the heavy lifting of secure password and token management and is necessary for the googledrive authentication chunk below.\n\ngoogledrive::drive_auth(\n  email = gargle::gargle_oauth_email(),\n  path = NULL,\n  scopes = \"https://www.googleapis.com/auth/drive\",\n  cache = gargle::gargle_oauth_cache(),\n  use_oob = gargle::gargle_oob_default(),\n  token = NULL)\n\nUnfortunately, to use the googledrive package you must check the box that empowers the package to function as designed. If you’re uncomfortable giving the googledrive that much power you will need to pivot your workflow away from using GoogleDrive directly. However, NCEAS does offer access to an internal server called “Aurora” where data can be securely saved and shared among group members without special authentication like what googledrive requires. Reach out to our team if this seems like a more attractive option for your working group and we can offer training on how to use this powerful tool!\n\n\n\nNow that you’ve authorized the googledrive package, you can start downloading the Google Drive files you need through R! Let’s say that you want to download a csv file from a folder or shared drive. You can save the URL of that folder/shared drive to a variable.\nThe googledrive package makes it straightforward to access Drive folders and files with the as_id function. This function allows the full link to a file or folder to serve as a direct connection to that file/folder. Most of the other googledrive functions will require a URL that is wrapped with as_id in this way. You would replace “your url here” with the actual link but make sure it is in quotation marks.\n\ndrive_url &lt;- googledrive::as_id(\"your url here\")\n\nTo list all the contents of this folder, we can use the drive_ls function. You will get a dataframe-like object of the files back as the output. An example is shown below in the screenshot. Here, this Google Drive folder contains 4 csv files: ingredients.csv, favorite_soups.csv, favorite_fruits.csv and favorite_desserts.csv\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url)\ndrive_folder\n\n\n\n\nIf it has been a while since you’ve used googledrive, it will prompt you to refresh your token. Simply enter the number that corresponds to the correct Google Drive account.\n\n\n\nIf you only want to list files of a certain type, you can specify this in the type argument. And let’s say that my folder contains a bunch of csv files, but I only want to download the one named “favorite_desserts.csv”. In that case, I can also put a matching string in the pattern argument in order to filter down to 1 file.\n\ndrive_folder &lt;- googledrive::drive_ls(path = drive_url,\n                                      type = \"csv\", \n                                      pattern = \"favorite_desserts\")\ndrive_folder\n\n\n\n\nOnce we’ve narrowed down to the file we want, we can download it using drive_download. This function takes the file identifier as an argument so we can access it using drive_folder$id.\n\ngoogledrive::drive_download(file = drive_folder$id)\n\nThis will automatically download the file to our working directory. If you want, you can specify a different path to download to. Just put the new file path into the path argument, replacing the “your path here”, but keep the quotation marks.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\")\n\nIf you’ve downloaded the file before, and you want to overwrite it, there’s a handy overwrite argument that you can set to TRUE. Note that the default is FALSE.\n\ngoogledrive::drive_download(file = drive_folder$id, \n                            path = \"your path here\",\n                            overwrite = T)\n\nIf there are multiple files in the Drive folder and you want to download them all, you can use a loop like so:\n\n# For each file:\nfor(focal_file in drive_folder$name){\n  \n  # Find the file identifier for that file\n  file_id &lt;- subset(drive_folder, name == focal_file)\n\n  # Download that file\n  drive_download(file = file_id$id, \n                 path = \"your path here\",\n                 overwrite = T)\n}"
  },
  {
    "objectID": "tip_packages.html",
    "href": "tip_packages.html",
    "title": "Streamlined R Package Loading",
    "section": "",
    "text": "Loading packages / libraries in R can be cumbersome when working collaboratively because there is no guarantee that you all have the same packages installed. While you could comment-out an install.packages() line for every package you need for a given script, we recommend using the R package librarian to greatly simplify this process!\nlibrarian::shelf() accepts the names of all of the packages–either CRAN or GitHub–installs those that are missing in that particular R session and then attaches all of them."
  },
  {
    "objectID": "tip_packages.html#overview",
    "href": "tip_packages.html#overview",
    "title": "Streamlined R Package Loading",
    "section": "",
    "text": "Loading packages / libraries in R can be cumbersome when working collaboratively because there is no guarantee that you all have the same packages installed. While you could comment-out an install.packages() line for every package you need for a given script, we recommend using the R package librarian to greatly simplify this process!\nlibrarian::shelf() accepts the names of all of the packages–either CRAN or GitHub–installs those that are missing in that particular R session and then attaches all of them."
  },
  {
    "objectID": "tip_packages.html#traditional-package-loading",
    "href": "tip_packages.html#traditional-package-loading",
    "title": "Streamlined R Package Loading",
    "section": "Traditional Package Loading",
    "text": "Traditional Package Loading\nTo load packages typically you’d have something like the following in your script:\n\n# Install packages (if needed)\ninstall.packages(\"tidyverse\")\ninstall.packages(\"devtools\")\ndevtools::install_github(\"NCEAS/scicomptools\")\n\n# Load libraries\nlibrary(tidyverse); library(scicomptools)"
  },
  {
    "objectID": "tip_packages.html#package-loading-with-librarian",
    "href": "tip_packages.html#package-loading-with-librarian",
    "title": "Streamlined R Package Loading",
    "section": "Package Loading with librarian",
    "text": "Package Loading with librarian\nWith librarian::shelf() however this becomes much cleaner! In addition to being fewer lines, using librarian also removes the possibility that someone running your code misses one of the packages that your script depends on and then the script breaks for them later on. librarian::shelf() automatically detects whether a package is installed, installs it if necessary, and then attaches the package.\nIn essence, librarian::shelf() wraps install.packages(), devtools::install_github(), and library() into a single, human-readable function.\n\n# Install and load packages!\nlibrarian::shelf(tidyverse, NCEAS/scicomptools)\n\nWhen using librarian::shelf(), package names do not need to be quoted and GitHub packages can be installed without the additional steps of installing the devtools package and using devtools::install_github() instead of install.packages()."
  },
  {
    "objectID": "wg_tools.html",
    "href": "wg_tools.html",
    "title": "Suggested Tools",
    "section": "",
    "text": "We primarily work on collaborative projects where we synthesize existing data to draw larger inferences than any single data set would allow. Because of this, we strongly recommend that each tool used by a team accomplish as many purposes as possible to avoid a project accruing endless “one off” tools that fit a specific purpose but do not accomplish any other tasks. Streamlining your workflow to just a few broadly useful programs also helps reduce the barrier to entry to training new team members and ensures that within team protocols are clear and concise to follow.\nThe analytical software options available at NCEAS follow directly from this ethos. Although occasionally providing specialty programs (upon request), we have otherwise carefully assembled a powerful lineup of scripted, cross-platform, scalable applications that are well-supported, generate robust results, and permit batch processing. Although these packages require an initial time investment to learn, and may seem intimidating to scientists familiar with only “point-and-click” software, we strongly argue that the long-term payoff is well worth the time investment at the start.\nWe recommend the following programs, websites, and platforms:"
  },
  {
    "objectID": "wg_tools.html#collaborative-tools",
    "href": "wg_tools.html#collaborative-tools",
    "title": "Suggested Tools",
    "section": "Collaborative Tools",
    "text": "Collaborative Tools\n\nVideo Conferencing\n\nBetween your in-person meetings at NCEAS (and during them if you have remote participants!) we recommend that you use Zoom. During the COVID-19 pandemic, Zoom became–arguably–the video chatting program and the value of your group’s familiarity with this program should not be underestimated.\n\n\nMessaging\n\nYour group will be doing a lot of written communication so we recommend creating a Slack community. The advantage of Slack over email is that you can have faster real-time conversations among multiple people without crowding your inbox. Slack also supports attaching files and “pinning” posts to make it easier to find them later. You can also create “channels” for sub-topics within your group (e.g., #paper 1, #analysis, etc.) that will allow your group members to track only the subtopics that are relevant to them. You can download the Slack desktop app here. Another team at NCEAS has created this guide for getting set up on Slack.\n\n\nDocument Sharing\n\nWe recommend that you share documents among your group using Google Drive to help you centralize your information. Our team will set up a Shared Google Drive for every working group. This drive can also be used as a staging area for your data during the data collection and exploration phases of your project. This is in part because many people are already at least somewhat familiar with Google Drive. In addition, there is an R package called googledrive that will allow you to directly connect any R scripts to Google Drive files and folders. This can be great for ensuring that all scripts use the same raw data and can be useful for exporting synthesized data products to a given Drive folder.\nFor the more analytical intensive phase of your project, NCEAS also has several analytical servers that can be used to store large data that need to be processed frequently. We’ll discuss NCEAS’ servers in greater detail further on in the onboarding process."
  },
  {
    "objectID": "wg_tools.html#computing-tools",
    "href": "wg_tools.html#computing-tools",
    "title": "Suggested Tools",
    "section": "Computing Tools",
    "text": "Computing Tools\n\nCode Storage & Versioning\n\nGit is a great program to use for tracking changes to your code as your project grows and evolves. Git is one type of “version control” software which is specifically built to track changes to code in an informative and useful way. It is analogous to using “track changes” in Microsoft Word but is built in a more seamless way and is purpose-built for working with code. You can install Git following the instructions here.\n\nSimilarly to R versus RStudio, we recommend that you create an account with GitHub so that the changes you track with Git can be viewed and interacted with in a straightforward way. GitHub is the industry standard for tracking changes to code and is a great way of making code for a specific project publicly-accessible once you are at a stage where that feels appropriate. We offer an introductory workshop on Git and GitHub that we are happy to offer to your group if that is of interest!\n\n\nAnalysis\n\nOur team is most well-versed in R and many data synthesis ends can be accomplished in this language. R’s primary advantage is its amazing user community! R users have developed all kinds of custom functions and packages so even when what you want to do is not supported by an R package on CRAN, chances are that you can find the tools you need online!\n\nWe strongly recommend using R through RStudio as the RStudio interface allows several ‘quality of life’ improvements that you will grow to greatly appreciate if you’re not already an RStudio user. RStudio enables an easier connection to Git (see below), facilitates use of the command line, and allows you to generate PDF or HTML reports with embedded code chunks for easy sharing in and outside of your group.\n\nIf your group is working with genomes or other large data files, you may find R’s memory limit to be a serious hindrance. If this is the case (or if you prefer to not use R!) we suggest Python as an alternative. Python works better with larger files and is also a well-respected programming language."
  },
  {
    "objectID": "tip_notebook-vs-script.html",
    "href": "tip_notebook-vs-script.html",
    "title": "Notebooks versus Scripts",
    "section": "",
    "text": "When coding in R, either R scripts (.R files) or R markdowns (.Rmd files) are viable options but they have different advantages and disadvantages that we will cover below."
  },
  {
    "objectID": "tip_notebook-vs-script.html#r-scripts",
    "href": "tip_notebook-vs-script.html#r-scripts",
    "title": "Notebooks versus Scripts",
    "section": "R Scripts",
    "text": "R Scripts\n\nPositives\nR scripts’ greatest strength is their flexibility. They allow you to format a file in whatever way is most intuitive to you. Additionally, R scripts can be cleaner for for loops insofar as they need not be concerned with staying within a given code chunk (as would be the case for a .Rmd). Developing a new workflow can be swiftly accomplished in an R script as some or all of the code in a script can be run by simply selecting the desired lines rather than manually running the desired chunks in a .Rmd file. Finally, R scripts can also be a better home for custom functions that can be sourced by another file (even a .Rmd!) for making repeated operations simpler to read.\n\n\nPotential Weaknesses\nThe benefit of extreme flexibility in R scripts can sometimes be a disadvantage however. We’ve all seen (and written) R scripts that have few or no comments or where lines of code are densely packed without spacing or blank lines to help someone new to the code understand what is being done. R scripts can certainly be written in a way that is accessible to those without prior knowledge of what the script accomplishes but they do not enforce such structure. This can make it easy, especially when we’re feeling pressed for time, to exclude structure that helps our code remain reproducible and understandable."
  },
  {
    "objectID": "tip_notebook-vs-script.html#r-markdowns",
    "href": "tip_notebook-vs-script.html#r-markdowns",
    "title": "Notebooks versus Scripts",
    "section": "R Markdowns",
    "text": "R Markdowns\n\nPositives\nR markdown files’ ability to “knit” as HTML or PDF documents makes them extremely useful in creating outward-facing reports. This is particularly the case when the specific code is less important to communicate than visualizations and/or analyses of the data but .Rmd files do facilitate echoing the code so that report readers can see how background operations were accomplished. The code chunk structure of these files can also nudge users towards including valuable comments (both between chunks and within them) though of course .Rmd files do not enforce such non-code content.\n\n\nPotential Weaknesses\nR markdowns can fail to knit due to issues even when the code within the chunks works as desired. Duplicate code chunk names or a failure to install LaTeX can be a frustrating hurdle to overcome between functioning code and a knit output file. When code must be re-run repeatedly (as is often the case when developing a new workflow) the stop-and-start nature of running each code chunk separately can also be a small irritation."
  },
  {
    "objectID": "tip_notebook-vs-script.html#script-vs.-markdown-summary",
    "href": "tip_notebook-vs-script.html#script-vs.-markdown-summary",
    "title": "Notebooks versus Scripts",
    "section": "Script vs. Markdown Summary",
    "text": "Script vs. Markdown Summary\nTaken together, both R scripts and R markdown files can empower users to write reproducible, transparent code. However, both file types have some key limitations that should be taken into consideration when choosing which to use as you set out to create a new code product."
  },
  {
    "objectID": "tip_names.html",
    "href": "tip_names.html",
    "title": "Good Naming Conventions",
    "section": "",
    "text": "When you first start working on a project with your group members, figuring out what to name your folders/files may not be at the top of your priority list. However, following a good naming convention will allow team members to quickly locate files and figure out what they contain. The organized naming structure will also allow new members of the group to be onboarded more easily!"
  },
  {
    "objectID": "tip_names.html#overview",
    "href": "tip_names.html#overview",
    "title": "Good Naming Conventions",
    "section": "",
    "text": "When you first start working on a project with your group members, figuring out what to name your folders/files may not be at the top of your priority list. However, following a good naming convention will allow team members to quickly locate files and figure out what they contain. The organized naming structure will also allow new members of the group to be onboarded more easily!"
  },
  {
    "objectID": "tip_names.html#naming-tips",
    "href": "tip_names.html#naming-tips",
    "title": "Good Naming Conventions",
    "section": "Naming Tips",
    "text": "Naming Tips\nHere is a summary of some naming tips that we recommend. These were taken from the Reproducibility Best Practices module in the LTER’s SSECR course. Please feel free to refer to the aforementioned link for more information.\n\nNames should be informative\n\nAn ideal file name should give some information about the file’s contents, purpose, and relation to other project files.\nFor example, if you have a bunch of scripts that need to be run in order, consider adding step numbers to the start of each file name (e.g., “01_harmonize_data.R” or “step01_harmonize_data.R”).\n\nNames should avoid spaces and special characters\n\nSpaces and special characters (e.g., é, ü, etc.) in folder/file names may cause errors when someone with a Windows computer tries to read those file paths. You can replace spaces with delimiters like underscores or hyphens to increase machine readability.\n\nFollow a consistent naming convention throughout!\n\nIf you and your group members find a naming convention that works, stick with it! Having a consistent naming convention is key to getting new collaborators to follow it."
  },
  {
    "objectID": "pages_deprecated/wg_sci-comp-def.html",
    "href": "pages_deprecated/wg_sci-comp-def.html",
    "title": "What is Scientific Computing?",
    "section": "",
    "text": "In the most fundamental sense, scientific computing refers to the process of organizing, managing, and analyzing scientific data using computers. This process takes place at the interface between several distinct disciplines:\n\nInformatics provides both a planning framework and operational rules for acquiring, handling, interpreting, and storing the underlying information in a useful and efficient manner\nComputer science and the technologies it produces provide the overarching computational environment, including the core processing “engine” and a means to control it\nRelevant branches of mathematics, statistics, and probability arm us with numerical models, algorithms, error representations, and other constructs for describing and solving problems quantitatively\nLast but not least, the target science itself (e.g., ecology, geosciences, evolutionary biology, etc.) provides an overall motivation, research paradigm, and conceptual model to guide the analysis\n\nWith these many links to other fields, modern scientific computing is now viewed as a field of study unto itself, constantly evolving in step with advances in the disciplines on which it is based."
  },
  {
    "objectID": "pages_deprecated/wg_infrastructure.html",
    "href": "pages_deprecated/wg_infrastructure.html",
    "title": "Computing Infrastructure",
    "section": "",
    "text": "NCEAS offers an array of helpful computing infrastructure for your group. These resources are purely optional but can be a significant aid to your projects’ progression both when you’re in-person in Santa Barbara and between those meetings! Our team is ready and willing to explain these in greater detail or help your group build them into your workflow so don’t hesitate to reach out!"
  },
  {
    "objectID": "pages_deprecated/wg_infrastructure.html#analytical-servers",
    "href": "pages_deprecated/wg_infrastructure.html#analytical-servers",
    "title": "Computing Infrastructure",
    "section": "Analytical Servers",
    "text": "Analytical Servers\nWorking groups have access to several high-performance computers at NCEAS, which provide advanced analytical, web, and database capabilities that far exceed the capabilities of desktop computers. We are available to set up and instruct you in the use of these Linux/Unix systems for demanding scientific analyses and modeling runs that benefit from lots of memory or storage, or access to multiple CPU’s.\n\nOur most powerful system (pictured) currently offers 384GB of RAM memory, along with 44 cores (CPUs), and several terabytes of fast storage. Several scientific software packages – such as R/RStudio, Python, Matlab, and QGIS – are already installed. Ask us if you need us to install any specific analytical libraries or packages.\nWe can also set up shared storage space on our server to facilitate data sharing within your working group. This is particularly valuable for large files that would take up a significant amount of your group’s cloud storage system (e.g., Google Drive, Dropbox, etc.)\nTo request access to the NCEAS analytical server, please coordinate with your PIs and contact us with your request."
  },
  {
    "objectID": "pages_deprecated/wg_infrastructure.html#data-preservation",
    "href": "pages_deprecated/wg_infrastructure.html#data-preservation",
    "title": "Computing Infrastructure",
    "section": "Data Preservation",
    "text": "Data Preservation\nWhile we know that you are just getting started, it is never too early to start thinking about where you are going to store the data your group will collate and synthesize! NCEAS is committed to practicing and promoting open science, making scientific research and its supporting data and information accessible to all levels of society. Therefore, we recommend that any input data used for your synthesis work be well documented and preserved in a long-term data repository.\n\nIt is required to document and preserve any products resulting from working group activities in a long-term data repository, such as the Environmental Data Initiative (EDI) data repository or the NCEAS-supported KNB Data Repository, which are part of the Data Observation Network for Earth (DataONE).\n\nWe are happy to discuss which of these options might fit your group best though we recognize this discussion can probably be left alone until your group has amassed a substantial amount of data."
  },
  {
    "objectID": "tip_paths.html",
    "href": "tip_paths.html",
    "title": "Reproducible File Paths",
    "section": "",
    "text": "This section contains our recommendations for handling file paths. When you code collaboratively (e.g., with GitHub), accounting for the difference between your folder structure and those of your colleagues becomes critical. Ideally your code should be completely agnostic about (1) the operating system of the computer it is running on (i.e., Windows vs. Mac) and (2) the folder structure of the computer. We can–fortunately–handle these two considerations relatively simply. This may seem somewhat dry but it is worth mentioning that failing to use relative file paths is a significant hindrance to reproducibility (see Trisovic et al. 2022).\nYou may also find our tutorial on storing user-specific information valuable in this context."
  },
  {
    "objectID": "tip_paths.html#overview",
    "href": "tip_paths.html#overview",
    "title": "Reproducible File Paths",
    "section": "",
    "text": "This section contains our recommendations for handling file paths. When you code collaboratively (e.g., with GitHub), accounting for the difference between your folder structure and those of your colleagues becomes critical. Ideally your code should be completely agnostic about (1) the operating system of the computer it is running on (i.e., Windows vs. Mac) and (2) the folder structure of the computer. We can–fortunately–handle these two considerations relatively simply. This may seem somewhat dry but it is worth mentioning that failing to use relative file paths is a significant hindrance to reproducibility (see Trisovic et al. 2022).\nYou may also find our tutorial on storing user-specific information valuable in this context."
  },
  {
    "objectID": "tip_paths.html#preserve-file-paths-as-objects",
    "href": "tip_paths.html#preserve-file-paths-as-objects",
    "title": "Reproducible File Paths",
    "section": "1. Preserve File Paths as Objects",
    "text": "1. Preserve File Paths as Objects\nDepending on the operating system of the computer, the slashes between folder names are different (\\ versus /). The file.path function automatically detects the computer operating system and inserts the correct slash. We recommend using this function and assigning your file path to an object.\n\nmy_path &lt;- file.path(\"path\", \"to\", \"my\", \"file\")\nmy_path\n\n[1] \"path/to/my/file\"\n\n\nOnce you have that path object, you can use it everywhere you import or export information to/from the code (with another use of file.path to get the right type of slash!).\n\n# Import\nmy_raw_data &lt;- read.csv(file = file.path(my_path, \"raw_data.csv\"))\n\n# Export\nwrite.csv(x = data_object, file = file.path(my_path, \"tidy_data.csv\"))"
  },
  {
    "objectID": "tip_paths.html#create-necessary-sub-folders-in-the-code",
    "href": "tip_paths.html#create-necessary-sub-folders-in-the-code",
    "title": "Reproducible File Paths",
    "section": "2. Create Necessary Sub-Folders in the Code",
    "text": "2. Create Necessary Sub-Folders in the Code\nUsing file.path guarantees that your code will work regardless of the upstream folder structure but what about the folders that you need to export or import things to/from? For example, say your graphs.R script saves a couple of useful exploratory graphs to the “Plots” folder, how would you guarantee that everyone running graphs.R has a “Plots folder”? You can use the dir.create function to create the folder in the code (and include your path object from step 1!).\n\n# Create needed folder\ndir.create(path = file.path(my_path, \"Plots\"), showWarnings = FALSE)\n\n# Then export to that folder\nggplot2::ggsave(filename = file.path(my_path, \"Plots\", \"my_plot.png\"))\n\nThe showWarnings argument of dir.create simply warns you if the folder you’re creating already exists or not. There is no negative to “creating” a folder that already exists (nothing is overwritten!!) but the warning can be confusing so we can silence it ahead of time."
  },
  {
    "objectID": "tip_paths.html#file-paths-summary",
    "href": "tip_paths.html#file-paths-summary",
    "title": "Reproducible File Paths",
    "section": "File Paths Summary",
    "text": "File Paths Summary\nWe strongly recommend following these guidelines so that your scripts work regardless of (1) the operating system, (2) folders “upstream” of the working directory, and (3) folders within the project. This will help your code by flexible and reproducible when others are attempting to re-run your scripts!"
  },
  {
    "objectID": "internal_deploy-shiny.html",
    "href": "internal_deploy-shiny.html",
    "title": "Deploy Shiny Apps",
    "section": "",
    "text": "Note\n\n\n\nPlease note that the following instructions for deploying Shiny apps are meant to serve as internal documentation and will only work if you have sudo power on the SciComp team’s Shiny server.\nFeel free to contact us if you have a LTER-related Shiny app that you would like to deploy on our server!\nSciComp team members can deploy LTER-related Shiny apps on our server at: https://shiny.lternet.edu/\nTo deploy your working app, first make sure that all the files live at a GitHub repository. Once our sysadmin Nick Outin has made you an account on the server, you can log in via SSH."
  },
  {
    "objectID": "internal_deploy-shiny.html#log-in",
    "href": "internal_deploy-shiny.html#log-in",
    "title": "Deploy Shiny Apps",
    "section": "Log In",
    "text": "Log In\nFor Mac users, open the Terminal app and type the following command, replacing &lt;YOUR-USERNAME&gt; with your own username.\n\nssh &lt;YOUR-USERNAME&gt;@shiny.lternet.edu\n\nWindows users can log in with PuTTY, using shiny.lternet.edu as the host name.\nCheck out NCEAS’ guide to using SSH for additional help."
  },
  {
    "objectID": "internal_deploy-shiny.html#set-up-proper-permissions",
    "href": "internal_deploy-shiny.html#set-up-proper-permissions",
    "title": "Deploy Shiny Apps",
    "section": "Set Up Proper Permissions",
    "text": "Set Up Proper Permissions\n\nJoin shiny-apps\nA shiny-apps user group has been created in this server. Set up the proper permissions for your account by adding yourself to this group with this command.\n\nsudo usermod -aG shiny-apps &lt;YOUR-USERNAME&gt;\n\nTo decipher this command a bit: sudo will enable you to run commands as a user with full control and privileges. The usermod command is used to modify user account details. The basic syntax is:\n\nusermod [OPTIONS] &lt;YOUR-USERNAME&gt;\n\nThe -G option will add the user to a supplementary group. The -aG combined options will add the user to the new supplementary group while also leaving them in the other supplementary group(s) they were already a part of.\n\n\n\n\n\n\nNote\n\n\n\nAs an FYI, the apps on the server will run as the user shiny, so shiny is also in the shiny-apps user group.\n\n\n\n\nAllow Access to Your Home Directory\nAdditionally, you need to allow shiny access to your user home directory by running:\n\nsudo chmod a+x /home/&lt;YOUR-USERNAME&gt;\n\nThis chmod (change mode) command is used to control file permissions. Basic syntax:\n\nchmod [OPTIONS] MODE FILE/DIRECTORY\n\nThere are two “modes” that you can use to set permissions: Symbolic and Octal mode. The Symbolic mode uses operators and letters. For example, the a option denotes all the owner/groups the file/directory belongs to. The + operator grants the permission, and the letter x stands for the execute permission."
  },
  {
    "objectID": "internal_deploy-shiny.html#tell-git-who-you-are",
    "href": "internal_deploy-shiny.html#tell-git-who-you-are",
    "title": "Deploy Shiny Apps",
    "section": "Tell git Who You Are",
    "text": "Tell git Who You Are\nNow that your account has the proper permissions, use the git config commands to tell git who you are.\n\ngit config --global user.name \"&lt;YOUR-GITHUB-USERNAME&gt;\"\ngit config --global user.email \"&lt;YOUR-GITHUB-EMAIL&gt;\"\n\nReplace &lt;YOUR-GITHUB-USERNAME&gt; and &lt;YOUR-GITHUB-EMAIL&gt; with your own credentials. You can check to see if you entered your details correctly by entering git config --list."
  },
  {
    "objectID": "internal_deploy-shiny.html#copy-your-app-to-the-server",
    "href": "internal_deploy-shiny.html#copy-your-app-to-the-server",
    "title": "Deploy Shiny Apps",
    "section": "Copy Your App to the Server",
    "text": "Copy Your App to the Server\nThe easiest way to get all the files for your app to the server is by git clone.\nIf you like, you can create a folder named “github” to store all your future apps. For example, in the screenshot below, I created a “github” folder using the mkdir command in my home directory.\nTo check that the folder was created, I can list all the files/folders in my current directory with ls. Since I want my app to be inside the “github” folder, I used the cd command to change into that directory.\nFinally, git clone the GitHub repo that has all the files for your app. Here, I am cloning the lterpalettefinder-shiny repo to my local directory on the virtual machine as lterpalettefinder.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can go one level above the current directory by typing cd .."
  },
  {
    "objectID": "internal_deploy-shiny.html#install-necessary-r-packages",
    "href": "internal_deploy-shiny.html#install-necessary-r-packages",
    "title": "Deploy Shiny Apps",
    "section": "Install Necessary R Packages",
    "text": "Install Necessary R Packages\nAfter you cloned the repository for your app, you can start installing the necessary R packages! To make these packages available for all users, you will want to execute commands with the root user’s privileges by typing the sudo -i command. Then open R by simply typing “R” and install packages with the usual install.packages() function.\n\n\nMissing Dependencies?\nIf you run into an error installing a R package, it’s likely because the server does not have the required dependencies installed yet.\nFor example, I wanted to install the lterpalettefinder R package, but I got lots of errors on missing dependencies instead.\n\nI saw that the curl dependency was missing, so I attempted to install that. However, R gives me another error.\n\nLooking closer, it asks me to install libcurl4-openssl-dev first. To install this Ubuntu package, I exited R with q() and logged off root with exit. Once I’m back in my own user profile, I can use the sudo apt install command to install libcurl4-openssl-dev.\n\nIf you get these prompts to restart services, you can tap Return/Enter to continue the installation.\n\n\nAfterwards, libcurl4-openssl-dev installed successfully, and then I can finally install the missing curl dependency in R!\n\nYou can repeat a similar process to find and install the rest of the required dependencies before you are able to install certain R packages."
  },
  {
    "objectID": "internal_deploy-shiny.html#symlink-to-deployed-folder",
    "href": "internal_deploy-shiny.html#symlink-to-deployed-folder",
    "title": "Deploy Shiny Apps",
    "section": "Symlink to Deployed Folder",
    "text": "Symlink to Deployed Folder\nSince all the Shiny apps are located under /srv/shiny-server/, how can we deploy the app we have in our local directory? We can create a symlink (symbolic link) between the local directory and /srv/shiny-server/. A symlink is essentially a pointer to other folders.\nCreate the link by running this command:\n\nsudo ln -s &lt;LOCAL-DIRECTORY-OF-APP&gt; &lt;SHINY-SERVER-DIRECTORY-OF-APP&gt;\n\nReplace &lt;LOCAL-DIRECTORY-OF-APP&gt; with your own directory and replace &lt;SHINY-SERVER-DIRECTORY-OF-APP&gt; with the file path of where you would like your app to deploy.\nFor example, the actual lterpalettefinder app lives under my home directory, but it needs to be deployed under /srv/shiny-server/, so I ran this command to link the two:\n\nNow /srv/shiny-server/lterpalettefinder points to /home/anchen/github/lterpalettefinder!\nYou can check the current symlinks by navigating to /srv/shiny-server/ and typing ls -l.\n\n\n\n\n\n\nNote\n\n\n\nThe name of the deployed folder corresponds to the URL: https://shiny.lternet.edu/&lt;YOUR-APP&gt;/"
  },
  {
    "objectID": "internal_deploy-shiny.html#debug-app-and-check-its-live-link",
    "href": "internal_deploy-shiny.html#debug-app-and-check-its-live-link",
    "title": "Deploy Shiny Apps",
    "section": "Debug App and Check its Live Link",
    "text": "Debug App and Check its Live Link\nIf everything goes right, your app will be live at https://shiny.lternet.edu/&lt;YOUR-APP&gt;/! If not, don’t worry and try troubleshooting what went wrong. Remember to check file paths and required R packages."
  },
  {
    "objectID": "tutorial_quarto-website.html",
    "href": "tutorial_quarto-website.html",
    "title": "Making a Website with Quarto",
    "section": "",
    "text": "Quarto is a new tool developed by RStudio (the company, not the program) to create a more ‘what you see is what you get’ editor for creating markdown files and products (e.g., books, websites, etc.). Additionally, it includes a visual editor that allows users to insert headings and embed figures via buttons that are intuitively labeled rather than through somewhat arcane HTML text or symbols. While Quarto is still in its infancy, it is rapidly gathering a following due to the aforementioned visual editor and for the ease with which quarto documents and websites can be created.\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDo all of the pre-workshop steps relevant to our “Collaborative Coding with GitHub” workshop\nDownload Quarto\n\nFeel free to skip any steps that you have already completed!"
  },
  {
    "objectID": "tutorial_quarto-website.html#overview",
    "href": "tutorial_quarto-website.html#overview",
    "title": "Making a Website with Quarto",
    "section": "",
    "text": "Quarto is a new tool developed by RStudio (the company, not the program) to create a more ‘what you see is what you get’ editor for creating markdown files and products (e.g., books, websites, etc.). Additionally, it includes a visual editor that allows users to insert headings and embed figures via buttons that are intuitively labeled rather than through somewhat arcane HTML text or symbols. While Quarto is still in its infancy, it is rapidly gathering a following due to the aforementioned visual editor and for the ease with which quarto documents and websites can be created.\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDo all of the pre-workshop steps relevant to our “Collaborative Coding with GitHub” workshop\nDownload Quarto\n\nFeel free to skip any steps that you have already completed!"
  },
  {
    "objectID": "tutorial_quarto-website.html#create-a-quarto-website-r-project",
    "href": "tutorial_quarto-website.html#create-a-quarto-website-r-project",
    "title": "Making a Website with Quarto",
    "section": "Create a Quarto Website R Project",
    "text": "Create a Quarto Website R Project\nTo begin, click the “Project” button in the top right of your RStudio session.\n\nIn the resulting dialogue, click the “New Directory” option.\n\n\n\nFrom the list of options for project templates, select “Quarto Website”.\n\n\n\nPick a title and check the “Create a git repository” checkbox. For your title, short but descriptive titles are most effective. Once that is done, click “Create Project” in the bottom right of the window.\n\n\n\nAfter a few seconds, RStudio should refresh with a Quarto document (such documents have the file extension “.qmd”) and a “_quarto.yml” file open.\n\n\n\nPart of Quarto’s central philosophy is that all of the formatting of individual .qmd files in a project is governed by the settings created by a singular .yml file. In an R markdown project some of the global settings are set in .yml but other settings are handled within each .Rmd file. This centralization is a key innovation in streamlining projects and is one reason for Quarto’s quick popularity."
  },
  {
    "objectID": "tutorial_quarto-website.html#preparing-project-for-web-deployment",
    "href": "tutorial_quarto-website.html#preparing-project-for-web-deployment",
    "title": "Making a Website with Quarto",
    "section": "Preparing Project for Web Deployment",
    "text": "Preparing Project for Web Deployment\nTo prepare your project for web deployment via GitHub Pages, we have three quick steps that we must first complete.\nFirst, in the “_quarto.yml” file, add output-dir: docs as a subheading beneath the project: heading. Make sure that the indentation is the same as the type: website but the new line can be either above or below that line.\n\n\n\nSecond, in the “Terminal” pane run touch .nojekyll. This creates a file called “.nojekyll” that is necessary for hosting your website via GitHub Pages.\nThird, in the “Terminal” pane run quarto render. This processes the template .qmd files you currently have in the repository and prepares them to become actual web pages.\nOnce you’ve done these three things you can move on to creating a GitHub repository so that we can take the necessary steps to having GitHub host your website!"
  },
  {
    "objectID": "tutorial_quarto-website.html#make-a-new-github-repository",
    "href": "tutorial_quarto-website.html#make-a-new-github-repository",
    "title": "Making a Website with Quarto",
    "section": "Make a New GitHub Repository",
    "text": "Make a New GitHub Repository\nFrom your GitHub “Repositories” tab, click the  green  “New” button.\n\nAdd a title to your repository and add a description. Once you’ve added these two things, scroll down and click the  green  “Create repository” button.\n\n\n\nBe sure that you do not add a README, do not add a gitignore, and do not add a license. Adding any of these three will cause a merge conflict when we link the project that you just created with the GitHub repository that you are in the process of creating.\n\n\n\nAfter a few seconds you should be placed on your new repository’s landing page which will look like the below image because there isn’t anything in your repository (yet).\nCopy the link in the field and go back to your RStudio session."
  },
  {
    "objectID": "tutorial_quarto-website.html#adding-your-project-to-github",
    "href": "tutorial_quarto-website.html#adding-your-project-to-github",
    "title": "Making a Website with Quarto",
    "section": "Adding your Project to GitHub",
    "text": "Adding your Project to GitHub\nThe following steps include a sequence of command line operations that will be relayed in code chunks below. Unless otherwise stated, all of the following code should be run in “Terminal”.\nIf you didn’t check the “Create a git repository” button while creating the R project, you’ll need to do that via the command line now. If you did check that box, you should skip this step!\n\n# Start a git repository on the \"main\" branch\ngit init -b main\n\nStage all of the files in your project to the git repository. This includes the .yml file, all .qmd files and all of their rendered versions created when you ran quarto render earlier. This code is equivalent to checking the box for the files in the “Git” pane of RStudio.\n\n# Stage all files\ngit add .\n\nOnce everything has been staged, you now must commit those staged files with a message.\n\n# Commit all files with the message in quotes\ngit commit -m \"Initial commit\"\n\nNow that your project files have been committed, you need to tell your computer where you will be pushing to and pulling from. Paste the link you copied at the end of the “Make a New GitHub Repository” into the code shown in the chunk below (instead of GITHUB_URL) and run it.\n\n# Tell your computer which GitHub repository to connect to\ngit remote add origin GITHUB_URL\n\nVerify that URL before continuing.\n\n# Confirm that URL worked\ngit remote -v\n\nFinally, push your commited changes to the repostory that you set as the remote in the preceding two steps.\n\n# Push all of the content to the main branch\ngit push -u origin main\n\nNow, go back to GitHub and refresh the page to see your project content safe and sound in your new GitHub repository!"
  },
  {
    "objectID": "tutorial_quarto-website.html#deploy-website-via-github",
    "href": "tutorial_quarto-website.html#deploy-website-via-github",
    "title": "Making a Website with Quarto",
    "section": "Deploy Website via GitHub",
    "text": "Deploy Website via GitHub\nIn order to get your new website actually on the web, we’ll need to tell GitHub that we want our website to be accessible at a .github.io URL.\nTo do this, go to the “Settings” tab with a gear icon and click it. You may be prompted to re-enter your GitHub password, do so and you can proceed.\n\nIn the resulting page, look towards the bottom of the left sidebar of settings categories and click the “Pages” option. This is at the very bottom of the sidebar in the screen capture below but is towards the middle of all of the settings categories Github offers you.\n\nScroll down to the middle of this page and where it says “Branch” click the dropdown menu that says “None” by default.\n\nSelect “main” from the dropdown.\n\n\n\nThis opens up a new dropdown menu where you can select which folder in your repository contains your website’s content (it defaults to “/ (root)”). Because we specified output-dir: docs in the .yml file earlier we can select “/docs” from the dropdown menu.\n\n\n\nOnce you’ve told GitHub that you want a website generated from the “docs” folder on the main branch, click the “Save” button.\n\n\n\nFrom this moment your website has begun being deployed by GitHub! You can check the status of the building process by navigating to the “Actions” tab of your repository.\nSelect the “pages build and deployment workflow” in the list of workflows on the bottom righthand side of the page.\n\nThis shows you GitHub’s building and deployment process as a flowchart. While it is working on each step there will be an amber circle next to the name of that sub-task. When a sub-task is completed, the amber circle becomes a green circle with a check mark.\n\nWhen the three steps are complete the amber clock symbol next to the “pages build and deployment” action will turn into a larger green circle with a check mark. This is GitHub’s way of telling you that your website is live and accessible to anyone on the internet.\n\nYou can now visit your website by visiting its dedicated URL. This URL can be found by returning to the “Settings” tab and then scrolling through the sidebar to the “Pages” section.\nAlternately, the website for your repository always uses the following composition: https://repository owner.github.io/repository name/\n\nIf we visit that link, we can see that our website is live!\n\n\nGitHub Housekeeping\nWe recommend a quick housekeeping step now to make it easier to find this URL in the future. Copy the URL from the Pages setting area and return to the “Code” tab of the repository.\nOnce there, click the small gear icon to the right of the “About” header.\n\nIn the resulting window, paste the copied URL into the “Website” field. Once you’ve pasted it in, click the green “Save changes” button.\n\n\n\nThis places the link to your deployed website in an intuitive, easy-to-find location both for interested third parties and yourself in the future."
  },
  {
    "objectID": "tutorial_quarto-website.html#adding-website-content",
    "href": "tutorial_quarto-website.html#adding-website-content",
    "title": "Making a Website with Quarto",
    "section": "Adding Website Content",
    "text": "Adding Website Content\nNow that you have a live website you can build whatever you’d like! Given the wide range of possibility, we’ll only cover how to add a new page but the same process applies to any edit to the living webpage.\nTo add a new page create a new Quarto document. You can do this by going to the “File” menu, entering the “New File” options, and selecting “Quarto Document…”\n\n\n\nSimilarly to an R markdown file, this will open a new window that lets you enter a title and author as well as decide what format you want to render files to along with some other settings options. You only need to click the “Create” button in the bottom right of this dialogue (though you can definitely play with the other options and text boxes as you desire).\n\n\n\nAfter a moment, a new .qmd file will open in Quarto’s visual editor. For the purposes of this tutorial, you only need to add a title in the top of the file but for a real website you can add whatever content sparks joy for you!\n\n\n\nSave that file into your project folder. Its name can be anything but be sure that you remember what you name it!\n\n\n\nAdd the name of the new Quarto document to the .yml file in the website navbar area (in this example the file is called “more-stuff.qmd”).\n\n\n\nOnce you’ve added the file to the fundamental architecture of your website, you need to tell Quarto to re-build the part of the website that GitHub looks for when it deploys. To do this run quarto render in the Terminal.\nIf you want to preview your changes, run quarto preview in the Terminal and a new browser window will be displayed showing your current website content. This preview continues until you click the red stop sign icon in RStudio so be sure to end it when you’re done with the preview!\n\nRegardless, once you’ve run either quarto render or quarto preview you need to stage and commit all changed files indicated in the Git pane of RStudio. As a reminder, to stage files you check the box next to them, to commit staged files, type an informative message and press the “Commit” button in the right side of the window.\n\nSwitch back to GitHub and you’ll see an amber dot next to the commit hash just beneath and to the left of the green “Code” button.\n\nWhen the amber dot turns into a green check mark that means that your edits to your website are now included in the live version of your site!\n\nWhen you visit your website you may need to refresh the page for your edits to appear but all new visitors will see the updated content when they load the page."
  },
  {
    "objectID": "tutorial_quarto-website.html#supplementary-information",
    "href": "tutorial_quarto-website.html#supplementary-information",
    "title": "Making a Website with Quarto",
    "section": "Supplementary Information",
    "text": "Supplementary Information\nQuarto is developing at a rapid pace so quality of life changes and new functionalities are introduced relatively frequently. Additionally, Quarto supports user-created “extensions” that can be downloaded in a given project and then used (similar to the way user-developed R packages can be shared) so if you want to do something that Quarto itself doesn’t support, chances are you’ll be able to find an extension that handles it.\nQuarto’s documentation of website creation and formatting is extremely thorough and is a great resource as you become more comfortable with your new website. We hope this tutorial was useful to you and welcome constructively critical feedback! Please post an issue with any thoughts for improvement that you have."
  },
  {
    "objectID": "wg_services.html",
    "href": "wg_services.html",
    "title": "Working Together",
    "section": "",
    "text": "We are excited to work with your team to help you develop reproducible workflows to process, harmonize, and analyze the large amount of (heterogeneous) data necessary to conduct synthesis science. Those reproducible workflows will help you to integrate new information more easily, iterate more quickly to test your various hypotheses, and enable you to better collaborate. We currently offer a range of options for what our collaboration might look like.\nThe categories below are not exhaustive so if you think your needs will fall between or outside of these tasks, please don’t hesitate to contact our team to start that conversation!"
  },
  {
    "objectID": "wg_services.html#tasks",
    "href": "wg_services.html#tasks",
    "title": "Working Together",
    "section": "Tasks",
    "text": "Tasks\nThis level of collaboration is the core of our value to working groups! When your group identifies a data-related need (e.g., designing an analytical workflow, creating a website, writing an R Shiny app, etc.), you reach out to our team and get the conversation started. During that time we will work closely with you to define the scope of the work and get a clear picture of what “success” looks like in this context.\nOnce the task is appropriately defined, the conversation moves on to how independently you’d like us to work. This varies dramatically between tasks even within a single working group and there is no single right answer! For some tasks, we are capable of working completely independently and returning to your team with a finished product in hand for review but we are equally comfortable working closely with you throughout the life-cycle of a task."
  },
  {
    "objectID": "wg_services.html#analytical-sprints",
    "href": "wg_services.html#analytical-sprints",
    "title": "Working Together",
    "section": "Analytical Sprints",
    "text": "Analytical Sprints\nFor more intense support, we offer “analytical sprints.” If your group requests an analytical sprint, you will get one of our team members working full time only for your group’s tasks for 3-4 weeks! This can be a great option if your group has many smaller tasks or fewer larger projects that are particularly time-sensitive.\nWe are excited that these sprints are a part of our menu of offerings to you all but please reach out to us to start the conversation before requesting a sprint so that we can make sure we are all on the same page."
  },
  {
    "objectID": "wg_services.html#weekly-office-hours",
    "href": "wg_services.html#weekly-office-hours",
    "title": "Working Together",
    "section": "Weekly Office Hours",
    "text": "Weekly Office Hours\nEach of our staff members offers a one-hour block weekly as a standing office hour each week. This is a great time to join us with small hurdles or obstacles you’re experiencing in a given week. For example, previous office hours have dealt with topics like refreshing on Git/GitHub vocabulary, authenticating the googledrive R package, or solving a specific error in a new R script."
  },
  {
    "objectID": "wg_services.html#trainings",
    "href": "wg_services.html#trainings",
    "title": "Working Together",
    "section": "Trainings",
    "text": "Trainings\nWe have three primary methods for helping your group further develop skills in reproducible data science to enable your team to better collaborate and efficiently tackle data and analytical challenges. To access a category of our skill development content, click the corresponding dropdown menu in the navbar at the top of this website.\nWe are also happy to design new workshops, tutorials, or tips if your group wants training on something within our knowledge base for which we haven’t yet built something.\n\nWorkshops\nWorkshops are typically done remotely and last 2-3 hours but we can be flexible with that timing depending on your group’s needs. We can also accommodate time during one of your meetings if desired.\n\n\nTutorials\nThe tutorials tend to be smaller in scope than the full workshops and more ‘go at your own pace’-style. Despite that, they are still built to maximize value to your group either as review or first contact with a given subject.\n\n\nTips\nFinally, we’ve also curated a set of ‘code tips’ that are even smaller in scale than the tutorials. These are often just a short summary of our team’s opinion on a given subject."
  },
  {
    "objectID": "wg_facilitation.html",
    "href": "wg_facilitation.html",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "",
    "text": "Facilitating a working group meeting is both hugely rewarding and a significant effort. NCEAS and the LTER Network have gathered some guiding resources that may prove to be helpful to your meeting. See the sub-sections below for more details."
  },
  {
    "objectID": "wg_facilitation.html#lter-network-office-guidance",
    "href": "wg_facilitation.html#lter-network-office-guidance",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "LTER Network Office Guidance",
    "text": "LTER Network Office Guidance\nThe LTER Network Office (LNO) has curated some useful resources on their website (lternet.edu) that we strongly recommend exploring! In particular, the LNO has centralized resources for working group primary investigators (PIs) (see here). Additionally, the LNO has created some broader resources for all members of working groups (see here).\nThe LTER Code of Conduct is also publicly available if you’d like to explore it."
  },
  {
    "objectID": "wg_facilitation.html#nceas-resources-for-working-groups",
    "href": "wg_facilitation.html#nceas-resources-for-working-groups",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "NCEAS’ Resources for Working Groups",
    "text": "NCEAS’ Resources for Working Groups\nNCEAS hosts many working groups and so has drawn from its wealth of experience to assemble a set of useful information on its Resources for Working Groups page.\nIn particular, we recommend checking out the documents nested in the “How to Run a Working Group” dropdown menu. Your group may also want to check out the “Reimbursement Forms” dropdown after your meetings in Santa Barbara to ensure that your eligible expenses get reimbursed!"
  },
  {
    "objectID": "wg_facilitation.html#diversity-equity-and-inclusion",
    "href": "wg_facilitation.html#diversity-equity-and-inclusion",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "Diversity, Equity, and Inclusion",
    "text": "Diversity, Equity, and Inclusion\nThe LTER Network and NCEAS are committed to improving diversity and inclusion in science. For the LTER Network Office’s diversity, equity and inclusion resources see here. For more information on NCEAS’ commitment to this mission–and related links–please visit NCEAS’ DEIJ page."
  },
  {
    "objectID": "wg_facilitation.html#visiting-santa-barbara",
    "href": "wg_facilitation.html#visiting-santa-barbara",
    "title": "Meeting Facilitation & Santa Barbara Tips",
    "section": "Visiting Santa Barbara",
    "text": "Visiting Santa Barbara\nWhen your group is visiting at NCEAS’ office in beautiful Santa Barbara there are additional resources available to your group and opportunities for recreation and bonding as a team! Check out NCEAS’ general computing resources page for a quick rundown of some resources available only in Santa Barbara (including how to get on the WiFi!).\nWe very much recommend taking an afternoon or morning on one of the days of your visit to get out and explore Santa Barbara! NCEAS’ page for things to do in SB for new employees might prove to be a helpful resource for you and your group as you’re deciding where to eat and what to do!"
  },
  {
    "objectID": "modules_tutorials/quarto-website_add-content.html",
    "href": "modules_tutorials/quarto-website_add-content.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "Now that you have a live website you can build whatever you’d like! Given the wide range of possibility, we’ll only cover how to add a new page but the same process applies to any edit to the living webpage.\nTo add a new page create a new Quarto document. You can do this by going to the “File” menu, entering the “New File” options, and selecting “Quarto Document…”\n\n\n\nSimilarly to an R markdown file, this will open a new window that lets you enter a title and author as well as decide what format you want to render files to along with some other settings options. You only need to click the “Create” button in the bottom right of this dialogue (though you can definitely play with the other options and text boxes as you desire).\n\n\n\nAfter a moment, a new .qmd file will open in Quarto’s visual editor. For the purposes of this tutorial, you only need to add a title in the top of the file but for a real website you can add whatever content sparks joy for you!\n\n\n\nSave that file into your project folder. Its name can be anything but be sure that you remember what you name it!\n\n\n\nAdd the name of the new Quarto document to the .yml file in the website navbar area (in this example the file is called “more-stuff.qmd”).\n\n\n\nOnce you’ve added the file to the fundamental architecture of your website, you need to tell Quarto to re-build the part of the website that GitHub looks for when it deploys. To do this run quarto render in the Terminal.\nIf you want to preview your changes, run quarto preview in the Terminal and a new browser window will be displayed showing your current website content. This preview continues until you click the red stop sign icon in RStudio so be sure to end it when you’re done with the preview!\n\nRegardless, once you’ve run either quarto render or quarto preview you need to stage and commit all changed files indicated in the Git pane of RStudio. As a reminder, to stage files you check the box next to them, to commit staged files, type an informative message and press the “Commit” button in the right side of the window.\n\nSwitch back to GitHub and you’ll see an amber dot next to the commit hash just beneath and to the left of the green “Code” button.\n\nWhen the amber dot turns into a green check mark that means that your edits to your website are now included in the live version of your site!\n\nWhen you visit your website you may need to refresh the page for your edits to appear but all new visitors will see the updated content when they load the page."
  },
  {
    "objectID": "modules_tutorials/github-create.html",
    "href": "modules_tutorials/github-create.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "From your GitHub “Repositories” tab, click the  green  “New” button.\n\nAdd a title to your repository and add a description. Once you’ve added these two things, scroll down and click the  green  “Create repository” button.\n\n\n\nBe sure that you do not add a README, do not add a gitignore, and do not add a license. Adding any of these three will cause a merge conflict when we link the project that you just created with the GitHub repository that you are in the process of creating.\n\n\n\nAfter a few seconds you should be placed on your new repository’s landing page which will look like the below image because there isn’t anything in your repository (yet).\nCopy the link in the field and go back to your RStudio session."
  },
  {
    "objectID": "modules_tutorials/github-website-deploy.html",
    "href": "modules_tutorials/github-website-deploy.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "In order to get your new website actually on the web, we’ll need to tell GitHub that we want our website to be accessible at a .github.io URL.\nTo do this, go to the “Settings” tab with a gear icon and click it. You may be prompted to re-enter your GitHub password, do so and you can proceed.\n\nIn the resulting page, look towards the bottom of the left sidebar of settings categories and click the “Pages” option. This is at the very bottom of the sidebar in the screen capture below but is towards the middle of all of the settings categories Github offers you.\n\nScroll down to the middle of this page and where it says “Branch” click the dropdown menu that says “None” by default.\n\nSelect “main” from the dropdown.\n\n\n\nThis opens up a new dropdown menu where you can select which folder in your repository contains your website’s content (it defaults to “/ (root)”). Because we specified output-dir: docs in the .yml file earlier we can select “/docs” from the dropdown menu.\n\n\n\nOnce you’ve told GitHub that you want a website generated from the “docs” folder on the main branch, click the “Save” button.\n\n\n\nFrom this moment your website has begun being deployed by GitHub! You can check the status of the building process by navigating to the “Actions” tab of your repository.\nSelect the “pages build and deployment workflow” in the list of workflows on the bottom righthand side of the page.\n\nThis shows you GitHub’s building and deployment process as a flowchart. While it is working on each step there will be an amber circle next to the name of that sub-task. When a sub-task is completed, the amber circle becomes a green circle with a check mark.\n\nWhen the three steps are complete the amber clock symbol next to the “pages build and deployment” action will turn into a larger green circle with a check mark. This is GitHub’s way of telling you that your website is live and accessible to anyone on the internet.\n\nYou can now visit your website by visiting its dedicated URL. This URL can be found by returning to the “Settings” tab and then scrolling through the sidebar to the “Pages” section.\nAlternately, the website for your repository always uses the following composition: https://repository owner.github.io/repository name/\n\nIf we visit that link, we can see that our website is live!\n\n\nGitHub Housekeeping\nWe recommend a quick housekeeping step now to make it easier to find this URL in the future. Copy the URL from the Pages setting area and return to the “Code” tab of the repository.\nOnce there, click the small gear icon to the right of the “About” header.\n\nIn the resulting window, paste the copied URL into the “Website” field. Once you’ve pasted it in, click the green “Save changes” button.\n\n\n\nThis places the link to your deployed website in an intuitive, easy-to-find location both for interested third parties and yourself in the future."
  },
  {
    "objectID": "modules_tutorials/googledrive-auth.html",
    "href": "modules_tutorials/googledrive-auth.html",
    "title": "LTER Scientific Computing Team",
    "section": "",
    "text": "In order to connect R with a GoogleDrive, we’ll need to authorize googledrive to act on our behalf. This only needs to be done once (per computer) so follow along and you’ll be building GoogleDrive into your workflows in no time!\nFirst, install the googledrive and httpuv R packages. The googledrive package’s need is self-evident while the httpuv package makes the following steps a little easier than googledrive makes it alone. Be sure to load the googledrive package after you install it!\n\n# Install packages\ninstall.packages(c(\"googledrive\", \"httpuv\"))\n\n# Load them\nlibrary(googledrive)\n\nOnce you’ve installed the packages we can begin the authentication in R using the drive_auth function in the googledrive package.\n\ngoogledrive::drive_auth(email = \"enter your gmail here!\")\n\nIf this is your first time using googledrive, drive_auth will kick you to a new tab of your browser (see below for a screen grab of that screen) where you can pick which Gmail you’d like to connect to R.\n\n\n\nClick the Gmail you want to use and you will get a second screen where Google tells you that “Tidyverse API” wants access to your Google Account. This message is followed by three checkboxes, the first two are grayed out but the third is unchecked.\n\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nThis next bit is vitally important so carefully read and follow the next instruction!\n\n\nIn this screen, you must check the unchecked box to be able to use the googledrive R package. If you do not check this box all attempts to use googledrive functions will get an error that says “insufficient permissions”.\n\n\n\nWhile granting access to “see, edit, create, and”delete” all of your Google Drive files” sounds like a significant security risk, those powers are actually why you’re using the googledrive package in the first place! You want to be able to download existing Drive files, change them in R on your computer, and then put them back in Google Drive which is exactly what is meant by “see, edit, create, and delete”.\nAlso, this power only applies to the computer you’re currently working on! Granting access on your work computer allows only that computer to access your Drive files. So don’t worry about giving access to your Drive to the whole world, that is protected by the same failsafes that you use when you let your computer remember a password to a website you frequent.\nAfter you’ve checked the authorization box, scroll down and click the “Continue” button.\n\n\n\nThis should result in a plain text page that tells you to close this window and return to R. If you see this message you are ready to use the googledrive package!"
  },
  {
    "objectID": "tutorial_json.html",
    "href": "tutorial_json.html",
    "title": "Storing User-Specific Information with JSONs",
    "section": "",
    "text": "Working groups sometimes need to handle user-specific information in their code. For example, if your group stores your data in the cloud (e.g., in Box, in Dropbox, etc.) each user will have a different “absolute file path” to the synced version of the data folder on their personal computer. Similarly, groups may find it valuable to use their email address in the code. While you could simply have each group member add their information (file path, email, etc.) and comment out all but one of them when you work in that script, there is a better option: user-specific JSON files!\nThe main advantage of this method is that you and your group members do not have to manually change any user-specific information in scripts just because a different person runs them!\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nInstall the jsonlite R package\n\nFeel free to skip any steps that you have already completed!\n\n\n\nFirst, you’ll need to use RStudio to create your JSON file by creating a new text file (in the top left of RStudio click “File”  “New File”  “Text File”). In the new text file, add content that looks like this:\n\n{\n  \"email\":\"my_email@gmail.com\"\n  \"dropbox_path\":\"path/to/dropbox-sync/for/me\"\n}\n\nReplace the content on the right of the colon with your actual information. If desired, you can add as many other pieces of user-specific information as you’d like! Simply follow the \"info name\":\"info content\" format and make sure that each piece of information is on its own line.\nOne small note here for when you work with your group: all group members need to use exactly the same name to the left of each colon.\nYou’ll see later when we show an example of this but you can think of the information on the left of the colon as comparable with a column name. It doesn’t matter that the text in the “rows” will differ between users as long as the script has a consistent “column” in which to look for that text.\n\n\n\nThis may seem self-evident but all group members need to use the same file name for this new JSON file. We recommend user.json if you are undecided. This will let scripts that refer to the JSON use the same file name regardless of which user is running the code (same spirit as using consistent names for each piece of information in the file.)\n\n\n\nIf you’re using version control for your project (which we strongly recommend!), you’ll want Git to ignore the fact that this file differs for each user. Navigate to the .gitignore file of your project and put in the name of your JSON file as one of the files to ignore. We don’t want to push the JSON to GitHub since each person’s file will look different (that is our intent after all) and you wouldn’t want to accidentally overwrite your teammate’s user-specific information or cause a merge conflict.\nFor a deeper dive into the .gitignore check out that module of our “Collaborative Coding with GitHub” workshop!\n\n\n\nIf you’ve made it through the preceding steps, you can now use the information you stored in the JSON file. You’ll need to use the jsonlite R package to read in your file but once you’ve done that, you can access the information inside of it in classic R fashion.\nSee an example below:\n\n# Load needed library\nlibrary(jsonlite)\n\n# Read in the JSON file\nuser_info &lt;- jsonlite::read_json(\"user.json\")\n\n# Grab the file path out of it\ndropbox &lt;- user_info$dropbox_path\n\n# Use it as you would any other file path\nmy_data &lt;- read.csv(file = file.path(dropbox, \"2024_data.csv\"))\n\nNow everyone in your group can use the same script because their personal file paths are readily accessible without needing to be hard-coded! The same theory applies to any other piece of information your group finds it valuable to store in the JSON.\n\n\nIdentifying and manually writing out an absolute file path can be cumbersome so we’ve found a nice work-around (at least for Mac users) that you may find useful. First, in Finder, navigate to the last folder in the file path you’d like to preserve. In the row of folder names in the bottom of the Finder window, right-click the folder name and select “Copy ‘&lt;folder name&gt;’ as Pathname”.\nOnce you’ve done that, you can simply paste the file path into your JSON file."
  },
  {
    "objectID": "tutorial_json.html#overview",
    "href": "tutorial_json.html#overview",
    "title": "Storing User-Specific Information with JSONs",
    "section": "",
    "text": "Working groups sometimes need to handle user-specific information in their code. For example, if your group stores your data in the cloud (e.g., in Box, in Dropbox, etc.) each user will have a different “absolute file path” to the synced version of the data folder on their personal computer. Similarly, groups may find it valuable to use their email address in the code. While you could simply have each group member add their information (file path, email, etc.) and comment out all but one of them when you work in that script, there is a better option: user-specific JSON files!\nThe main advantage of this method is that you and your group members do not have to manually change any user-specific information in scripts just because a different person runs them!\n\n\nTo follow along with this tutorial you will need to take the following steps:\n\nDownload R\nDownload RStudio\nInstall the jsonlite R package\n\nFeel free to skip any steps that you have already completed!\n\n\n\nFirst, you’ll need to use RStudio to create your JSON file by creating a new text file (in the top left of RStudio click “File”  “New File”  “Text File”). In the new text file, add content that looks like this:\n\n{\n  \"email\":\"my_email@gmail.com\"\n  \"dropbox_path\":\"path/to/dropbox-sync/for/me\"\n}\n\nReplace the content on the right of the colon with your actual information. If desired, you can add as many other pieces of user-specific information as you’d like! Simply follow the \"info name\":\"info content\" format and make sure that each piece of information is on its own line.\nOne small note here for when you work with your group: all group members need to use exactly the same name to the left of each colon.\nYou’ll see later when we show an example of this but you can think of the information on the left of the colon as comparable with a column name. It doesn’t matter that the text in the “rows” will differ between users as long as the script has a consistent “column” in which to look for that text.\n\n\n\nThis may seem self-evident but all group members need to use the same file name for this new JSON file. We recommend user.json if you are undecided. This will let scripts that refer to the JSON use the same file name regardless of which user is running the code (same spirit as using consistent names for each piece of information in the file.)\n\n\n\nIf you’re using version control for your project (which we strongly recommend!), you’ll want Git to ignore the fact that this file differs for each user. Navigate to the .gitignore file of your project and put in the name of your JSON file as one of the files to ignore. We don’t want to push the JSON to GitHub since each person’s file will look different (that is our intent after all) and you wouldn’t want to accidentally overwrite your teammate’s user-specific information or cause a merge conflict.\nFor a deeper dive into the .gitignore check out that module of our “Collaborative Coding with GitHub” workshop!\n\n\n\nIf you’ve made it through the preceding steps, you can now use the information you stored in the JSON file. You’ll need to use the jsonlite R package to read in your file but once you’ve done that, you can access the information inside of it in classic R fashion.\nSee an example below:\n\n# Load needed library\nlibrary(jsonlite)\n\n# Read in the JSON file\nuser_info &lt;- jsonlite::read_json(\"user.json\")\n\n# Grab the file path out of it\ndropbox &lt;- user_info$dropbox_path\n\n# Use it as you would any other file path\nmy_data &lt;- read.csv(file = file.path(dropbox, \"2024_data.csv\"))\n\nNow everyone in your group can use the same script because their personal file paths are readily accessible without needing to be hard-coded! The same theory applies to any other piece of information your group finds it valuable to store in the JSON.\n\n\nIdentifying and manually writing out an absolute file path can be cumbersome so we’ve found a nice work-around (at least for Mac users) that you may find useful. First, in Finder, navigate to the last folder in the file path you’d like to preserve. In the row of folder names in the bottom of the Finder window, right-click the folder name and select “Copy ‘&lt;folder name&gt;’ as Pathname”.\nOnce you’ve done that, you can simply paste the file path into your JSON file."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Working groups have often asked our team to synthesize and wrangle data from various heterogeneous datasets. For example, the Plant Reproduction working group had their own plant species data that they compiled from literature and expert knowledge. To get more data, we pulled from the TRY plant trait database and wrangled the dataset. After fixing the TRY column headers, values and units, we combined synonymous columns together with the working group’s original dataset. We ended up with a comprehensive, tidy dataset that covered over 100 plant species.\nIn another example, the Soil Phosphorus working group had gathered a bunch of soil nutrient datasets that all had different column headers and units. To combine these datasets together, we standardized the column headers via a data key, which consisted of a spreadsheet connecting the old column names to the new combined column names. Then we were able to synthesize the separate csv files into one master dataset.\nData synthesizing remains one of our most popular requests so please do not hesitate to reach out to us if we can help with this task, no matter how big or small."
  },
  {
    "objectID": "portfolio.html#data-synthesizing",
    "href": "portfolio.html#data-synthesizing",
    "title": "Portfolio",
    "section": "",
    "text": "Working groups have often asked our team to synthesize and wrangle data from various heterogeneous datasets. For example, the Plant Reproduction working group had their own plant species data that they compiled from literature and expert knowledge. To get more data, we pulled from the TRY plant trait database and wrangled the dataset. After fixing the TRY column headers, values and units, we combined synonymous columns together with the working group’s original dataset. We ended up with a comprehensive, tidy dataset that covered over 100 plant species.\nIn another example, the Soil Phosphorus working group had gathered a bunch of soil nutrient datasets that all had different column headers and units. To combine these datasets together, we standardized the column headers via a data key, which consisted of a spreadsheet connecting the old column names to the new combined column names. Then we were able to synthesize the separate csv files into one master dataset.\nData synthesizing remains one of our most popular requests so please do not hesitate to reach out to us if we can help with this task, no matter how big or small."
  },
  {
    "objectID": "portfolio.html#spatial-analyses-wrangling",
    "href": "portfolio.html#spatial-analyses-wrangling",
    "title": "Portfolio",
    "section": "Spatial Analyses & Wrangling",
    "text": "Spatial Analyses & Wrangling\nOur team has been instrumental in acquiring, wrangling, and summarizing spatial data. For the Silica Exports working group we developed a workflow (see here) that accomplishes the following tasks:\n\nCreates shapefiles that identify the drainage basin linked to several hundred stream gages\n“Cookie cuts” spatial data within those drainage basins (e.g., land cover, lithology, precipitation, etc.)\nSummarizes that extracted data both as rasters and as data tables for use as explanatory variables in other analyses\n\nWe are comfortable working with such data and can help your team acquire and/or process spatial data if that is of interest!"
  },
  {
    "objectID": "portfolio.html#figures-for-publication",
    "href": "portfolio.html#figures-for-publication",
    "title": "Portfolio",
    "section": "Figures for Publication",
    "text": "Figures for Publication\nAdditionally, our team is available to help your group create visualizations for publications! We have generated many figures for the Plant Reproduction working group’s paper on mast seeding synchrony using ggplot. Please feel free to reach out to us for help on visualizations when your team has reached the writing stage in your project."
  },
  {
    "objectID": "portfolio.html#project-websites",
    "href": "portfolio.html#project-websites",
    "title": "Portfolio",
    "section": "Project Websites",
    "text": "Project Websites\nOur team also helped build a website for the Soil Organic Matter (SOM) working group. One of this group’s primary products was a synthesized data package containing observed data, modifications of that data, and models based on them. The website operates in part to publicize this data product but also to provide a central location for other resources developed by or important to the SOM group.\nFor your group we can (if desired):\n\nBuild a website using Quarto\n\nAll website content creation can be done via RStudio which your group may already be somewhat familiar with\nQuarto also offers a new “visual editor” that lets you format text as you would in any word processor (i.e., Microsoft Word, Pages, etc.)\n\nMaintain the website OR help you to maintain it\n\nQuarto is written entirely in “Markdown syntax” which makes it easily maintained by either our team or yours depending on your preference\nWe have also created a tutorial on making websites with Quarto that you are welcome to explore!"
  },
  {
    "objectID": "portfolio.html#r-packages",
    "href": "portfolio.html#r-packages",
    "title": "Portfolio",
    "section": "R Packages",
    "text": "R Packages\n\n\nscicomptools\nWhile much of the work we do is specific to a given working group or task, sometimes we realize afterwards that our functions have the potential to be useful beyond the scope for which they were initially written. To that end, we have created the R package scicomptools!\n\nPackage Description\nThis package contains a diverse mix of functions for everything from repetitive data wrangling tasks to checking whether you have a token attached for GitHub. In addition, functions that we wrote that are deprecated (usually because their internal workings have been superseded by packages on CRAN) are removed from the package but retained in the GitHub repository in case they are useful to you! All functions–both live and deprecated–are summarized in the README on the GitHub repository so take a look!\n\n\nInstallation Instructions\nTo install the package in R, use the following:\n\ninstall.packages(\"scicomptools\")\n\n\n\n\n\nHERON\nWhen we’ve developed enough custom functions for a working group’s workflow, we can move those functions into its very own R package! For example, we’ve created a package for the Silica Exports working group called HERON (“HElpers for River ObservatioN”).\n\nPackage Description\nThis package contains several helper functions that are integral to the group’s workflow, which includes identifying inflection points (i.e., hills & valleys) in trendlines and running separate regressions on each chunk of the line between such points. HERON is meant to be used for workflows involving the EGRET and SiZer R packages.\n\n\nInstallation Instructions\nTo install the package in R, use the following:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"lter/HERON\")"
  },
  {
    "objectID": "portfolio.html#shiny-apps",
    "href": "portfolio.html#shiny-apps",
    "title": "Portfolio",
    "section": "Shiny Apps",
    "text": "Shiny Apps\n\nOur team created another R package–lterpalettefinder–to extract color palettes from user-supplied photos. To help non-R users still explore this package and have fun pulling color palettes from photos, we created this standalone Shiny app. This app lets anyone interact with lterpalettefinder via their browser with no R experience required!\nWe have also built apps to help working groups visualize data or present survey results in an interactive, visually-appealing format. Shiny apps can also include useful ‘overview’ portions that serve as an excellent landing page for third parties to your group’s activities!"
  },
  {
    "objectID": "wg_team-coding.html",
    "href": "wg_team-coding.html",
    "title": "Team Coding: 5 Essentials",
    "section": "",
    "text": "Have you ever had trouble running someone else’s code or re-running your own old code? Working in a synthesis group can bring up challenges like these as members try to run workflows written by others or written by themselves at the last in-person meeting months ago. To make the process easier and more reproducible, here is a list of our top 5 best practices to follow as you collaborate on scripts that address your group’s scientific questions. These are merely suggestions but we hope that they help facilitate seamless synthesis science!"
  },
  {
    "objectID": "wg_team-coding.html#prioritize-future-you",
    "href": "wg_team-coding.html#prioritize-future-you",
    "title": "Team Coding: 5 Essentials",
    "section": "1. Prioritize ‘Future You’",
    "text": "1. Prioritize ‘Future You’\nIf something takes more time now but will work better in the long run, invest the time now to save yourself heartache in the future. This can apply to taking the time to make a document listing the scripts in a given workflow, adding descriptive comments in an existing script, and many other contexts. By investing this time now, you will save ‘future you’ from unnecessary labor."
  },
  {
    "objectID": "wg_team-coding.html#always-leave-comments",
    "href": "wg_team-coding.html#always-leave-comments",
    "title": "Team Coding: 5 Essentials",
    "section": "2. Always Leave Comments",
    "text": "2. Always Leave Comments\nLeave enough comments in your code so that other members of your team (and ‘future you’!) can understand what your script does. This is a crucial habit that will benefit you immensely. By making your code more human-readable, you open the door for improved communication among team members. This makes it easier for people who weren’t involved in your workflow to jump in and give feedback if necessary or to onboard new team members who join later in the project. Plus, it is less of a hassle to edit and maintain well-commented code in the future; you can make changes without spending too much time deciphering each line of code."
  },
  {
    "objectID": "wg_team-coding.html#use-relative-file-paths",
    "href": "wg_team-coding.html#use-relative-file-paths",
    "title": "Team Coding: 5 Essentials",
    "section": "3. Use Relative File Paths",
    "text": "3. Use Relative File Paths\nWhen coding collaboratively, accounting for the difference between your folder structure and those of your colleagues becomes critical. For example, if you read in a data file using its absolute file path (e.g. “/users/my_name/documents/project/data/raw_data/example.csv”), only you will be able to successfully run that line of code and–by extension–your entire script! Also, the slashes between folder names are different depending on each person’s computer operating system, which means even a relative file path will only work for your teammates that share your computer brand.\nIf you’re an R user, there are two quick things you can do in your code to avoid these problems:\n\nRelative Paths – Use the dir.create function in your script to create any necessary folders.\nNeed a data folder? Use dir.create(\"data\") and you’ll create an empty data folder. Anyone else running your code will create the same folder and you can safely assume that part of the file path going forward.\n\n\nOperating System Differences – Use the file.path function with folder names without slashes.\nReading in data? Use file.path(\"data\", \"raw_data\", \"site_a.csv\") and file.path will automatically sense the computer’s operating system and insert the correct slashes for each user.\nFor example, if you are already working in the directory called “project”, then you can access example.csv using this relative file path: data/raw_data/example.csv. You can improve beyond even that by using the file.path function to automatically detect the computer operating system and insert the correct slash for you and anyone else running the code. We recommend using this function and assigning your file path to an object so you can use it anytime.\n\nmy_path &lt;- file.path(\"data\", \"raw_data\")\nmy_raw_data &lt;- read.csv(file = file.path(my_path, \"example.csv\"))"
  },
  {
    "objectID": "wg_team-coding.html#store-raw-data-in-the-cloud",
    "href": "wg_team-coding.html#store-raw-data-in-the-cloud",
    "title": "Team Coding: 5 Essentials",
    "section": "4. Store Raw Data in the Cloud",
    "text": "4. Store Raw Data in the Cloud\nIf you’re a GitHub user, you may be tempted to store your data files there, but GitHub limits the size of files allowed in repositories. Adding files larger than 50MB will receive a warning, and files larger than 100MB will be blocked. If you’re working with big datasets or spatial data, you can exceed this limit pretty fast.\nTo avoid this, we recommend instead that you store your raw data files in the cloud and make them available to everyone in your group. For example, you can create a folder for raw data in a Shared Google Drive (which we can create for you!). Then, you can download the data using the googledrive R package or with any other Google Drive API in your preferred language."
  },
  {
    "objectID": "wg_team-coding.html#meta-document",
    "href": "wg_team-coding.html#meta-document",
    "title": "Team Coding: 5 Essentials",
    "section": "5. Meta-Document",
    "text": "5. Meta-Document\nDocumenting every individual script is important, but it’s also well worth the time and effort to document the big picture of your workflow. As you continue to build on your workflow, it can be hard to keep track of each script’s role and how they relate to each other. You might need to update a script upstream and then try to figure out what other scripts downstream need to be updated next in order to account for the new edits. If you’re not using a workflow management software, then it’s best to thoroughly document how each script fits into the larger workflow. The README is a great place to document each step along the way.\n\n\n\nRegal Fritillary on Milkweed, Konza Prairie Biological Station - Photographer: Jill Haukos"
  },
  {
    "objectID": "internal_team-onboard.html",
    "href": "internal_team-onboard.html",
    "title": "Sci Comp Team Onboarding",
    "section": "",
    "text": "This page contains a quick, cookbook-style set of instructions for new staff on the Scientific Computing Support Team. We are so excited to have you join us and we hope that these instructions are clear and easy-to-follow. Please don’t hesitate to reach out if you run into any issues (see the “Our Team” tab of this site) or post a GitHub issue yourself on this website’s repository!\nListed below are a mix of references and tutorials on concepts we aim to promote to researchers who we support, as well as tools and workflows we use in our team. The goal is to give context about open and reproducible science principles NCEAS is promoting to the scientific community. You will also find information on getting access to the tools that you will need and some general information about working at NCEAS."
  },
  {
    "objectID": "internal_team-onboard.html#background-reading-material",
    "href": "internal_team-onboard.html#background-reading-material",
    "title": "Sci Comp Team Onboarding",
    "section": "Background Reading Material",
    "text": "Background Reading Material\n\nHampton et al 2015. “The Tao of open science for ecology”\nBorer et al 2009. “Effective Data Management”\nHeidborn 2008. “Shedding Light on the Dark Data in the Long Tail of Science”\nFegraus et al 2005. “Maximizing the Value of Ecological Data with Structured Metadata:”"
  },
  {
    "objectID": "internal_team-onboard.html#programs-resources",
    "href": "internal_team-onboard.html#programs-resources",
    "title": "Sci Comp Team Onboarding",
    "section": "Programs & Resources",
    "text": "Programs & Resources\n\nGitHub Tutorial\nWe rely heavily on GitHub for collaboration, and there are a few things you must do to get set up. First, complete the NCEAS GitHub Tutorial here. This tutorial will help you with getting started with git and GitHub using RStudio.\n\n\nLTER GitHub Organization\nRegister (if you have not already) for a personal GitHub account and send your username to Marty to be added to the LTER GitHub. This GitHub Organization “owns” the working groups repositories that you will be directly working with so being added to this organization will give you access to needed repositories.\n\n\nNCEAS GitHub Enterprise Organization\nWe use a GitHub Enterprise account to keep track of tasks for the Scientific Computing Support of LTER working groups. To be added to the account you must create an NCEAS account.\nFollow the directions outlined here and send your username to Marty Downs (not the email outlined in the Google Doc).\nAfter you have been added, you will be able to access the Issues tab of the lter-wg-scicomp GitHub repository. We use issues on this repository to keep track of tasks and projects as well as who is primarily responsible for a given task and which working group gave us the task initially.\n\n\nSlack\nWe use Slack to communicate with one another throughout the day. To be added to the NCEAS Slack group, register here. If you already have a slack account, be sure to use the email address you used to register.\nFind and join our #scicomp channel by clicking the plus sign next to the Channels section. Feel free to join any other channels that you might find interesting! Popular channels include #diversity, #nceas, and #social.\nFinally, complete this short tutorial on using Slack.\n\n\nNCEAS Server: Aurora\nWe use the Aurora server (located at aurora.nceas.ucsb.edu) when working with RStudio or JupyterHub. Send an email to help@nceas.ucsb.edu requesting an account mentioning you are working with Marty. Thomas Hetmank or Nick Outin will contact you with directions for setting up an account."
  },
  {
    "objectID": "internal_team-onboard.html#recurring-nceas-meetings",
    "href": "internal_team-onboard.html#recurring-nceas-meetings",
    "title": "Sci Comp Team Onboarding",
    "section": "Recurring NCEAS Meetings",
    "text": "Recurring NCEAS Meetings\nThere are a number of recurring meetings that you are encouraged to attend. At your hiring, you should receive an invitation to the NCEAS Google Calendar which has links to all the meeting information. These include Coffee Klatch (a coffee social), Hacky Hours, and Data Science Chats (to name a few).\nWe also have a team Google Calendar to manage team events. Marty will add you if you have not been already."
  },
  {
    "objectID": "internal_team-onboard.html#data-science-background-and-tutorials",
    "href": "internal_team-onboard.html#data-science-background-and-tutorials",
    "title": "Sci Comp Team Onboarding",
    "section": "Data Science Background and Tutorials",
    "text": "Data Science Background and Tutorials\nFor good background information on the tools that we use, read through and practice examples of the following chapters from a training we did for postdocs in early 2020: https://science-for-nature-and-people.github.io/2020-data-collab-workshop/2020-02-snapp/index.html\nChapters 3, 5, 8 are a good background about the tools we use.\nChapters 9, 10 are a good introduction to data modeling.\n\nR Self-Assessment\nFinally, please complete this R Training Assessment to self-assess your skills in R.\nSchedule a session with the rest of the team to debrief on your experience."
  },
  {
    "objectID": "internal_team-onboard.html#hr-related-resources",
    "href": "internal_team-onboard.html#hr-related-resources",
    "title": "Sci Comp Team Onboarding",
    "section": "HR-related Resources",
    "text": "HR-related Resources\n\nReporting Hours & Kronos\nWe submit vacation and sick day hours online through Kronos. You should receive an email at the start of your employment adding you to an email listserv that will send reminders on when to approve each month’s timesheet.\n\n\nUC Path\nUC Path is the online HR portal and can be accessed with your UCSB Net ID. It is where you can access things like your paycheck, next date of paycheck, and benefits (if applicable)."
  }
]